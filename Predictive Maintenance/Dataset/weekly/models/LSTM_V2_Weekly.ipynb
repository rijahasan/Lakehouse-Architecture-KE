{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# ----- Step 1: Load and Prepare Weekly Data -----\n",
        "df_weekly = pd.read_csv(\"/content/weekly_train.csv\")\n",
        "df_weekly['Week'] = pd.to_datetime(df_weekly['week'])  # Convert 'week' column to datetime\n",
        "df_weekly = df_weekly.sort_values(by=['fdr_Id', 'Week'])\n",
        "\n",
        "# Shift the Fault_Occurred column to create target labels\n",
        "df_weekly['target'] = df_weekly.groupby('fdr_Id')['Fault_Occurred'].shift(-1)\n",
        "df_weekly = df_weekly.dropna(subset=['target'])\n",
        "df_weekly['target'] = df_weekly['target'].astype(int)\n",
        "\n",
        "# ----- Step 2: Define Features and Normalize -----\n",
        "exclude_cols = ['fdr_Id', 'Week', 'Fault_Occurred', 'target', 'week']\n",
        "features = [col for col in df_weekly.columns if col not in exclude_cols]\n",
        "scaler = MinMaxScaler()\n",
        "df_weekly[features] = scaler.fit_transform(df_weekly[features])\n",
        "\n",
        "# ----- Step 3: Create Feeder-Wise Sequences -----\n",
        "def create_sequences_by_feeder(df, seq_length=3):\n",
        "    X_list, y_list = [], []\n",
        "    feeders = df['fdr_Id'].unique()\n",
        "    for feeder in feeders:\n",
        "        feeder_df = df[df['fdr_Id'] == feeder].reset_index(drop=True)\n",
        "        for i in range(len(feeder_df) - seq_length):\n",
        "            seq_X = feeder_df[features].iloc[i:i+seq_length].values\n",
        "            seq_y = feeder_df['target'].iloc[i + seq_length]\n",
        "            X_list.append(seq_X)\n",
        "            y_list.append(seq_y)\n",
        "    return torch.tensor(np.array(X_list), dtype=torch.float32), torch.tensor(np.array(y_list), dtype=torch.float32)\n",
        "\n",
        "seq_length = 10\n",
        "X, y = create_sequences_by_feeder(df_weekly, seq_length=seq_length)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "# ----- Step 4: Split Data and Create DataLoaders -----\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ----- Step 5: Define the LSTM Model -----\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, dropout=0.3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        x = hidden[-1]\n",
        "        x = self.fc(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Instantiate model\n",
        "model = LSTMModel(input_dim=X.shape[2]).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ----- Step 6: Train the Model -----\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X).squeeze()\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# ----- Step 7: Evaluate the Model -----\n",
        "model.eval()\n",
        "y_pred_list, y_test_list = [], []\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        output = model(batch_X).squeeze()\n",
        "        y_pred_list.extend(output.cpu().numpy())\n",
        "        y_test_list.extend(batch_y.cpu().numpy())\n",
        "\n",
        "y_pred_binary = (np.array(y_pred_list) > 0.5).astype(int)\n",
        "y_test_cpu = np.array(y_test_list)\n",
        "\n",
        "precision = precision_score(y_test_cpu, y_pred_binary)\n",
        "recall = recall_score(y_test_cpu, y_pred_binary)\n",
        "f1 = f1_score(y_test_cpu, y_pred_binary)\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# ----- Step 8: Predict Feeders Going Down in Target Week -----\n",
        "target_week = pd.to_datetime(\"2025-01-06\")\n",
        "df_filtered = df_weekly[df_weekly['Week'] < target_week]\n",
        "\n",
        "feeder_predictions = []\n",
        "feeders = df_filtered['fdr_Id'].unique()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for feeder in feeders:\n",
        "        feeder_df = df_filtered[df_filtered['fdr_Id'] == feeder].reset_index(drop=True)\n",
        "        if len(feeder_df) >= seq_length:\n",
        "            seq_data = feeder_df[features].iloc[-seq_length:].values\n",
        "            seq_tensor = torch.tensor(seq_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            pred_prob = model(seq_tensor).item()\n",
        "            feeder_predictions.append((feeder, pred_prob))\n",
        "\n",
        "threshold = 0.24\n",
        "down_feeders = {(f, prob) for f, prob in feeder_predictions if prob > threshold}\n",
        "\n",
        "# ----- Step 9: Compare Predictions with Actual Data -----\n",
        "actual_data = pd.read_csv(\"/content/weekly_test.csv\")\n",
        "actual_data['Week'] = pd.to_datetime(actual_data['week'])\n",
        "actual_failures = set(actual_data[actual_data['Fault_Occurred'] == 1]['fdr_Id'])\n",
        "\n",
        "predicted_failures = {f for f, _ in down_feeders}\n",
        "true_positives = actual_failures & predicted_failures\n",
        "false_negatives = actual_failures - predicted_failures\n",
        "false_positives = predicted_failures - actual_failures\n",
        "\n",
        "precision = len(true_positives) / (len(true_positives) + len(false_positives) + 1e-9)\n",
        "recall = len(true_positives) / (len(true_positives) + len(false_negatives) + 1e-9)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "\n",
        "print(f\"Correct Predictions: {len(true_positives)}\")\n",
        "print(f\"Missed Failures: {len(false_negatives)}\")\n",
        "print(f\"Incorrect Predictions: {len(false_positives)}\")\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCb6bohZ10Gm",
        "outputId": "54e74f62-6b54-4fa9-befc-70e9de65c1cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6260\n",
            "Epoch 2/10, Loss: 0.5691\n",
            "Epoch 3/10, Loss: 0.5620\n",
            "Epoch 4/10, Loss: 0.5627\n",
            "Epoch 5/10, Loss: 0.5562\n",
            "Epoch 6/10, Loss: 0.5538\n",
            "Epoch 7/10, Loss: 0.5519\n",
            "Epoch 8/10, Loss: 0.5539\n",
            "Epoch 9/10, Loss: 0.5497\n",
            "Epoch 10/10, Loss: 0.5504\n",
            "Precision: 0.5993, Recall: 0.5254, F1 Score: 0.5599\n",
            "Correct Predictions: 113\n",
            "Missed Failures: 16\n",
            "Incorrect Predictions: 12\n",
            "Precision: 0.9040, Recall: 0.8760, F1 Score: 0.8898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Newer Version"
      ],
      "metadata": {
        "id": "80-SL6kLTxmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# ----- Step 1: Load and Prepare Weekly Data -----\n",
        "df_weekly = pd.read_csv(\"/content/weekly_train.csv\")\n",
        "df_weekly['Week'] = pd.to_datetime(df_weekly['week'])  # Convert 'week' column to datetime\n",
        "df_weekly = df_weekly.sort_values(by=['fdr_Id', 'Week'])\n",
        "\n",
        "# Shift the Fault_Occurred column to create target labels\n",
        "df_weekly['target'] = df_weekly.groupby('fdr_Id')['Fault_Occurred'].shift(-1)\n",
        "df_weekly = df_weekly.dropna(subset=['target'])\n",
        "df_weekly['target'] = df_weekly['target'].astype(int)\n",
        "\n",
        "# ----- Step 2: Define Features and Normalize -----\n",
        "exclude_cols = ['fdr_Id', 'Week', 'Fault_Occurred', 'target', 'week']\n",
        "features = [col for col in df_weekly.columns if col not in exclude_cols]\n",
        "scaler = MinMaxScaler()\n",
        "df_weekly[features] = scaler.fit_transform(df_weekly[features])\n",
        "\n",
        "# ----- Step 3: Create Feeder-Wise Sequences -----\n",
        "def create_sequences_by_feeder(df, seq_length=3):\n",
        "    X_list, y_list = [], []\n",
        "    feeders = df['fdr_Id'].unique()\n",
        "    for feeder in feeders:\n",
        "        feeder_df = df[df['fdr_Id'] == feeder].reset_index(drop=True)\n",
        "        for i in range(len(feeder_df) - seq_length):\n",
        "            seq_X = feeder_df[features].iloc[i:i+seq_length].values\n",
        "            seq_y = feeder_df['target'].iloc[i + seq_length]\n",
        "            X_list.append(seq_X)\n",
        "            y_list.append(seq_y)\n",
        "    return torch.tensor(np.array(X_list), dtype=torch.float32), torch.tensor(np.array(y_list), dtype=torch.float32)\n",
        "\n",
        "seq_length = 20\n",
        "X, y = create_sequences_by_feeder(df_weekly, seq_length=seq_length)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "# ----- Step 4: Split Data and Create DataLoaders -----\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ----- Step 5: Define the LSTM Model -----\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, dropout=0.3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        x = hidden[-1]\n",
        "        x = self.fc(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Instantiate model\n",
        "model = LSTMModel(input_dim=X.shape[2]).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ----- Step 6: Train the Model -----\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X).squeeze()\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# ----- Step 7: Evaluate the Model -----\n",
        "model.eval()\n",
        "y_pred_list, y_test_list = [], []\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        output = model(batch_X).squeeze()\n",
        "        y_pred_list.extend(output.cpu().numpy())\n",
        "        y_test_list.extend(batch_y.cpu().numpy())\n",
        "\n",
        "y_pred_binary = (np.array(y_pred_list) > 0.5).astype(int)\n",
        "y_test_cpu = np.array(y_test_list)\n",
        "\n",
        "precision = precision_score(y_test_cpu, y_pred_binary)\n",
        "recall = recall_score(y_test_cpu, y_pred_binary)\n",
        "f1 = f1_score(y_test_cpu, y_pred_binary)\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# ----- Step 8: Predict Feeders Going Down for Each Week of January 2025 -----\n",
        "weeks_to_predict = [\n",
        "    pd.to_datetime(\"2025-01-06\"),  # Week 1\n",
        "    pd.to_datetime(\"2025-01-13\"),  # Week 2\n",
        "    pd.to_datetime(\"2025-01-20\"),  # Week 3\n",
        "    pd.to_datetime(\"2025-01-27\"),  # Week 4\n",
        "]\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for target_week in weeks_to_predict:\n",
        "        df_filtered = df_weekly[df_weekly['Week'] < target_week]\n",
        "        feeder_predictions = []\n",
        "        feeders = df_filtered['fdr_Id'].unique()\n",
        "\n",
        "        for feeder in feeders:\n",
        "            feeder_df = df_filtered[df_filtered['fdr_Id'] == feeder].reset_index(drop=True)\n",
        "            if len(feeder_df) >= seq_length:\n",
        "                seq_data = feeder_df[features].iloc[-seq_length:].values\n",
        "                seq_tensor = torch.tensor(seq_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "                pred_prob = model(seq_tensor).item()\n",
        "                feeder_predictions.append({\n",
        "                    'week': target_week.strftime('%Y-%m-%d'),\n",
        "                    'fdr_Id': feeder,\n",
        "                    'predicted_probability': pred_prob\n",
        "                })\n",
        "\n",
        "        all_predictions.extend(feeder_predictions)\n",
        "\n",
        "# Convert predictions to DataFrame and save\n",
        "predictions_df = pd.DataFrame(all_predictions)\n",
        "predictions_df.to_csv(\"/content/predicted_feeder_failures_jan2025.csv\", index=False)\n",
        "print(\"Predictions saved to /content/predicted_feeder_failures_jan2025.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3UxuLaQ_0wz",
        "outputId": "92bb3d49-61d7-4728-aae6-6f7026aa24fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6259\n",
            "Epoch 2/10, Loss: 0.5625\n",
            "Epoch 3/10, Loss: 0.5539\n",
            "Epoch 4/10, Loss: 0.5500\n",
            "Epoch 5/10, Loss: 0.5476\n",
            "Epoch 6/10, Loss: 0.5479\n",
            "Epoch 7/10, Loss: 0.5444\n",
            "Epoch 8/10, Loss: 0.5449\n",
            "Epoch 9/10, Loss: 0.5435\n",
            "Epoch 10/10, Loss: 0.5410\n",
            "Precision: 0.6027, Recall: 0.5411, F1 Score: 0.5702\n",
            "Predictions saved to /content/predicted_feeder_failures_jan2025.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Step 9: Compare Predictions with Actual Data for Each Week -----\n",
        "actual_data = pd.read_csv(\"/content/weekly_test.csv\")\n",
        "actual_data['Week'] = pd.to_datetime(actual_data['week'])\n",
        "\n",
        "evaluation_results = []\n",
        "threshold = 0.37  # Use the same threshold\n",
        "\n",
        "for target_week in weeks_to_predict:\n",
        "    week_str = target_week.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Filter predictions and actuals for this week\n",
        "    week_predictions = predictions_df[predictions_df['week'] == week_str]\n",
        "    predicted_failures = set(week_predictions[week_predictions['predicted_probability'] > threshold]['fdr_Id'])\n",
        "\n",
        "    week_actuals = actual_data[actual_data['Week'] == target_week]\n",
        "    actual_failures = set(week_actuals[week_actuals['Fault_Occurred'] == 1]['fdr_Id'])\n",
        "\n",
        "    true_positives = actual_failures & predicted_failures\n",
        "    false_negatives = actual_failures - predicted_failures\n",
        "    false_positives = predicted_failures - actual_failures\n",
        "\n",
        "    precision = len(true_positives) / (len(true_positives) + len(false_positives) + 1e-9)\n",
        "    recall = len(true_positives) / (len(true_positives) + len(false_negatives) + 1e-9)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'week': week_str,\n",
        "        'correct_predictions': len(true_positives),\n",
        "        'missed_failures': len(false_negatives),\n",
        "        'incorrect_predictions': len(false_positives),\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    })\n",
        "\n",
        "    print(f\"\\nResults for {week_str}:\")\n",
        "    print(f\"Correct Predictions: {len(true_positives)}\")\n",
        "    print(f\"Missed Failures: {len(false_negatives)}\")\n",
        "    print(f\"Incorrect Predictions: {len(false_positives)}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Save evaluation results to CSV\n",
        "evaluation_df = pd.DataFrame(evaluation_results)\n",
        "evaluation_df.to_csv(\"/content/evaluation_results_jan2025.csv\", index=False)\n",
        "print(\"\\nEvaluation results saved to /content/evaluation_results_jan2025.csv\")\n",
        "\n",
        "# ---- NEW PART: Calculate and print average F1 Score ----\n",
        "average_f1 = evaluation_df['f1_score'].mean()\n",
        "print(f\"\\nAverage F1 Score over all weeks: {average_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTsevZjaU0kl",
        "outputId": "916c63c6-2b9c-409f-efe0-6946724a1c41"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for 2025-01-06:\n",
            "Correct Predictions: 34\n",
            "Missed Failures: 22\n",
            "Incorrect Predictions: 28\n",
            "Precision: 0.5484, Recall: 0.6071, F1 Score: 0.5763\n",
            "\n",
            "Results for 2025-01-13:\n",
            "Correct Predictions: 37\n",
            "Missed Failures: 26\n",
            "Incorrect Predictions: 25\n",
            "Precision: 0.5968, Recall: 0.5873, F1 Score: 0.5920\n",
            "\n",
            "Results for 2025-01-20:\n",
            "Correct Predictions: 44\n",
            "Missed Failures: 23\n",
            "Incorrect Predictions: 18\n",
            "Precision: 0.7097, Recall: 0.6567, F1 Score: 0.6822\n",
            "\n",
            "Results for 2025-01-27:\n",
            "Correct Predictions: 40\n",
            "Missed Failures: 20\n",
            "Incorrect Predictions: 22\n",
            "Precision: 0.6452, Recall: 0.6667, F1 Score: 0.6557\n",
            "\n",
            "Evaluation results saved to /content/evaluation_results_jan2025.csv\n",
            "\n",
            "Average F1 Score over all weeks: 0.6265\n"
          ]
        }
      ]
    }
  ]
}