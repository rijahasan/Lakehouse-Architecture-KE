{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Newer Version"
      ],
      "metadata": {
        "id": "80-SL6kLTxmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# ----- Step 1: Load and Prepare Weekly Data -----\n",
        "df_weekly = pd.read_csv(\"/content/weekly_train.csv\")\n",
        "df_weekly['Week'] = pd.to_datetime(df_weekly['week'])  # Convert 'week' column to datetime\n",
        "df_weekly = df_weekly.sort_values(by=['fdr_Id', 'Week'])\n",
        "\n",
        "# Shift the Fault_Occurred column to create target labels\n",
        "df_weekly['target'] = df_weekly.groupby('fdr_Id')['Fault_Occurred'].shift(-1)\n",
        "df_weekly = df_weekly.dropna(subset=['target'])\n",
        "df_weekly['target'] = df_weekly['target'].astype(int)\n",
        "\n",
        "# ----- Step 2: Define Features and Normalize -----\n",
        "exclude_cols = ['fdr_Id', 'Week', 'Fault_Occurred', 'target', 'week']\n",
        "features = [col for col in df_weekly.columns if col not in exclude_cols]\n",
        "scaler = MinMaxScaler()\n",
        "df_weekly[features] = scaler.fit_transform(df_weekly[features])\n",
        "\n",
        "# ----- Step 3: Create Feeder-Wise Sequences -----\n",
        "def create_sequences_by_feeder(df, seq_length):\n",
        "    X_list, y_list = [], []\n",
        "    feeders = df['fdr_Id'].unique()\n",
        "    for feeder in feeders:\n",
        "        feeder_df = df[df['fdr_Id'] == feeder].reset_index(drop=True)\n",
        "        for i in range(len(feeder_df) - seq_length):\n",
        "            seq_X = feeder_df[features].iloc[i:i+seq_length].values\n",
        "            seq_y = feeder_df['target'].iloc[i + seq_length]\n",
        "            X_list.append(seq_X)\n",
        "            y_list.append(seq_y)\n",
        "    return torch.tensor(np.array(X_list), dtype=torch.float32), torch.tensor(np.array(y_list), dtype=torch.float32)\n",
        "\n",
        "seq_short = 5\n",
        "seq_long = 45\n",
        "\n",
        "X_short, y_short = create_sequences_by_feeder(df_weekly, seq_short)\n",
        "X_long, y_long = create_sequences_by_feeder(df_weekly, seq_long)\n",
        "\n",
        "# Make sure the datasets are aligned in size\n",
        "min_len = min(len(X_short), len(X_long))\n",
        "X_short, y_short = X_short[:min_len], y_short[:min_len]\n",
        "X_long, y_long = X_long[:min_len], y_long[:min_len]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_short, X_long, y = X_short.to(device), X_long.to(device), y_short.to(device)\n",
        "\n",
        "# ----- Step 4: Split Data and Create DataLoaders -----\n",
        "split_idx = int(len(X_short) * 0.8)\n",
        "X_short_train, X_short_test = X_short[:split_idx], X_short[split_idx:]\n",
        "X_long_train, X_long_test = X_long[:split_idx], X_long[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(TensorDataset(X_short_train, X_long_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_short_test, X_long_test, y_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ----- Step 5: Define the Dual LSTM Model -----\n",
        "class DualLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, dropout=0.3):\n",
        "        super(DualLSTMModel, self).__init__()\n",
        "\n",
        "        # Short-term LSTM\n",
        "        self.lstm_short = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        # Long-term LSTM\n",
        "        self.lstm_long = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        # Combine both\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, x_short, x_long):\n",
        "        _, (hidden_short, _) = self.lstm_short(x_short)\n",
        "        _, (hidden_long, _) = self.lstm_long(x_long)\n",
        "\n",
        "        combined = torch.cat((hidden_short[-1], hidden_long[-1]), dim=1)\n",
        "        output = self.fc(combined)\n",
        "        return torch.sigmoid(output)\n",
        "\n",
        "# Instantiate model\n",
        "model = DualLSTMModel(input_dim=X_short.shape[2]).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ----- Step 6: Train the Model -----\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_short, batch_long, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_short, batch_long).squeeze()\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# ----- Step 7: Evaluate the Model -----\n",
        "model.eval()\n",
        "y_pred_list, y_test_list = [], []\n",
        "with torch.no_grad():\n",
        "    for batch_short, batch_long, batch_y in test_loader:\n",
        "        output = model(batch_short, batch_long).squeeze()\n",
        "        y_pred_list.extend(output.cpu().numpy())\n",
        "        y_test_list.extend(batch_y.cpu().numpy())\n",
        "\n",
        "# Using simple 0.5 threshold for now\n",
        "y_pred_binary = (np.array(y_pred_list) > 0.5).astype(int)\n",
        "y_test_cpu = np.array(y_test_list)\n",
        "\n",
        "precision = precision_score(y_test_cpu, y_pred_binary)\n",
        "recall = recall_score(y_test_cpu, y_pred_binary)\n",
        "f1 = f1_score(y_test_cpu, y_pred_binary)\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# ----- Step 8: Predict Feeders Going Down for Each Week of January 2025 -----\n",
        "weeks_to_predict = [\n",
        "    pd.to_datetime(\"2025-01-06\"),  # Week 1\n",
        "    pd.to_datetime(\"2025-01-13\"),  # Week 2\n",
        "    pd.to_datetime(\"2025-01-20\"),  # Week 3\n",
        "    pd.to_datetime(\"2025-01-27\"),  # Week 4\n",
        "]\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for target_week in weeks_to_predict:\n",
        "        df_filtered = df_weekly[df_weekly['Week'] < target_week]\n",
        "        feeder_predictions = []\n",
        "        feeders = df_filtered['fdr_Id'].unique()\n",
        "\n",
        "        for feeder in feeders:\n",
        "            feeder_df = df_filtered[df_filtered['fdr_Id'] == feeder].reset_index(drop=True)\n",
        "            if len(feeder_df) >= max(seq_short, seq_long):\n",
        "                seq_data_short = feeder_df[features].iloc[-seq_short:].values\n",
        "                seq_data_long = feeder_df[features].iloc[-seq_long:].values\n",
        "\n",
        "                seq_tensor_short = torch.tensor(seq_data_short, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "                seq_tensor_long = torch.tensor(seq_data_long, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "                pred_prob = model(seq_tensor_short, seq_tensor_long).item()\n",
        "                feeder_predictions.append({\n",
        "                    'week': target_week.strftime('%Y-%m-%d'),\n",
        "                    'fdr_Id': feeder,\n",
        "                    'predicted_probability': pred_prob\n",
        "                })\n",
        "\n",
        "        all_predictions.extend(feeder_predictions)\n",
        "\n",
        "# Convert predictions to DataFrame and save\n",
        "predictions_df = pd.DataFrame(all_predictions)\n",
        "predictions_df.to_csv(\"/content/predicted_feeder_failures_jan2025.csv\", index=False)\n",
        "print(\"Predictions saved to /content/predicted_feeder_failures_jan2025.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3UxuLaQ_0wz",
        "outputId": "714668e8-ad95-42ce-975f-0077ea9c7a30"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6412\n",
            "Epoch 2/10, Loss: 0.6014\n",
            "Epoch 3/10, Loss: 0.5816\n",
            "Epoch 4/10, Loss: 0.5711\n",
            "Epoch 5/10, Loss: 0.5734\n",
            "Epoch 6/10, Loss: 0.5663\n",
            "Epoch 7/10, Loss: 0.5644\n",
            "Epoch 8/10, Loss: 0.5610\n",
            "Epoch 9/10, Loss: 0.5609\n",
            "Epoch 10/10, Loss: 0.5620\n",
            "Precision: 0.5399, Recall: 0.3458, F1 Score: 0.4216\n",
            "Predictions saved to /content/predicted_feeder_failures_jan2025.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# thresh opt 1 for all"
      ],
      "metadata": {
        "id": "uGcqHnn4a5o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Step 9: Compare Predictions with Actual Data for All Weeks (Global Threshold Optimization) -----\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load actual data\n",
        "actual_data = pd.read_csv(\"/content/weekly_test.csv\")\n",
        "actual_data['Week'] = pd.to_datetime(actual_data['week'])\n",
        "\n",
        "# Threshold search space\n",
        "thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "\n",
        "all_week_predictions = []\n",
        "all_week_actuals = []\n",
        "\n",
        "for target_week in weeks_to_predict:\n",
        "    week_str = target_week.strftime('%Y-%m-%d')\n",
        "\n",
        "    week_predictions = predictions_df[predictions_df['week'] == week_str]\n",
        "    week_actuals = actual_data[actual_data['Week'] == target_week]\n",
        "\n",
        "    # Save all week data together\n",
        "    all_week_predictions.append(week_predictions[['fdr_Id', 'predicted_probability']])\n",
        "    all_week_actuals.append(week_actuals[['fdr_Id', 'Fault_Occurred']])\n",
        "\n",
        "# Concatenate all weeks\n",
        "all_week_predictions = pd.concat(all_week_predictions)\n",
        "all_week_actuals = pd.concat(all_week_actuals)\n",
        "\n",
        "# Merge predictions with actuals\n",
        "merged = pd.merge(all_week_predictions, all_week_actuals, on='fdr_Id', how='inner')\n",
        "\n",
        "# Now search for the best global threshold\n",
        "best_threshold = 0.0\n",
        "best_f1 = -1.0\n",
        "best_precision = 0.0\n",
        "best_recall = 0.0\n",
        "\n",
        "for threshold in thresholds:\n",
        "    merged['predicted_label'] = (merged['predicted_probability'] > threshold).astype(int)\n",
        "\n",
        "    tp = ((merged['predicted_label'] == 1) & (merged['Fault_Occurred'] == 1)).sum()\n",
        "    fp = ((merged['predicted_label'] == 1) & (merged['Fault_Occurred'] == 0)).sum()\n",
        "    fn = ((merged['predicted_label'] == 0) & (merged['Fault_Occurred'] == 1)).sum()\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-9)\n",
        "    recall = tp / (tp + fn + 1e-9)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = threshold\n",
        "        best_precision = precision\n",
        "        best_recall = recall\n",
        "\n",
        "print(f\"\\nBest Global Threshold Found: {best_threshold:.2f}\")\n",
        "print(f\"Precision: {best_precision:.4f}, Recall: {best_recall:.4f}, F1 Score: {best_f1:.4f}\")\n",
        "\n",
        "# ---- Now Evaluate Week-by-Week Using This Best Threshold ----\n",
        "evaluation_results = []\n",
        "\n",
        "for target_week in weeks_to_predict:\n",
        "    week_str = target_week.strftime('%Y-%m-%d')\n",
        "\n",
        "    week_predictions = predictions_df[predictions_df['week'] == week_str]\n",
        "    week_actuals = actual_data[actual_data['Week'] == target_week]\n",
        "\n",
        "    predicted_failures = set(week_predictions[week_predictions['predicted_probability'] > best_threshold]['fdr_Id'])\n",
        "    actual_failures = set(week_actuals[week_actuals['Fault_Occurred'] == 1]['fdr_Id'])\n",
        "\n",
        "    true_positives = actual_failures & predicted_failures\n",
        "    false_negatives = actual_failures - predicted_failures\n",
        "    false_positives = predicted_failures - actual_failures\n",
        "\n",
        "    precision = len(true_positives) / (len(true_positives) + len(false_positives) + 1e-9)\n",
        "    recall = len(true_positives) / (len(true_positives) + len(false_negatives) + 1e-9)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'week': week_str,\n",
        "        'correct_predictions': len(true_positives),\n",
        "        'missed_failures': len(false_negatives),\n",
        "        'incorrect_predictions': len(false_positives),\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    })\n",
        "\n",
        "    print(f\"\\nResults for {week_str}:\")\n",
        "    print(f\"Correct Predictions: {len(true_positives)}\")\n",
        "    print(f\"Missed Failures: {len(false_negatives)}\")\n",
        "    print(f\"Incorrect Predictions: {len(false_positives)}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Save evaluation results to CSV\n",
        "evaluation_df = pd.DataFrame(evaluation_results)\n",
        "evaluation_df.to_csv(\"/content/evaluation_results_jan2025.csv\", index=False)\n",
        "print(\"\\nEvaluation results saved to /content/evaluation_results_jan2025.csv\")\n",
        "\n",
        "# ---- NEW PART: Calculate and print average F1 Score ----\n",
        "average_f1 = evaluation_df['f1_score'].mean()\n",
        "print(f\"\\nAverage F1 Score over all weeks: {average_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBTsz6rkblCO",
        "outputId": "a2291f39-b003-48a3-f414-0266b029f170"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Global Threshold Found: 0.20\n",
            "Precision: 0.5026, Recall: 0.7846, F1 Score: 0.6127\n",
            "\n",
            "Results for 2025-01-06:\n",
            "Correct Predictions: 42\n",
            "Missed Failures: 14\n",
            "Incorrect Predictions: 54\n",
            "Precision: 0.4375, Recall: 0.7500, F1 Score: 0.5526\n",
            "\n",
            "Results for 2025-01-13:\n",
            "Correct Predictions: 47\n",
            "Missed Failures: 16\n",
            "Incorrect Predictions: 49\n",
            "Precision: 0.4896, Recall: 0.7460, F1 Score: 0.5912\n",
            "\n",
            "Results for 2025-01-20:\n",
            "Correct Predictions: 54\n",
            "Missed Failures: 13\n",
            "Incorrect Predictions: 42\n",
            "Precision: 0.5625, Recall: 0.8060, F1 Score: 0.6626\n",
            "\n",
            "Results for 2025-01-27:\n",
            "Correct Predictions: 50\n",
            "Missed Failures: 10\n",
            "Incorrect Predictions: 46\n",
            "Precision: 0.5208, Recall: 0.8333, F1 Score: 0.6410\n",
            "\n",
            "Evaluation results saved to /content/evaluation_results_jan2025.csv\n",
            "\n",
            "Average F1 Score over all weeks: 0.6119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# just to test threshold out"
      ],
      "metadata": {
        "id": "UaNlFI7ycED6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Step 9: Compare Predictions with Actual Data for Each Week -----\n",
        "actual_data = pd.read_csv(\"/content/weekly_test.csv\")\n",
        "actual_data['Week'] = pd.to_datetime(actual_data['week'])\n",
        "\n",
        "evaluation_results = []\n",
        "threshold = 0.31  # Use the same threshold\n",
        "\n",
        "for target_week in weeks_to_predict:\n",
        "    week_str = target_week.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Filter predictions and actuals for this week\n",
        "    week_predictions = predictions_df[predictions_df['week'] == week_str]\n",
        "    predicted_failures = set(week_predictions[week_predictions['predicted_probability'] > threshold]['fdr_Id'])\n",
        "\n",
        "    week_actuals = actual_data[actual_data['Week'] == target_week]\n",
        "    actual_failures = set(week_actuals[week_actuals['Fault_Occurred'] == 1]['fdr_Id'])\n",
        "\n",
        "    true_positives = actual_failures & predicted_failures\n",
        "    false_negatives = actual_failures - predicted_failures\n",
        "    false_positives = predicted_failures - actual_failures\n",
        "\n",
        "    precision = len(true_positives) / (len(true_positives) + len(false_positives) + 1e-9)\n",
        "    recall = len(true_positives) / (len(true_positives) + len(false_negatives) + 1e-9)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'week': week_str,\n",
        "        'correct_predictions': len(true_positives),\n",
        "        'missed_failures': len(false_negatives),\n",
        "        'incorrect_predictions': len(false_positives),\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    })\n",
        "\n",
        "    print(f\"\\nResults for {week_str}:\")\n",
        "    print(f\"Correct Predictions: {len(true_positives)}\")\n",
        "    print(f\"Missed Failures: {len(false_negatives)}\")\n",
        "    print(f\"Incorrect Predictions: {len(false_positives)}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Save evaluation results to CSV\n",
        "evaluation_df = pd.DataFrame(evaluation_results)\n",
        "evaluation_df.to_csv(\"/content/evaluation_results_jan2025.csv\", index=False)\n",
        "print(\"\\nEvaluation results saved to /content/evaluation_results_jan2025.csv\")\n",
        "\n",
        "# ---- NEW PART: Calculate and print average F1 Score ----\n",
        "average_f1 = evaluation_df['f1_score'].mean()\n",
        "print(f\"\\nAverage F1 Score over all weeks: {average_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTsevZjaU0kl",
        "outputId": "b1a56d30-a1c9-4221-f791-37e64bdfda5f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for 2025-01-06:\n",
            "Correct Predictions: 42\n",
            "Missed Failures: 14\n",
            "Incorrect Predictions: 46\n",
            "Precision: 0.4773, Recall: 0.7500, F1 Score: 0.5833\n",
            "\n",
            "Results for 2025-01-13:\n",
            "Correct Predictions: 47\n",
            "Missed Failures: 16\n",
            "Incorrect Predictions: 41\n",
            "Precision: 0.5341, Recall: 0.7460, F1 Score: 0.6225\n",
            "\n",
            "Results for 2025-01-20:\n",
            "Correct Predictions: 55\n",
            "Missed Failures: 12\n",
            "Incorrect Predictions: 33\n",
            "Precision: 0.6250, Recall: 0.8209, F1 Score: 0.7097\n",
            "\n",
            "Results for 2025-01-27:\n",
            "Correct Predictions: 48\n",
            "Missed Failures: 12\n",
            "Incorrect Predictions: 40\n",
            "Precision: 0.5455, Recall: 0.8000, F1 Score: 0.6486\n",
            "\n",
            "Evaluation results saved to /content/evaluation_results_jan2025.csv\n",
            "\n",
            "Average F1 Score over all weeks: 0.6410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#thresholds individual for each weeks"
      ],
      "metadata": {
        "id": "u0dOPvCdboyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Step 9: Compare Predictions with Actual Data for Each Week with Threshold Optimization -----\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load actual data\n",
        "actual_data = pd.read_csv(\"/content/weekly_test.csv\")\n",
        "actual_data['Week'] = pd.to_datetime(actual_data['week'])\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "# Define threshold search space\n",
        "thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "\n",
        "for target_week in weeks_to_predict:\n",
        "    week_str = target_week.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Filter predictions and actuals for this week\n",
        "    week_predictions = predictions_df[predictions_df['week'] == week_str]\n",
        "    week_actuals = actual_data[actual_data['Week'] == target_week]\n",
        "\n",
        "    best_threshold = 0.0\n",
        "    best_f1 = -1.0\n",
        "    best_precision = 0.0\n",
        "    best_recall = 0.0\n",
        "    best_tp = best_fn = best_fp = 0\n",
        "\n",
        "    # Optimize threshold for best F1\n",
        "    for threshold in thresholds:\n",
        "        predicted_failures = set(week_predictions[week_predictions['predicted_probability'] > threshold]['fdr_Id'])\n",
        "        actual_failures = set(week_actuals[week_actuals['Fault_Occurred'] == 1]['fdr_Id'])\n",
        "\n",
        "        true_positives = actual_failures & predicted_failures\n",
        "        false_negatives = actual_failures - predicted_failures\n",
        "        false_positives = predicted_failures - actual_failures\n",
        "\n",
        "        precision = len(true_positives) / (len(true_positives) + len(false_positives) + 1e-9)\n",
        "        recall = len(true_positives) / (len(true_positives) + len(false_negatives) + 1e-9)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "            best_precision = precision\n",
        "            best_recall = recall\n",
        "            best_tp = len(true_positives)\n",
        "            best_fn = len(false_negatives)\n",
        "            best_fp = len(false_positives)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'week': week_str,\n",
        "        'best_threshold': best_threshold,\n",
        "        'correct_predictions': best_tp,\n",
        "        'missed_failures': best_fn,\n",
        "        'incorrect_predictions': best_fp,\n",
        "        'precision': best_precision,\n",
        "        'recall': best_recall,\n",
        "        'f1_score': best_f1\n",
        "    })\n",
        "\n",
        "    print(f\"\\nResults for {week_str}:\")\n",
        "    print(f\"Best Threshold: {best_threshold:.2f}\")\n",
        "    print(f\"Correct Predictions: {best_tp}\")\n",
        "    print(f\"Missed Failures: {best_fn}\")\n",
        "    print(f\"Incorrect Predictions: {best_fp}\")\n",
        "    print(f\"Precision: {best_precision:.4f}, Recall: {best_recall:.4f}, F1 Score: {best_f1:.4f}\")\n",
        "\n",
        "# Save evaluation results to CSV\n",
        "evaluation_df = pd.DataFrame(evaluation_results)\n",
        "evaluation_df.to_csv(\"/content/evaluation_results_jan2025.csv\", index=False)\n",
        "print(\"\\nEvaluation results saved to /content/evaluation_results_jan2025.csv\")\n",
        "\n",
        "# ---- NEW PART: Calculate and print average F1 Score ----\n",
        "average_f1 = evaluation_df['f1_score'].mean()\n",
        "print(f\"\\nAverage F1 Score over all weeks: {average_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiYcQpX9a4Tk",
        "outputId": "1e26d022-a53b-4156-ba4b-42ab49567b89"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for 2025-01-06:\n",
            "Best Threshold: 0.19\n",
            "Correct Predictions: 49\n",
            "Missed Failures: 7\n",
            "Incorrect Predictions: 57\n",
            "Precision: 0.4623, Recall: 0.8750, F1 Score: 0.6049\n",
            "\n",
            "Results for 2025-01-13:\n",
            "Best Threshold: 0.19\n",
            "Correct Predictions: 53\n",
            "Missed Failures: 10\n",
            "Incorrect Predictions: 53\n",
            "Precision: 0.5000, Recall: 0.8413, F1 Score: 0.6272\n",
            "\n",
            "Results for 2025-01-20:\n",
            "Best Threshold: 0.33\n",
            "Correct Predictions: 53\n",
            "Missed Failures: 14\n",
            "Incorrect Predictions: 27\n",
            "Precision: 0.6625, Recall: 0.7910, F1 Score: 0.7211\n",
            "\n",
            "Results for 2025-01-27:\n",
            "Best Threshold: 0.41\n",
            "Correct Predictions: 40\n",
            "Missed Failures: 20\n",
            "Incorrect Predictions: 18\n",
            "Precision: 0.6897, Recall: 0.6667, F1 Score: 0.6780\n",
            "\n",
            "Evaluation results saved to /content/evaluation_results_jan2025.csv\n",
            "\n",
            "Average F1 Score over all weeks: 0.6578\n"
          ]
        }
      ]
    }
  ]
}