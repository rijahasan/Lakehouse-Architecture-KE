{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307d9d65-e229-460c-89fd-f0a7faa8264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType\n",
    "\n",
    "# connection strings \n",
    "CATALOG_URI = \"http://nessie:19120/api/v1\"  # Nessie Server URI\n",
    "WAREHOUSE = \"s3://warehouse/\"               # Minio Address to Write to\n",
    "STORAGE_URI = \"http://172.18.0.3:9000\"     # Minio IP address from docker inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb4a1fe-a9f5-4475-98b3-57fb78332f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-591f59ea-ac60-43e3-9173-a97db8e8a2ab;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#utils;2.24.8 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.24.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.24.8 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.24.8 in central\n",
      ":: resolution report :: resolve 649ms :: artifacts dl 34ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.24.8 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.24.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-591f59ea-ac60-43e3-9173-a97db8e8a2ab\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/20ms)\n",
      "25/02/06 05:00:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    }
   ],
   "source": [
    "# connecting with pyspark \n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('billing')\n",
    "        # Include necessary packages\n",
    "        .set(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,software.amazon.awssdk:bundle:2.24.8,software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "        # Enable Iceberg and Nessie extensions\n",
    "    # org.postgresql:postgresql:42.7.3,\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', CATALOG_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Set Minio as the S3 endpoint for Iceberg storage\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', STORAGE_URI)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    ")\n",
    "\n",
    "\n",
    "# creating the connection \n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa040293-2edb-406c-9d20-99d8dc945a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+\n",
      "|           namespace|           tableName|isTemporary|\n",
      "+--------------------+--------------------+-----------+\n",
      "|EnergyConsumption...|EnergyConsumption...|      false|\n",
      "|             billing|    billing_data_raw|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_network_and_...|fact_network_and_...|      false|\n",
      "|fact_network_and_...|fact_network_and_...|      false|\n",
      "|       fault_tickets|fault_tickets_dat...|      false|\n",
      "|        feedermaster|feedermaster_data...|      false|\n",
      "|       feedervoltage|feedervoltage_dat...|      false|\n",
      "|           pmtmaster|  pmtmaster_data_raw|      false|\n",
      "|        power_report|power_report_data...|      false|\n",
      "|            recovery|   recovery_data_raw|      false|\n",
      "|               sales|      sales_data_raw|      false|\n",
      "+--------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm the connection, if it shows the tables, then its working \n",
    "\n",
    "spark.sql(\"SHOW TABLES IN nessie\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c1c5fd8-9302-44f2-a08c-fd3df1a610f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Bank Account Number=1, Contract Account (Partner-Independent Data)=400000001080, Document Date='01.01.2024', Changed On='00000000', Document Type='PY', Reference Specifications from Contract=30000008, Posting date in the document='30.12.2023', IBC='NORTH NAZIMABAD', Rate Category='Commercial with Fixed Charges TOD', Region='C10', Cycle Day=11, Ord-Ind-PSC (OIP)='ORD', Tariff='A2-', Value Date='30.12.2023', Bank Number of Other Bank Key=2106, Reference specifications for bank details='S1', Bank clearing account='AYC015', Cheque Number=None, Additional info=420016572054, Payment Lot='P2401011646M', Item number in a payment lot=2656, Resetting type=None, Reset Document=None, Calendar Year/Month='DEC 2023', No. of Stubs=1, Cash Amount=52091.0, Cheque Amount=0.0, Total Amount=52091.0, Auxillary Cash Amount=0.0, Auxillary Cheque Amount=0.0, Total Auxillary Amount=0.0, Overall Total=52091.0, SD Suspense Amount=None, Cash Suspense Amount=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datapath of all tables \n",
    "\n",
    "billing_data = spark.read.table(\"nessie.billing.billing_data_raw\")\n",
    "fault_tickets = spark.read.table(\"nessie.fault_tickets.fault_tickets_data_raw\")\n",
    "energy_consumption = spark.read.table(\"nessie.EnergyConsumptionFeederwise.EnergyConsumptionFeederwise_data_raw\")\n",
    "feeder_master = spark.read.table(\"nessie.feedermaster.feedermaster_data_raw\")\n",
    "voltage = spark.read.table(\"nessie.feedervoltage.feedervoltage_data_raw\")\n",
    "pmt_master = spark.read.table(\"nessie.pmtmaster.pmtmaster_data_raw\")\n",
    "power_report = spark.read.table(\"nessie.power_report.power_report_data_raw\")\n",
    "recovery_data = spark.read.table(\"nessie.recovery.recovery_data_raw\")\n",
    "\n",
    "# confirm if the paths are correct \n",
    "billing_data.head()\n",
    "fault_tickets.head()\n",
    "energy_consumption.head()\n",
    "feeder_master.head()\n",
    "voltage.head()\n",
    "pmt_master.head()\n",
    "power_report.head()\n",
    "recovery_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24073190-5632-41ba-a1ea-98787c745760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional libraries\n",
    "from pyspark.sql.functions import to_date, split, col, min, max, when\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# cleaning the data\n",
    "\n",
    "\n",
    "class InitClean:\n",
    "    def __init__(self, path):\n",
    "        self.dataframe = path\n",
    "\n",
    "    def drop_duplicates(self):\n",
    "        \"\"\"\n",
    "        returns a new DataFrame with entire duplicate rows removed.\n",
    "        \"\"\"\n",
    "        self.dataframe = self.dataframe.dropDuplicates()\n",
    "        return self\n",
    "\n",
    "    def replace_garbage(self):\n",
    "        \"\"\"\n",
    "        changes the column type to allow null even\n",
    "        if there were no null values previously.\n",
    "        \"\"\"\n",
    "        invalid_values = ['[NULL]', '-', 'Not assigned', 'NULL', 'null']\n",
    "        self.dataframe = self.dataframe.replace(invalid_values, None)\n",
    "        return self\n",
    "\n",
    "    def convert_to_date(self, columns: list):\n",
    "        \"\"\"\n",
    "        converts the column type to date, however lists the values as yyyy-mm-dd,\n",
    "        date operations can work on this type. To change the visualisation format\n",
    "        use date_format(to_date(col(column_name), \"dd.MM.yyyy\"), \"dd-MM-yyyy\")\n",
    "        although, date_format changes the data type to string but it can be used\n",
    "        for visualization only.\n",
    "        \"\"\"\n",
    "        for column in columns:\n",
    "            self.dataframe = self.dataframe.withColumn(\n",
    "                column, to_date(col(column)), \"dd.MM.yyyy\")\n",
    "\n",
    "        for column in columns:\n",
    "            data_type = self.dataframe.schema[column].dataType\n",
    "            if isinstance(data_type, DateType):\n",
    "                print(\n",
    "                    f\"Text to date conversion successful for column: {column}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def break_date_time(self, columns: list):\n",
    "        \"\"\"\n",
    "        withColumn returns a new DataFrame by adding a column\n",
    "        or replacing the existing column that has the same name.\n",
    "        \"\"\"\n",
    "        for column in columns:\n",
    "            if column in self.dataframe.columns:\n",
    "                self.dataframe = self.dataframe \\\n",
    "                    .withColumn(f\"{column}_Date\", split(col(column), \" \").getItem(0))\n",
    "                self.convert_to_date([f\"{column}_Date\"])\n",
    "                self.dataframe = self.dataframe \\\n",
    "                    .withColumn(f\"{column}_Time\", concat(\n",
    "                        split(split(col(column), \" \").getItem(\n",
    "                            1), \":\").getItem(0), lit(\":\"),\n",
    "                        split(split(col(column), \" \").getItem(\n",
    "                            1), \":\").getItem(1), lit(\":\"),\n",
    "                        regexp_replace(split(split(col(column), \" \").getItem(\n",
    "                            1), \":\").getItem(2), \"\\\\s.*\", \"\"),\n",
    "                        lit(\" \"),\n",
    "                        regexp_replace(col(column), \".*\\\\s([AP]M).*\", \"$1\")\n",
    "                    ))\n",
    "        return self\n",
    "\n",
    "    def drop_empty_columns(self):\n",
    "        \"\"\"\n",
    "        drops columns that are entirely null or empty across all rows.\n",
    "\n",
    "        Returns:\n",
    "        self: modified DataFrame with empty columns removed\n",
    "        \"\"\"\n",
    "        # get columns with all null values\n",
    "        empty_columns = [\n",
    "            column for column in self.dataframe.columns\n",
    "            if self.dataframe.filter(col(column).isNotNull()).count() == 0\n",
    "        ]\n",
    "\n",
    "        # drop the identified empty columns\n",
    "        if empty_columns:\n",
    "            print(f\"Dropping empty columns: {empty_columns}\")\n",
    "            self.dataframe = self.dataframe.drop(*empty_columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def adjust_column_types(self):\n",
    "        \"\"\"\n",
    "        automatically adjusts column data types based on the min and max values in each column.\n",
    "        handles NULL values appropriately and maintains data consistency.\n",
    "        \"\"\"\n",
    "        for column in self.dataframe.columns:\n",
    "            try:\n",
    "                # get current data type\n",
    "                current_type = self.dataframe.schema[column].dataType\n",
    "\n",
    "                # skip if column is StringType\n",
    "                if isinstance(current_type, StringType):\n",
    "                    continue\n",
    "\n",
    "                # first ensure nulls are handled properly\n",
    "                df_clean = self.dataframe.withColumn(\n",
    "                    column,\n",
    "                    when(col(column).isin(\n",
    "                        ['[NULL]', 'NULL', 'null', '-']), None)\n",
    "                    .otherwise(col(column))\n",
    "                )\n",
    "\n",
    "                # for non-double columns, determine the best type\n",
    "                stats = df_clean.select(\n",
    "                    col(column).cast(DoubleType()).alias(column)\n",
    "                ).agg(\n",
    "                    min(column).alias(\"min_val\"),\n",
    "                    max(column).alias(\"max_val\")\n",
    "                ).collect()[0]\n",
    "\n",
    "                min_val, max_val = stats[\"min_val\"], stats[\"max_val\"]\n",
    "\n",
    "                # skip if both values are None\n",
    "                if min_val is None and max_val is None:\n",
    "                    continue\n",
    "\n",
    "                # determine appropriate type based on value range\n",
    "                if all(x is not None for x in [min_val, max_val]):\n",
    "                    if all(isinstance(x, (int, float)) for x in [min_val, max_val]):\n",
    "                        if min_val >= -9223372036854775808 and max_val <= 9223372036854775807:\n",
    "                            new_type = LongType()\n",
    "                        else:\n",
    "                            new_type = DoubleType()\n",
    "\n",
    "                        # apply the new type\n",
    "                        self.dataframe = df_clean.withColumn(\n",
    "                            column,\n",
    "                            col(column).cast(new_type)\n",
    "                        )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not process column {column}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        \"\"\"\n",
    "        returns the cleaned dataframe.\n",
    "        \"\"\"\n",
    "        return self.dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdab9bf-f6fb-419c-9d32-14a206598aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver code \n",
    "\n",
    "# Functions: \n",
    "# drop_duplicates, replace_garbage, convert_to_date, break_date_time, drop_empty_columns, adjust_column_types\n",
    "\n",
    "# Databases \n",
    "# billing_data, fault_tickets, energy_consumption_feeder_wise, feeder_master, feeder_voltage, pmt_master, power_report, recovery_data \n",
    "\n",
    "# Billing Table\n",
    "billing_cleaned = InitClean(billing_data) \\\n",
    "    .drop_duplicates() \\\n",
    "    .convert_to_date([\"Posting date in the document\", \"Due Date (Print Doc)\", \"Issue Date (Print Doc's Posting Date)\"]) \\\n",
    "    .replace_garbage() \\\n",
    "    .drop_empty_columns() \\\n",
    "    .adjust_column_types()\n",
    "\n",
    "# Energy Comsumption Feeder Wise \n",
    "energy_con_fw_cleaned = InitClean(energy_consumption) \\\n",
    "    .drop_duplicates() \\\n",
    "    .convert_to_date([\"Sentout Date\"]) \\\n",
    "    .replace_garbage() \\\n",
    "    .adjust_column_types() \\\n",
    "    .drop_empty_columns()\n",
    "\n",
    "# Cleaned Fault Tickets  \n",
    "# convert_to_time: columns that are being broken into date and time, and AreaSecureTAT, TrueCallerTAT\n",
    "# checks for if a column is empty or null then skip \n",
    "fault_tickets_clean = InitClean(fault_tickets)\\\n",
    "    .drop_duplicates() \\\n",
    "    .replace_garbage() \\\n",
    "    .drop_empty_columns() \\\n",
    "    .adjust_column_types() \\\n",
    "    .break_date_time([\"Ticket Created at\", \"Notification Time\", \"CompletedAt\", \"AreaSecuredTime\", \"TrueCallerTime\"]) \\\n",
    "\n",
    "# Cleaned Feeder Master Data\n",
    "feeder_mas_cleaned = InitClean(feeder_master) \\\n",
    "    .drop_duplicates() \\\n",
    "    .replace_garbage() \\\n",
    "    .drop_empty_columns() \\\n",
    "    .adjust_column_types()\n",
    "\n",
    "# Cleaned Voltage Data\n",
    "# Need to convert apply convert_to_time to Time table \n",
    "voltage_cleaned = InitClean(voltage) \\\n",
    "    .drop_duplicates() \\\n",
    "    .replace_garbage() \\\n",
    "    .drop_empty_columns() \\\n",
    "    .adjust_column_types()\n",
    "\n",
    "# Cleaned PMT Master Data\n",
    "pmt_mast_cleaned = InitClean(pmt_master) \\\n",
    "    .drop_duplicates() \\\n",
    "    .replace_garbage() \\\n",
    "    .drop_empty_columns() \\\n",
    "    .adjust_column_types()\n",
    "\n",
    "# Cleaned Power Report Data \n",
    "# need to apply convert_to_time to Time column \n",
    "power_rep_cleaned = InitClean(power_report) \\\n",
    "    .drop_duplicates() \\\n",
    "    .replace_garbage() \\\n",
    "    .drop_empty_columns() \\\n",
    "    .adjust_column_types()\n",
    "\n",
    "# Cleaned Recovery Data\n",
    "recovery_cleaned = InitClean(recovery_data) \\\n",
    "    .drop_duplicates() \\\n",
    "    .replace_garbage() \\\n",
    "    .drop_empty_columns() \\\n",
    "    .adjust_column_types()\n",
    "\n",
    "# to check the results\n",
    "cleaned_billing.dtypes\n",
    "cleand_ecfw.dtypes\n",
    "fault_tickets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c198e81a-53ad-4d46-862b-0a2b8ed62732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bank Account Number', 'int'),\n",
       " ('Contract Account (Partner-Independent Data)', 'bigint'),\n",
       " ('Document Date', 'string'),\n",
       " ('Changed On', 'string'),\n",
       " ('Document Type', 'string'),\n",
       " ('Reference Specifications from Contract', 'int'),\n",
       " ('Posting date in the document', 'string'),\n",
       " ('IBC', 'string'),\n",
       " ('Rate Category', 'string'),\n",
       " ('Region', 'string'),\n",
       " ('Cycle Day', 'int'),\n",
       " ('Ord-Ind-PSC (OIP)', 'string'),\n",
       " ('Tariff', 'string'),\n",
       " ('Value Date', 'string'),\n",
       " ('Bank Number of Other Bank Key', 'int'),\n",
       " ('Reference specifications for bank details', 'string'),\n",
       " ('Bank clearing account', 'string'),\n",
       " ('Cheque Number', 'int'),\n",
       " ('Additional info', 'bigint'),\n",
       " ('Payment Lot', 'string'),\n",
       " ('Item number in a payment lot', 'int'),\n",
       " ('Resetting type', 'int'),\n",
       " ('Reset Document', 'bigint'),\n",
       " ('Calendar Year/Month', 'string'),\n",
       " ('No. of Stubs', 'int'),\n",
       " ('Cash Amount', 'double'),\n",
       " ('Cheque Amount', 'double'),\n",
       " ('Total Amount', 'double'),\n",
       " ('Auxillary Cash Amount', 'double'),\n",
       " ('Auxillary Cheque Amount', 'double'),\n",
       " ('Total Auxillary Amount', 'double'),\n",
       " ('Overall Total', 'double'),\n",
       " ('SD Suspense Amount', 'string'),\n",
       " ('Cash Suspense Amount', 'double')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop SparkSession\n",
    "# spark.stop()\n",
    "recovery_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129dbc6-730a-473d-ab7b-0329a9ff289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUTURE WORK \n",
    "\n",
    "\n",
    "# from pyspark.sql.functions import col, to_date, regexp_replace\n",
    "# from dateutil.parser import parse\n",
    "# from pyspark.sql.types import DateType, StringType\n",
    "\n",
    "# def auto_detect_date_columns(df):\n",
    "#     \"\"\"\n",
    "#     Automatically detect potential date columns based on content\n",
    "#     \"\"\"\n",
    "#     date_columns = []\n",
    "#     for column, dtype in df.dtypes:\n",
    "#         # Only check string columns\n",
    "#         if dtype == 'string':\n",
    "#             # Sample first few rows to check date pattern\n",
    "#             sample = df.select(column).limit(10).rdd.flatMap(lambda x: x).collect()\n",
    "            \n",
    "#             for value in sample:\n",
    "#                 if value and isinstance(value, str):\n",
    "#                     try:\n",
    "#                         # Try parsing the value as a date\n",
    "#                         parsed_date = parse(value, fuzzy=False)\n",
    "#                         date_columns.append(column)\n",
    "#                         break\n",
    "#                     except:\n",
    "#                         continue\n",
    "    \n",
    "#     return date_columns\n",
    "\n",
    "# def intelligent_date_conversion(df, metadata_table=None):\n",
    "#     \"\"\"\n",
    "#     Intelligently convert date columns with metadata tracking\n",
    "    \n",
    "#     :param df: Input DataFrame\n",
    "#     :param metadata_table: Optional tracking table for previous conversions\n",
    "#     :return: Converted DataFrame\n",
    "#     \"\"\"\n",
    "#     # Detect potential date columns\n",
    "#     potential_date_columns = auto_detect_date_columns(df)\n",
    "    \n",
    "#     # If metadata table exists, filter out already converted columns\n",
    "#     if metadata_table:\n",
    "#         converted_columns = metadata_table.select('column_name').rdd.flatMap(lambda x: x).collect()\n",
    "#         potential_date_columns = [col for col in potential_date_columns if col not in converted_columns]\n",
    "    \n",
    "#     # Attempt conversion for each potential date column\n",
    "#     for column in potential_date_columns:\n",
    "#         # Try multiple common date formats\n",
    "#         date_formats = [\n",
    "#             'dd.MM.yyyy', \n",
    "#             'MM.dd.yyyy', \n",
    "#             'yyyy.MM.dd', \n",
    "#             'dd-MM-yyyy', \n",
    "#             'MM-dd-yyyy', \n",
    "#             'yyyy-MM-dd'\n",
    "#         ]\n",
    "        \n",
    "#         for date_format in date_formats:\n",
    "#             try:\n",
    "#                 # Attempt conversion with current format\n",
    "#                 df = df.withColumn(\n",
    "#                     column, \n",
    "#                     to_date(col(column), date_format)\n",
    "#                 )\n",
    "                \n",
    "#                 # If successful, break out of format loop\n",
    "#                 break\n",
    "#             except:\n",
    "#                 continue\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Automatic Date Detection:\n",
    "\n",
    "# Uses dateutil.parser to intelligently detect potential date columns\n",
    "# Checks only string columns\n",
    "# Samples first few rows to determine date-like content\n",
    "\n",
    "\n",
    "# Intelligent Conversion:\n",
    "\n",
    "# Tries multiple common date formats\n",
    "# Handles different international date formats\n",
    "# Skips already converted columns if metadata tracking is used\n",
    "\n",
    "\n",
    "# Metadata Tracking:\n",
    "\n",
    "# Optional tracking of already converted columns\n",
    "# Prevents redundant conversions in future runs\n",
    "\n",
    "\n",
    "# Additional Recommendations:\n",
    "\n",
    "# Create a metadata table to track:\n",
    "\n",
    "# Columns converted\n",
    "# Conversion date\n",
    "# Conversion format\n",
    "\n",
    "\n",
    "# Add logging to track conversion attempts and successes\n",
    "# Handle edge cases like:\n",
    "\n",
    "# Columns with mixed date formats\n",
    "# Partially invalid date columns\n",
    "\n",
    "\n",
    "\n",
    "# Considerations:\n",
    "\n",
    "# Performance overhead for large datasets\n",
    "# Might need fine-tuning based on your specific data patterns\n",
    "# Fuzzy parsing can sometimes misinterpret non-date strings\n",
    "\n",
    "\n",
    "\n",
    "# FUTURE WORK \n",
    "#                           - NEED TO INCLUDE ERROR HANDLING/ TRY EXCEPT CONDITIONS \n",
    "#                           - NEED TO PERFORM UNIT TESTING AND CHECKS FOR EVERY CONDITION, EG: DATE CONVERSION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
