{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9147bea-bcb9-4244-97f9-514e63185e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#172.18.0.4\"\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06a0ee2-b840-4052-a045-903190dcc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URI = \"http://nessie:19120/api/v1\"  # Nessie Server URI\n",
    "WAREHOUSE = \"s3://warehouse/\"               # Minio Address to Write to\n",
    "STORAGE_URI = \"http://172.18.0.3:9000\"     # Minio IP address from docker inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eaf47da-74a7-4f76-a4c4-3c99dd6acc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('billing')\n",
    "        # Include necessary packages\n",
    "        .set(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "        .set('spark.jars', '''/opt/spark/workjars/iceberg-spark-runtime-3.5_2.12-1.5.0.jar,/opt/spark/workjars/nessie-spark-extensions-3.5_2.12-0.77.1.jar,\n",
    "        /opt/spark/workjars/bundle-2.24.8.jar,/opt/spark/workjars/url-connection-client-2.24.8.jar''')\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', CATALOG_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', STORAGE_URI)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.driver.memory\", \"2g\")\n",
    "        .set(\"spark.executor.memoryOverhead\", \"512m\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"64\")\n",
    "        .set(\"spark.shuffle.spill\", \"true\")\n",
    "        .set(\"spark.shuffle.memoryFraction\", \"0.4\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1da920-e676-4759-b5f3-05e633e89f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 04:26:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# print(\"Spark Session Started\")\n",
    "# spark = SparkSession.builder.master(\"local[*]\").config(conf=conf).getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de25de0-529b-4562-bc38-e1bbbd3f5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DELETE FROM nessie.starschema.ageing_fact WHERE YEAR(BillingMonth) = 2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744d859-2693-4840-83e0-edf7476389ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aa0c324-7c66-4fcc-b834-0ecf22fd19af",
   "metadata": {},
   "source": [
    "### Adding predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8b4d158-b60c-47aa-b734-3596d7814949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(fdr_Id=99, Prediction Probability=0.8832055926322937, Predicted Fault=1, Actual Fault=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "csv = \"predictions/fault_predictions_finale.csv\"\n",
    "df = spark.read \\\n",
    "    .option(\"delimiter\",\",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d10e2-97f1-4966-b54f-ec3e7143268b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8b23a2d-91c0-40e8-b0a5-35d8430bbffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.predictions\")\n",
    "df.writeTo(\"nessie.predictions.fault_predictions_finale\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4777a4a-5719-4876-8f31-b39ef55e705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "12164a68-ae84-405d-90a9-b9d40802b890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE nessie.starschema.ageing_fact\n",
    "    DROP COLUMNS (\n",
    "`PSC Sub-Department`\n",
    "    )\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3361b-d38e-4028-a049-24fbd947effb",
   "metadata": {},
   "source": [
    "# Testing Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d211fb5e-65b0-441f-8fe0-7d65aa5adf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "import re\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import to_timestamp, regexp_replace\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e56306-1ee1-4dca-860d-2f9343f5a3a3",
   "metadata": {},
   "source": [
    "## Ageing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec32a8-d061-414b-b61b-94330016ca98",
   "metadata": {},
   "source": [
    "Testing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9395ac99-4713-4a39-93a2-31b08fe6477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:====================================>                    (14 + 8) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read through spark: 29.43564248085022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "csv = \"/mnt/HabibData/Ageing_202305.csv\"\n",
    "starttime = time.time()\n",
    "df = spark.read \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv)\n",
    "endtime = time.time()\n",
    "sparktime = endtime-starttime\n",
    "\n",
    "print(\"Time taken to read through spark:\",sparktime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "51a23783-de19-4c9a-8820-85a10204aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(r'_(\\d{6})', csv)# Extract month and year from the filename\n",
    "billing_month = match.group(1) if match else \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "14693302-3358-4d4b-a6fd-2d31ada2a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"BillingMonth\", lit(billing_month))# Add the BillingMonth column\n",
    "\n",
    "# Repartition the DataFrame\n",
    "# df_updated = df_updated.repartition(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "287d6fef-a054-4087-81cf-28ef2cfc2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------------+---------------------+-------------------+---------------+----------------+--------+------------+-------------+-------------+-------------+------+---+---------+---------+-----+------+----------------+----------------+----------------------+---------------+-----------+----+-----+---------+----+---------------+--------------+------+------------+--------------+------------+-------------+----+----------------+--------------+-------------------+-------------------+------------+------------------+------------+--------------+------------------+---------------+-------------------+-----------------------+-------------+----------------+-----------------------+------+-----------------+-------------------+------------+----------+---------------+---------------+------------------+------------+--------+----+-----------+------------+------------+-------------+-------------+--------------+-------------+--------------+------------+-------------+-----------------+------------------+---------------+----------------+------------+-------------+----------------+-----------------+-------------+-------------+--------------+---------+--------------+----------------+------------+---------+---------+-----------+-------+----------+-----------+--------+---------------------+------------------+-----------+--------------+--------------+----------------------+---------------+--------------+-----------+------------------+---------+-----------+------+-------+-------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+--------+--------------+------------+-------------------+-----------------+----------------------+--------------------+\n",
      "|AccountContract|Key Date|Business Partner|Legacy Account Number|Legacy Move In Date|Consumer Number|Contract Account|Contract|Move-In Date|Move-Out Date|Billing Class|Rate Category|Region|IBC|  IBCName|     PIBC|DCIBC|DC OIP|DC Rate Category|CA Creation Date|Contract Creation Date|ConsumerCounter|Postal Code| OIP|Phase|Cycle Day| MRU|Sanctioned Load|Connected Load|Status|Last DC Date|Last DC Reason|Premise Type|Customer Name| PMT|Set Aside Amount|Set Aside Code|Installement Number|Installement Amount|PSC Location|PSC Classification|PSC Ministry|PSC Department|PSC Sub-Department|Schedule Number|PSC Consumer Region|Strategic/Non-Strategic|Consumer Type|PSC Consumer IBC|Industry Classification|Agency|Last Payment Date|Last Payment Amount|Meter Number|Meter Make|Device Category|Last SIR Number|Last SIR CreatedOn|BillingMonth|BillType| BCM|NormalUnits|NormalAmount|AverageUnits|AverageAmount|AdjustedUnits|AdjustedAmount|AssessedUnits|AssessedAmount|RegularUnits|RegularAmount|IRBDetectionUnits|IRBDetectionAmount|IRBRevisedUnits|IRBRevisedAmount|CurrentUnits|CurrentAmount|12MonthsAvgUnits|12MonthsAvgAmount|UnitBilledYTD|12MonthsUnits|12MonthsAmount|FICAMonth|OpeningBalance|MigrationBalance|AmountBilled|LPSBilled|LPSWaived|BankCharges|Payment|Adjustment|IBCTransfer|WriteOff|PreviousYearAllowance|DownPaymentRequest|DownPayment|ClearingAmount|ClosingBalance|OutstandingDownPayment|SecurityDeposit|MNCVAdjustment|MNCVPayment|MNCVClearingAmount|A <=(366)|B (365)-(1)|C 0-30|D 31-60|E 61-90|F 91-120|G 121-150|H 151-180|I 181-210|J 211-240|K 241-270|L 271-300|M 301-330|N 331-360|O 361-390|P 391-420|Q 421-450|R 451-480|S 481-510|T 511-540|U 541-570|V 571-600|W 601-630|X 631-660|Y 1021-1050|Y 1051-1080|Y 1081-1110|Y 1111-1140|Y 1141-1170|Y 1171-1200|Y 1201-1230|Y 1231-1260|Y 1261-1460|Y 1461-1825|Y 661-690|Y 691-720|Y 721-750|Y 751-780|Y 781-810|Y 811-840|Y 841-870|Y 871-900|Y 901-930|Y 931-960|Y 961-990|Y 991-1020|Z >=1826|GrossBilledYTD|NetCreditYTD|12MonthsGrossBilled|12MonthsNetCredit|12MonthsAvgGrossBilled|12MonthsAvgNetCredit|\n",
      "+---------------+--------+----------------+---------------------+-------------------+---------------+----------------+--------+------------+-------------+-------------+-------------+------+---+---------+---------+-----+------+----------------+----------------+----------------------+---------------+-----------+----+-----+---------+----+---------------+--------------+------+------------+--------------+------------+-------------+----+----------------+--------------+-------------------+-------------------+------------+------------------+------------+--------------+------------------+---------------+-------------------+-----------------------+-------------+----------------+-----------------------+------+-----------------+-------------------+------------+----------+---------------+---------------+------------------+------------+--------+----+-----------+------------+------------+-------------+-------------+--------------+-------------+--------------+------------+-------------+-----------------+------------------+---------------+----------------+------------+-------------+----------------+-----------------+-------------+-------------+--------------+---------+--------------+----------------+------------+---------+---------+-----------+-------+----------+-----------+--------+---------------------+------------------+-----------+--------------+--------------+----------------------+---------------+--------------+-----------+------------------+---------+-----------+------+-------+-------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+--------+--------------+------------+-------------------+-----------------+----------------------+--------------------+\n",
      "|   400022403689|    NULL|      0102289415|                 NULL|               NULL|       HC000037|    400022403689|    NULL|        NULL|         NULL|         NULL|         NULL|    R1|149|Orangi-II|Undefined|  149|   ORD|            A1-R|        9/3/2013|                  NULL|              1|        159|NULL| NULL|     NULL|NULL|           NULL|          NULL| INACT|        NULL|          NULL|        NULL|MR. HAROON. .|NULL|            NULL|          NULL|               NULL|               NULL|        NULL|              NULL|        NULL|          NULL|              NULL|           NULL|               NULL|                   NULL|         NULL|            NULL|                   NULL|  NULL|             NULL|               NULL|        NULL|      NULL|           NULL|           NULL|              NULL|      202304|    NULL|NULL|       NULL|        NULL|        NULL|         NULL|         NULL|          NULL|         NULL|          NULL|        NULL|         NULL|             NULL|              NULL|           NULL|            NULL|        NULL|         NULL|            NULL|             NULL|         NULL|         NULL|          NULL|   202304|           0.0|             0.0|         0.0|      0.0|      0.0|          0|    0.0|       0.0|        0.0|       0|                  0.0|                 0|        0.0|           0.0|           0.0|                     0|            0.0|           0.0|        0.0|               0.0|      0.0|        0.0|   0.0|    0.0|    0.0|     0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|       0.0|     0.0|           0.0|         0.0|                0.0|              0.0|                   0.0|                 0.0|\n",
      "+---------------+--------+----------------+---------------------+-------------------+---------------+----------------+--------+------------+-------------+-------------+-------------+------+---+---------+---------+-----+------+----------------+----------------+----------------------+---------------+-----------+----+-----+---------+----+---------------+--------------+------+------------+--------------+------------+-------------+----+----------------+--------------+-------------------+-------------------+------------+------------------+------------+--------------+------------------+---------------+-------------------+-----------------------+-------------+----------------+-----------------------+------+-----------------+-------------------+------------+----------+---------------+---------------+------------------+------------+--------+----+-----------+------------+------------+-------------+-------------+--------------+-------------+--------------+------------+-------------+-----------------+------------------+---------------+----------------+------------+-------------+----------------+-----------------+-------------+-------------+--------------+---------+--------------+----------------+------------+---------+---------+-----------+-------+----------+-----------+--------+---------------------+------------------+-----------+--------------+--------------+----------------------+---------------+--------------+-----------+------------------+---------+-----------+------+-------+-------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+--------+--------------+------------+-------------------+-----------------+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "84afe94b-b92b-455c-b706-7c1d8be2975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "import re\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import to_timestamp, regexp_replace\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_date, year, month, sum as _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0d8a3-c80c-44cd-b796-0c9cb399efb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2e524ee1-11f0-48ff-a7d6-b3cd9d258bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_invalid_strings_with_null(df):\n",
    "    # Iterate over all columns in the DataFrame\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        # If the column is of string type, replace invalid values with null\n",
    "        if dtype == 'string':\n",
    "            df = df.withColumn(\n",
    "                col_name,\n",
    "                F.when(\n",
    "                    (F.col(col_name) == '.') | \n",
    "                    (F.col(col_name) == '-') | \n",
    "                    (F.col(col_name) == 'Undefined') | \n",
    "                    (F.col(col_name) == 'Not found'),\n",
    "                    F.lit(None)  # Replace with null\n",
    "                ).otherwise(F.col(col_name))  # Keep the original value if valid\n",
    "            )    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c3bdf45d-55e8-427b-89b7-66ee35b49979",
   "metadata": {},
   "outputs": [],
   "source": [
    "    columns_to_remove = [\n",
    "        'A <=(366)', 'B (365)-(1)', 'C 0-30', 'D 31-60', 'E 61-90', 'F 91-120', 'G 121-150',\n",
    "        'H 151-180', 'I 181-210', 'J 211-240', 'K 241-270', 'L 271-300', 'M 301-330', 'N 331-360',\n",
    "        'O 361-390', 'P 391-420', 'Q 421-450', 'R 451-480', 'S 481-510', 'T 511-540', 'U 541-570',\n",
    "        'V 571-600', 'W 601-630', 'X 631-660', 'Y 1021-1050', 'Y 1051-1080', 'Y 1081-1110', 'Y 1111-1140',\n",
    "        'Y 1141-1170', 'Y 1171-1200', 'Y 1201-1230', 'Y 1231-1260', 'Y 1261-1460', 'Y 1461-1825',\n",
    "        'Y 661-690', 'Y 691-720', 'Y 721-750', 'Y 751-780', 'Y 781-810', 'Y 811-840', 'Y 841-870',\n",
    "        'Y 871-900', 'Y 901-930', 'Y 931-960', 'Y 961-990', 'Y 991-1020', 'Z >=1826', 'Key Date',\n",
    "        'Legacy Account Number', 'Legacy Move In Date', 'Contract', 'Move-In Date', 'Move-Out Date',\n",
    "        'PIBC', 'DCIBC', 'DC OIP', 'DC Rate Category', 'CA Creation Date', 'Contract Creation Date',\n",
    "        'ConsumerCounter', 'Last SIR Number', 'Last SIR CreatedOn', 'AverageUnits',\n",
    "        'AverageAmount', 'AdjustedUnits', 'AdjustedAmount', 'AssessedUnits', 'AssessedAmount', 'FICAMonth',\n",
    "        'OpeningBalance', 'MigrationBalance', 'BankCharges', 'WriteOff', 'PreviousYearAllowance',\n",
    "        'DownPaymentRequest', 'DownPayment', 'OutstandingDownPayment', 'MNCVAdjustment', 'MNCVPayment',\n",
    "        'MNCVClearingAmount', 'Set Aside Amount', 'Set Aside Code', 'Installement Number', 'Installement Amount',\n",
    "        'Agency', 'Schedule Number', 'PSC Consumer IBC', 'Adjustment', 'IBCTransfer', 'ClearingAmount', ''\n",
    "      'PSC Location','PSC Department','PSC Classification','PSC Ministry','PSC Sub-Department'\n",
    "        ]\n",
    "    df_reduced = df.drop(*columns_to_remove)    # Drop the columns from the DataFrame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0dde2533-cfc2-4b5c-a74e-c456c586de95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 61\n"
     ]
    }
   ],
   "source": [
    "num_columns = len(df_reduced.columns)\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bec786b-3f4d-4512-a028-0e863d56f653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4345786"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "35376b4d-c714-4865-afa2-a7f7492cf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_reduced.filter(df_reduced.Status == 'ACT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6fa126e-e10d-4fde-b1b2-bdbbcf0b7698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3542316"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fea06e00-67e5-4b68-9df7-57679e65b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.drop('Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8060717e-256c-4fc9-8dac-0f3d6f3e2825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 65\n"
     ]
    }
   ],
   "source": [
    "num_columns = len(df_filtered.columns)\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9dc33a4f-1c52-4425-8fe0-5deac6ca8278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AccountContract', 'string'),\n",
       " ('Business Partner', 'string'),\n",
       " ('Consumer Number', 'string'),\n",
       " ('Contract Account', 'bigint'),\n",
       " ('Billing Class', 'string'),\n",
       " ('Rate Category', 'string'),\n",
       " ('Region', 'string'),\n",
       " ('IBC', 'string'),\n",
       " ('IBCName', 'string'),\n",
       " ('Postal Code', 'string'),\n",
       " ('OIP', 'string'),\n",
       " ('Phase', 'string'),\n",
       " ('Cycle Day', 'string'),\n",
       " ('MRU', 'string'),\n",
       " ('Sanctioned Load', 'double'),\n",
       " ('Connected Load', 'double'),\n",
       " ('Last DC Date', 'string'),\n",
       " ('Last DC Reason', 'int'),\n",
       " ('Premise Type', 'string'),\n",
       " ('Customer Name', 'string'),\n",
       " ('PMT', 'int'),\n",
       " ('PSC Location', 'string'),\n",
       " ('PSC Classification', 'string'),\n",
       " ('PSC Ministry', 'string'),\n",
       " ('PSC Department', 'string'),\n",
       " ('PSC Sub-Department', 'string'),\n",
       " ('PSC Consumer Region', 'string'),\n",
       " ('Strategic/Non-Strategic', 'string'),\n",
       " ('Consumer Type', 'string'),\n",
       " ('Industry Classification', 'string'),\n",
       " ('Last Payment Date', 'string'),\n",
       " ('Last Payment Amount', 'double'),\n",
       " ('Meter Number', 'string'),\n",
       " ('Meter Make', 'string'),\n",
       " ('Device Category', 'string'),\n",
       " ('BillingMonth', 'string'),\n",
       " ('BillType', 'string'),\n",
       " ('BCM', 'string'),\n",
       " ('NormalUnits', 'double'),\n",
       " ('NormalAmount', 'double'),\n",
       " ('RegularUnits', 'double'),\n",
       " ('RegularAmount', 'double'),\n",
       " ('IRBDetectionUnits', 'double'),\n",
       " ('IRBDetectionAmount', 'double'),\n",
       " ('IRBRevisedUnits', 'double'),\n",
       " ('IRBRevisedAmount', 'double'),\n",
       " ('CurrentUnits', 'double'),\n",
       " ('CurrentAmount', 'double'),\n",
       " ('12MonthsAvgUnits', 'double'),\n",
       " ('12MonthsAvgAmount', 'double'),\n",
       " ('UnitBilledYTD', 'double'),\n",
       " ('12MonthsUnits', 'double'),\n",
       " ('12MonthsAmount', 'double'),\n",
       " ('AmountBilled', 'double'),\n",
       " ('LPSBilled', 'double'),\n",
       " ('LPSWaived', 'double'),\n",
       " ('Payment', 'double'),\n",
       " ('ClosingBalance', 'double'),\n",
       " ('SecurityDeposit', 'double'),\n",
       " ('GrossBilledYTD', 'double'),\n",
       " ('NetCreditYTD', 'double'),\n",
       " ('12MonthsGrossBilled', 'double'),\n",
       " ('12MonthsNetCredit', 'double'),\n",
       " ('12MonthsAvgGrossBilled', 'double'),\n",
       " ('12MonthsAvgNetCredit', 'double')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "352d1a0e-de52-4cb0-b97d-dd4100ea7e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated = df_filtered.withColumn('Last Payment Date', to_timestamp(col('Last Payment Date'), 'dd-MMM-yy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51b00254-4269-4919-bb4d-b1553409159c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimestampType()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated.schema[\"Last Payment Date\"].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c3500c99-6a2a-43b9-a56e-3c29f27680ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|  Last Payment Date|\n",
      "+-------------------+\n",
      "|2023-01-04 00:00:00|\n",
      "|2023-05-08 00:00:00|\n",
      "|2023-05-08 00:00:00|\n",
      "|2023-02-06 00:00:00|\n",
      "|2022-05-06 00:00:00|\n",
      "|2023-05-08 00:00:00|\n",
      "|2023-05-30 00:00:00|\n",
      "|2023-05-05 00:00:00|\n",
      "|2023-05-30 00:00:00|\n",
      "|2023-05-08 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") #important warna error\n",
    "#CA Creation Date (MM/DD/YYYY), Contract Creation Date(DD/MM/YYYY), Last DC Date (DD-mon-YYYY) (same as last payment)\n",
    "df_updated.filter(df_updated[\"Last Payment Date\"].isNotNull()).select(\"Last Payment Date\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a32b2ea7-06b7-44fc-b600-82f2efea9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULL values\n",
    "df_updated = replace_invalid_strings_with_null(df_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9ef613e7-5138-4569-93ec-96205cc6247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===========================================>             (13 + 4) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------+----------------+-------------+-------------+------+---+-------+-----------+---+-----+---------+---+---------------+--------------+------------+--------------+------------+-------------+---+-------------------+-----------------------+-------------+-----------------------+-----------------+-------------------+------------+----------+---------------+------------+--------+---+-----------+------------+------------+-------------+-----------------+------------------+---------------+----------------+------------+-------------+----------------+-----------------+-------------+-------------+--------------+------------+---------+---------+-------+--------------+---------------+--------------+------------+-------------------+-----------------+----------------------+--------------------+\n",
      "|AccountContract|Business Partner|Consumer Number|Contract Account|Billing Class|Rate Category|Region|IBC|IBCName|Postal Code|OIP|Phase|Cycle Day|MRU|Sanctioned Load|Connected Load|Last DC Date|Last DC Reason|Premise Type|Customer Name|PMT|PSC Consumer Region|Strategic/Non-Strategic|Consumer Type|Industry Classification|Last Payment Date|Last Payment Amount|Meter Number|Meter Make|Device Category|BillingMonth|BillType|BCM|NormalUnits|NormalAmount|RegularUnits|RegularAmount|IRBDetectionUnits|IRBDetectionAmount|IRBRevisedUnits|IRBRevisedAmount|CurrentUnits|CurrentAmount|12MonthsAvgUnits|12MonthsAvgAmount|UnitBilledYTD|12MonthsUnits|12MonthsAmount|AmountBilled|LPSBilled|LPSWaived|Payment|ClosingBalance|SecurityDeposit|GrossBilledYTD|NetCreditYTD|12MonthsGrossBilled|12MonthsNetCredit|12MonthsAvgGrossBilled|12MonthsAvgNetCredit|\n",
      "+---------------+----------------+---------------+----------------+-------------+-------------+------+---+-------+-----------+---+-----+---------+---+---------------+--------------+------------+--------------+------------+-------------+---+-------------------+-----------------------+-------------+-----------------------+-----------------+-------------------+------------+----------+---------------+------------+--------+---+-----------+------------+------------+-------------+-----------------+------------------+---------------+----------------+------------+-------------+----------------+-----------------+-------------+-------------+--------------+------------+---------+---------+-------+--------------+---------------+--------------+------------+-------------------+-----------------+----------------------+--------------------+\n",
      "+---------------+----------------+---------------+----------------+-------------+-------------+------+---+-------+-----------+---+-----+---------+---+---------------+--------------+------------+--------------+------------+-------------+---+-------------------+-----------------------+-------------+-----------------------+-----------------+-------------------+------------+----------+---------------+------------+--------+---+-----------+------------+------------+-------------+-----------------+------------------+---------------+----------------+------------+-------------+----------------+-----------------+-------------+-------------+--------------+------------+---------+---------+-------+--------------+---------------+--------------+------------+-------------------+-----------------+----------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_updated.filter(df_updated[\"Region\"] == \"Undefined\").limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec18862-7d98-49aa-9539-4c5b4fab110b",
   "metadata": {},
   "source": [
    "#### IRB Amount = IRBRevisedAmount+IRBDetectionAmount\n",
    "#### IRB Units = IRBRevisedUnits+IRBDetectionUnits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f05f772e-10ce-47c5-a3fc-bfa942ad19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Update df_updated by adding two new columns IRBAmount and IRBUnits\n",
    "df_updated = df_updated.withColumn(\n",
    "    \"IRBAmount\", F.col(\"IRBRevisedAmount\") + F.col(\"IRBDetectionAmount\")\n",
    ").withColumn(\n",
    "    \"IRBUnits\", F.col(\"IRBRevisedUnits\") + F.col(\"IRBDetectionUnits\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "34dab59f-fcd3-4d9b-9aa9-4f9d4c801840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------------------+------------------+----------------+\n",
      "|CurrentAmount|RegularAmount|         IRBAmount|IRBDetectionAmount|IRBRevisedAmount|\n",
      "+-------------+-------------+------------------+------------------+----------------+\n",
      "|     14643.39|    113594.53|         -98951.14|              4.61|       -98955.75|\n",
      "|      2528.52|      3716.89|          -1188.37|           5115.37|        -6303.74|\n",
      "|    -20001.69|       1899.4|         -21901.09|           1367.44|       -23268.53|\n",
      "|      45330.7|     12995.33|32335.370000000003|           42764.0|       -10428.63|\n",
      "|     15137.15|      1757.29|          13379.86|           6214.08|         7165.78|\n",
      "+-------------+-------------+------------------+------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows where the values are not null and not zero for the specified columns\n",
    "df_updated.select(\"CurrentAmount\",\"RegularAmount\",\"IRBAmount\", \"IRBDetectionAmount\", \"IRBRevisedAmount\") \\\n",
    "    .filter(\n",
    "        (F.col(\"IRBAmount\").isNotNull()) & \n",
    "        (F.col(\"IRBAmount\") != 0) & \n",
    "        (F.col(\"IRBDetectionAmount\").isNotNull()) & \n",
    "        (F.col(\"IRBDetectionAmount\") != 0) & \n",
    "        (F.col(\"IRBRevisedAmount\").isNotNull()) & \n",
    "        (F.col(\"IRBRevisedAmount\") != 0)\n",
    "    ) \\\n",
    "    .limit(5) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1142d383-54da-4ce6-a2ea-37ef7b5fc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_IRBRevisedAmount = df_updated.filter(F.col(\"IRBRevisedAmount\") < 0)\n",
    "\n",
    "# # Show the rows with negative IRBRevisedAmount\n",
    "# negative_IRBRevisedAmount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53b698d4-b574-4159-97d7-6dd7d238906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = 'ageing_cleaned' \n",
    "# spark.sql(f\"CREATE NAMESPACE nessie.{table}\").show()\n",
    "# df_updated.writeTo(f\"nessie.{table}.{table}_data_raw\").createOrReplace()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e0854cf-bf28-4545-885b-6c47d0bd07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP table nessie.ageing_cleaned\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae14159b-c30c-4a33-8647-946a593e5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   # # df_cleaned.printSchema()\n",
    "   #  # df_cleaned.show()\n",
    "   #  table = table+'_cleaned' \n",
    "   #  df = df.repartition(200)    # Repartition the DataFrame\n",
    "   #  try:\n",
    "   #      if table in [r.namespace for r  in result]:\n",
    "   #          logging.info(f\"Table {table} exists, proceeding with appending data.\")\n",
    "   #          df.writeTo(f\"nessie.{table}.{table}_data_raw\").append()\n",
    "   #      else:\n",
    "   #          logging.info(f\"Table {table} does not exist, creating one\")\n",
    "   #          spark.sql(f\"CREATE NAMESPACE nessie.{table}\").show()\n",
    "   #          df.writeTo(f\"nessie.{table}.{table}_data_raw\").createOrReplace()        \n",
    "   #  except Exception as e:\n",
    "   #      logging.info(f\"Data could not be added: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b606764-18aa-4034-821d-789460ce411b",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "89b02db7-c67a-4bff-af40-d54a7b2e9a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, year, month, sum as _sum\n",
    "\n",
    "# Assuming `df` is your original flat dataframe\n",
    "    # billing_month is between April and September\n",
    "\n",
    "# Parse Last Payment Date to extract month and year for grouping\n",
    "df_updated = df_updated.withColumn(\"LastPaymentDate\", col(\"Last Payment Date\").cast(\"date\"))\n",
    "df_updated = df_updated.withColumn(\"PaymentYear\", year(col(\"LastPaymentDate\")))\n",
    "df_updated = df_updated.withColumn(\"PaymentMonth\", month(col(\"LastPaymentDate\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8de07158-5c42-40c4-8195-0c7a249c8d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+---------------+----------------+-------------+-------------+------+---+---------+-----------+----+-----+---------+----+---------------+--------------+------------+--------------+------------+--------------+----+-------------------+-----------------------+-------------+-----------------------+-----------------+-------------------+------------+----------+---------------+------------+--------+----+-----------+------------+------------+-------------+-----------------+------------------+---------------+----------------+------------+-------------+----------------+-----------------+-------------+-------------+--------------+------------+---------+---------+-------+--------------+---------------+--------------+------------+-------------------+-----------------+----------------------+--------------------+---------+--------+---------------+-----------+------------+----+-----+\n",
      "|     AccountContract|Business Partner|Consumer Number|Contract Account|Billing Class|Rate Category|Region|IBC|  IBCName|Postal Code| OIP|Phase|Cycle Day| MRU|Sanctioned Load|Connected Load|Last DC Date|Last DC Reason|Premise Type| Customer Name| PMT|PSC Consumer Region|Strategic/Non-Strategic|Consumer Type|Industry Classification|Last Payment Date|Last Payment Amount|Meter Number|Meter Make|Device Category|BillingMonth|BillType| BCM|NormalUnits|NormalAmount|RegularUnits|RegularAmount|IRBDetectionUnits|IRBDetectionAmount|IRBRevisedUnits|IRBRevisedAmount|CurrentUnits|CurrentAmount|12MonthsAvgUnits|12MonthsAvgAmount|UnitBilledYTD|12MonthsUnits|12MonthsAmount|AmountBilled|LPSBilled|LPSWaived|Payment|ClosingBalance|SecurityDeposit|GrossBilledYTD|NetCreditYTD|12MonthsGrossBilled|12MonthsNetCredit|12MonthsAvgGrossBilled|12MonthsAvgNetCredit|IRBAmount|IRBUnits|LastPaymentDate|PaymentYear|PaymentMonth|Year|Month|\n",
      "+--------------------+----------------+---------------+----------------+-------------+-------------+------+---+---------+-----------+----+-----+---------+----+---------------+--------------+------------+--------------+------------+--------------+----+-------------------+-----------------------+-------------+-----------------------+-----------------+-------------------+------------+----------+---------------+------------+--------+----+-----------+------------+------------+-------------+-----------------+------------------+---------------+----------------+------------+-------------+----------------+-----------------+-------------+-------------+--------------+------------+---------+---------+-------+--------------+---------------+--------------+------------+-------------------+-----------------+----------------------+--------------------+---------+--------+---------------+-----------+------------+----+-----+\n",
      "|40004322120334078832|      0104327001|       LC446428|    400043221203|         NULL|         NULL|    R1|149|Orangi-II|        159|NULL| NULL|     NULL|NULL|           NULL|          NULL|        NULL|          NULL|       HOUSE|MUHAMMAD IMRAN|NULL|               NULL|                   NULL|         NULL|                   NULL|             NULL|               NULL|        NULL|      NULL|           NULL|      202305|    NULL|NULL|       NULL|        NULL|        NULL|         NULL|             NULL|              NULL|           NULL|            NULL|        NULL|         NULL|            NULL|             NULL|         NULL|         NULL|          NULL|        NULL|     NULL|     NULL|   NULL|          NULL|           NULL|          NULL|        NULL|               NULL|             NULL|                  NULL|                NULL|     NULL|    NULL|           NULL|       NULL|        NULL|2023|    5|\n",
      "+--------------------+----------------+---------------+----------------+-------------+-------------+------+---+---------+-----------+----+-----+---------+----+---------------+--------------+------------+--------------+------------+--------------+----+-------------------+-----------------------+-------------+-----------------------+-----------------+-------------------+------------+----------+---------------+------------+--------+----+-----------+------------+------------+-------------+-----------------+------------------+---------------+----------------+------------+-------------+----------------+-----------------+-------------+-------------+--------------+------------+---------+---------+-------+--------------+---------------+--------------+------------+-------------------+-----------------+----------------------+--------------------+---------+--------+---------------+-----------+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add billing year and billing month \n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming the 'billing_month' column is in the form YYYYMM as an integer\n",
    "df_updated = df_updated.withColumn(\n",
    "    \"Year\", F.substring(F.col(\"billingmonth\").cast(\"string\"), 1, 4).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"Month\", F.substring(F.col(\"billingmonth\").cast(\"string\"), 5, 2).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_updated.limit(1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "235dad06-4ddf-4933-af24-549c2c49bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|billingmonth|Year|Month|\n",
      "+------------+----+-----+\n",
      "|  2023-04-01|2023|    4|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_updated.select(\"billingmonth\",\"Year\",\"Month\").limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "72aef8fe-7546-40a1-9943-1f2607fafbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_updated.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "909efa88-d98a-4cdb-bb3a-178cd0b11817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keepbillingmonth in date format for laying out charts\n",
    "from pyspark.sql.functions import to_date, col, lpad, lit, concat\n",
    "\n",
    "# Overwrite BillingMonth with a proper date (e.g., 2023-04-01)\n",
    "df_updated = df_updated.withColumn(\n",
    "    \"BillingMonth\",\n",
    "    to_date(concat(lpad(col(\"BillingMonth\").cast(\"string\"), 6, \"0\"), lit(\"01\")), \"yyyyMMdd\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14668321-eeb4-4227-a6f0-abf12ec70e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE nessie.starschema.ageing_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2018e49d-d084-43b5-86a4-784edf77de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to append first batch:  46.322553396224976\n"
     ]
    }
   ],
   "source": [
    "# 1. Ageing_Fact\n",
    "# save all 65 columns\n",
    "# spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.starschema\")\n",
    "start = time.time()\n",
    "df_updated.writeTo(\"nessie.starschema.ageing_fact\").createOrReplace()\n",
    "print(\"Time taken to append first batch: \",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62e4979-6e66-4d69-9f8e-50b6a5f64669",
   "metadata": {},
   "source": [
    "#### Merge with crm's consumer dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5a2d8-db3b-4887-b2cd-dfa58b1250ec",
   "metadata": {},
   "source": [
    "#### if ageingdata.month>3 and ageingdata.month<10 only valid for months of apr-sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a717a5c3-7ed5-484b-b44d-ef53fbe34274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if 4 <= int(billing_month[4:6]) <= 9:\n",
    "    print(\"yes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9566b-3145-4ca7-8806-5e822620aa3a",
   "metadata": {},
   "source": [
    "##### Consumer Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ef290018-4701-47a0-a113-425d29ed075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crm\n",
    "# Total complaints\n",
    "# Most Occured Subject \n",
    "\n",
    "# Average Complaint Resolution Time (hrs)\n",
    "# oms\n",
    "# Average Fault Duration Time (hrs)\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "#for actual use it will be the data that we just read and cleaned\n",
    "ageing_new_data = df_updated\n",
    "#for testing read from nessie\n",
    "# ageing_new_data = spark.read.table(\"nessie.starschema.ageing_fact\")\n",
    "# TEMP\n",
    "crm_consumer_dim_ = spark.read.table(\"nessie.starschema.crm_consumer_dim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f995edce-b22c-46cb-91aa-b57c8632cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Consumer_Dim\n",
    "# 2. IBC_Dim: Group by IBCCode and other IBC-related fields\n",
    "# ageing_new_data = df_updated\n",
    "# Perform the inner join between ageing_new_data and crm_consumer_dim_ on BillingMonth, Year, and AccountContract\n",
    "consumer_ageing_crm_dim = ageing_new_data.alias(\"ageing\").join(\n",
    "    crm_consumer_dim_.alias(\"crm\"), \n",
    "    (F.col(\"ageing.AccountContract\") == F.col(\"crm.AccountContract\")) &\n",
    "    (F.col(\"ageing.Month\") == F.col(\"crm.Month\")) &\n",
    "    (F.col(\"ageing.Year\") == F.col(\"crm.Year\")),\n",
    "    how='inner'\n",
    ").groupBy(\"ageing.AccountContract\",\"ageing.Month\",\"ageing.Year\").agg(\n",
    "    # First value of relevant Ageing columns (from ageing_new_data)\n",
    "    F.first(\"ageing.BillingMonth\").alias(\"Billing Month\"),\n",
    "    F.first(\"ageing.Business Partner\").alias(\"Business_Partner\"),\n",
    "    F.first(\"ageing.Consumer Type\").alias(\"ConsumerType\"),\n",
    "    F.first(\"ageing.Customer Name\").alias(\"ConsumerName\"),\n",
    "    F.first(\"ageing.Postal Code\").alias(\"PostalCode\"),\n",
    "    F.first(\"ageing.Billing Class\").alias(\"BillingClass\"),\n",
    "    F.first(\"ageing.Phase\").alias(\"Phase\"),\n",
    "    F.first(\"ageing.OIP\").alias(\"OIP\"),\n",
    "    F.first(\"ageing.Premise Type\").alias(\"Premise_Type\"),\n",
    "    F.first(\"ageing.IBCName\").alias(\"IBCName\"),\n",
    "    F.first(\"ageing.IBC\").alias(\"IBC_Code\"),\n",
    "    F.first(\"ageing.BCM\").alias(\"Billing Charge Mode (BCM)\"),\n",
    "    # First value of relevant crm columns (from crm_consumer_dim)\n",
    "    # F.first(\"crm.Month\").alias(\"Month\"), not needed cuz already contained in group by\n",
    "    # F.first(\"crm.Year\").alias(\"Year\"),\n",
    "    F.first(\"crm.StarCustomer\").alias(\"StarCustomer\"),\n",
    "\n",
    "    # Not taking aggregates from ageing because each account contract is unique in a month\n",
    "    F.first(\"ageing.IRBAmount\").alias(\"IRB Amount\"),\n",
    "    F.first(\"ageing.IRBUnits\").alias(\"IRB Units\"),\n",
    "    F.first(\"ageing.Sanctioned Load\").alias(\"Sanctioned Load\"),\n",
    "    F.first(\"ageing.Connected Load\").alias(\"Connected Load\"),\n",
    "    F.first(\"ageing.CurrentAmount\").alias(\"Current Amount\"),\n",
    "    F.first(\"ageing.CurrentUnits\").alias(\"Current Units\"),\n",
    "    \n",
    "    # Aggregated values from CRM\n",
    "    F.first(\"crm.Complaint Counts (Apr-Sep_23-24)\").alias(\"Complaint Counts\"),\n",
    "    F.first(\"crm.Average Complaint Resolution Time (mins)\").alias(\"Average Complaint Resolution Time (mins)\")\n",
    ")\n",
    "\n",
    "# Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c78926be-d452-4601-8303-553a8f0e31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                       (0 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----+-------------+----------------+--------------------+--------------------+----------+------------+----------+---+--------------------+-----------+--------+-------------------------+-------------+----------+---------+---------------+--------------+--------------+-------------+----------------+----------------------------------------+\n",
      "|     AccountContract|Month|Year|Billing Month|Business_Partner|        ConsumerType|        ConsumerName|PostalCode|BillingClass|     Phase|OIP|        Premise_Type|    IBCName|IBC_Code|Billing Charge Mode (BCM)| StarCustomer|IRB Amount|IRB Units|Sanctioned Load|Connected Load|Current Amount|Current Units|Complaint Counts|Average Complaint Resolution Time (mins)|\n",
      "+--------------------+-----+----+-------------+----------------+--------------------+--------------------+----------+------------+----------+---+--------------------+-----------+--------+-------------------------+-------------+----------+---------+---------------+--------------+--------------+-------------+----------------+----------------------------------------+\n",
      "|40000000131530000031|    5|2023|   2023-05-01|      0100000041|DOL Connection, 3...|         AQEEL AHMED|       136|           B|   POLY<20|IND|            INDUSTRY|N.Nazimabad|     124|                   Normal|       Others|       0.0|      0.0|           21.0|          56.0|     186690.47|     3751.924|               2|                      245.70000457763672|\n",
      "|40000000239730000138|    5|2023|   2023-05-01|      0100000149|      DOL Connection|ARAB EMIRATES KAR...|       082|        A1-R|POLY20TO90|ORD|OFFICE - PRIVATE ...|    Defence|     118|                   Normal|       Others|       0.0|      0.0|            2.0|          56.0|     1549548.2|    28912.124|               2|                       51.89999961853027|\n",
      "|40000000458630000347|    5|2023|   2023-05-01|      0100000368|  Bulk LT Connection|M/S TELENOR PVT L...|       094|           B|POLY20TO90|IND|                SHOP|N.Nazimabad|     124|                   Normal|       Others|     32.64|      0.0|           47.0|          47.0|     131778.44|      2067.23|               1|                      207.60000610351562|\n",
      "|40000000934030000703|    5|2023|   2023-05-01|      0100000844|DOL Connection, 3...|M/S MONIKA S EMBR...|       136|           B|   POLY<20|IND|            INDUSTRY|N.Nazimabad|     124|                   Normal|       Others|       0.0|      0.0|           11.0|          18.0|      17675.74|        357.0|               1|                       345.6000061035156|\n",
      "|40000001050030000816|    5|2023|   2023-05-01|      0100000960|DOL Connection, 3...|       MR NASIR KHAN|       136|           B|   POLY<20|IND|               HOTEL|N.Nazimabad|     124|                   Normal|       Others|       0.0|      0.0|           45.0|          45.0|     151719.01|       2352.0|               3|                        89.5999984741211|\n",
      "|40000001383630001130|    5|2023|   2023-05-01|      0100001293|      DOL Connection|M/S BLUE SQUARE I...|       136|        A2-C|    SINGLE|IND|                SHOP|N.Nazimabad|     124|                   Normal|       Others|       0.0|      0.0|            1.0|           1.0|      15020.18|        256.0|               4|                      60.900001525878906|\n",
      "|40000001535930166677|    5|2023|   2023-05-01|      0100004345|      DOL Connection|   MR FAISAL AHMED .|       136|        A1-R|   POLY<20|ORD|               HOUSE|N.Nazimabad|     124|                   Normal|Star Customer|       0.0|      0.0|            3.0|           3.0|      14710.06|        392.0|               2|                      101.99999618530273|\n",
      "|40000001570730170383|    5|2023|   2023-05-01|      0100004439|      DOL Connection|  MR . ABDUL SAMAD .|       136|        A1-R|    SINGLE|ORD|               HOUSE|N.Nazimabad|     124|                   Normal|Star Customer|       0.0|      0.0|            1.0|           1.0|      18892.66|        458.0|               1|                      262.20001220703125|\n",
      "|40000002345930183879|    5|2023|   2023-05-01|      0100005249|      DOL Connection|MR. MUHAMMAD HUSS...|       181|        A1-R|   POLY<20|ORD|                FLAT|    Defence|     118|                   Normal|       Others|       0.0|      0.0|            4.0|           4.0|       7241.66|        253.0|               3|                       134.0000025431315|\n",
      "|40000003462030179931|    5|2023|   2023-05-01|      0100134975|      DOL Connection|CAP. AKHTAR JAVAID .|       181|        A1-R|   POLY<20|ORD|               HOUSE|    Defence|     118|                   Normal|Star Customer|       0.0|      0.0|            5.0|           5.0|      50512.76|       1188.0|               1|                      221.39999389648438|\n",
      "|40000003664730176612|    5|2023|   2023-05-01|      0100087915|      DOL Connection|MR. MEHBOOB SIDRU...|       136|        A1-R|    SINGLE|ORD|                FLAT|N.Nazimabad|     124|                   Normal|       Others|       0.0|      0.0|            2.0|           2.0|      13380.09|        348.0|               1|                       73.80000305175781|\n",
      "|40000003857730180634|    5|2023|   2023-05-01|      0100135137|      DOL Connection|  MR. BILAL-USMAN. .|       181|        A1-R|   POLY<20|ORD|               HOUSE|    Defence|     118|                   Normal|       Others|       0.0|      0.0|            8.0|           8.0|      78260.84|       1798.0|               2|                      137.39999771118164|\n",
      "|40000004604130188781|    5|2023|   2023-05-01|      0100132883|      DOL Connection|MRS. FARZANA SHAU...|       181|        A1-R|   POLY<20|ORD|               HOUSE|    Defence|     118|                   Normal|Star Customer|       0.0|      0.0|           15.0|           1.0|     105588.04|       2354.0|               1|                                   423.0|\n",
      "|40000004778130180095|    5|2023|   2023-05-01|      0100131166|      DOL Connection|  MRS SAEEDA ANJUM .|       136|        A1-R|    SINGLE|ORD|                FLAT|N.Nazimabad|     124|                   Normal|       Others|       0.0|      0.0|            1.0|           1.0|       6920.45|        203.0|               1|                      322.79998779296875|\n",
      "|40000005008130176993|    5|2023|   2023-05-01|      0100063573|      DOL Connection|MR MUHAMMAD ASHRAF .|       136|        A1-R|    SINGLE|ORD|               HOUSE|N.Nazimabad|     124|                   Normal|Star Customer|       0.0|      0.0|            3.0|           3.0|       7972.75|        260.0|               1|                       34.20000076293945|\n",
      "+--------------------+-----+----+-------------+----------------+--------------------+--------------------+----------+------------+----------+---+--------------------+-----------+--------+-------------------------+-------------+----------+---------+---------------+--------------+--------------+-------------+----------------+----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consumer_ageing_crm_dim.limit(15).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80c1880c-d041-4eeb-a5cd-493ad9c7c060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE nessie.starschema.consumer_ageing_crm_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "10fc2e70-7638-47ec-bbb0-457cebae1f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consumer_ageing_crm_dim.writeTo(\"nessie.starschema.consumer_ageing_crm_dim\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6f723228-ba9b-4fd5-b6f6-fed110fa8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cff384-d608-4c3d-9eff-0db6c091e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### For other aggregates:\n",
    "# # 2. Consumer_Dim\n",
    "# # 2. IBC_Dim: Group by IBCCode and other IBC-related fields\n",
    "# # ageing_new_data = df_updated\n",
    "# # Perform the inner join between ageing_new_data and crm_consumer_dim_ on BillingMonth, Year, and AccountContract\n",
    "# consumer_ageing_crm_dim = ageing_new_data.alias(\"ageing\").join(\n",
    "#     crm_consumer_dim_.alias(\"crm\"), \n",
    "#     (F.col(\"ageing.AccountContract\") == F.col(\"crm.AccountContract\")) &\n",
    "#     (F.col(\"ageing.Month\") == F.col(\"crm.Month\")) &\n",
    "#     (F.col(\"ageing.Year\") == F.col(\"crm.Year\")),\n",
    "#     how='inner'\n",
    "# ).groupBy(\"ageing.AccountContract\",\"ageing.Month\",\"ageing.Year\").agg(\n",
    "#     # First value of relevant Ageing columns (from ageing_new_data)\n",
    "#     F.first(\"ageing.BillingMonth\").alias(\"Billing Month\"),\n",
    "#     F.first(\"ageing.Business Partner\").alias(\"Business_Partner\"),\n",
    "#     F.first(\"ageing.Consumer Type\").alias(\"ConsumerType\"),\n",
    "#     F.first(\"ageing.Customer Name\").alias(\"ConsumerName\"),\n",
    "#     F.first(\"ageing.Postal Code\").alias(\"PostalCode\"),\n",
    "#     F.first(\"ageing.Billing Class\").alias(\"BillingClass\"),\n",
    "#     F.first(\"ageing.Phase\").alias(\"Phase\"),\n",
    "#     F.first(\"ageing.OIP\").alias(\"OIP\"),\n",
    "#     F.first(\"ageing.Premise Type\").alias(\"Premise_Type\"),\n",
    "#     F.first(\"ageing.IBCName\").alias(\"IBCName\"),\n",
    "#     F.first(\"ageing.IBC\").alias(\"IBC_Code\"),\n",
    "#     # First value of relevant Ageing columns (from crm_consumer_dim)\n",
    "#     # F.first(\"crm.Month\").alias(\"Month\"), not needed cuz already contained in group by\n",
    "#     # F.first(\"crm.Year\").alias(\"Year\"),\n",
    "#     F.first(\"crm.StarCustomer\").alias(\"StarCustomer\"),\n",
    "\n",
    "#     # Aggregations from Ageing Data (IRB related columns)\n",
    "#     F.sum(\"ageing.IRBAmount\").alias(\"Total IRB Amount\"),\n",
    "#     F.sum(\"ageing.IRBUnits\").alias(\"Total IRB Units\"),\n",
    "#     F.avg(\"ageing.IRBAmount\").alias(\"Average IRB Amount\"),\n",
    "#     F.avg(\"ageing.IRBUnits\").alias(\"Average IRB Units\"),\n",
    "#     # Not taking aggregates from ageing because each account contract is unique in a month\n",
    "#     F.first(\"ageing.Sanctioned Load\").alias(\"Total Sanctioned Load\"),\n",
    "#     F.sum(\"ageing.Connected Load\").alias(\"Total Connected Load\"),\n",
    "#     F.avg(\"ageing.Sanctioned Load\").alias(\"Average Sanctioned Load\"),\n",
    "#     F.avg(\"ageing.Connected Load\").alias(\"Average Connected Load\"),\n",
    "    \n",
    "#     F.sum(\"ageing.CurrentAmount\").alias(\"Total Current Amount\"),\n",
    "#     F.avg(\"ageing.CurrentAmount\").alias(\"Average Current Amount\"),\n",
    "#     F.sum(\"ageing.CurrentUnits\").alias(\"Total Current Units\"),\n",
    "#     F.avg(\"ageing.CurrentUnits\").alias(\"Average Current Units\"),\n",
    "    \n",
    "#     # Aggregated values from CRM\n",
    "#     F.first(\"crm.Complaint Counts (Apr-Sep_23-24)\").alias(\"Complaint Counts\"),\n",
    "#     F.first(\"crm.Average Complaint Resolution Time (mins)\").alias(\"Average Complaint Resolution Time (mins)\")\n",
    "# )\n",
    "\n",
    "# # Show the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5f749-aa9d-4358-b62a-abfdae6fcf35",
   "metadata": {},
   "source": [
    "#### IBC Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b9004f6-8449-4422-8131-a97563897371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "#for actual use it will be the data that we just read and cleaned\n",
    "ageing_new_data = df_updated\n",
    "#TEST ONLY \n",
    "# ageing_new_data = spark.read.table(\"nessie.starschema.ageing_fact\")\n",
    "ibc_crm_dim_ = spark.read.table(\"nessie.starschema.ibc_crm_dim\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f502553-da2e-4f19-96fb-a412e7633331",
   "metadata": {},
   "source": [
    "##### With just crm, 33 ibcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69c2cfeb-6a94-4490-a754-2734effe61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Consumer_Dim\n",
    "# 2. IBC_Dim: Group by IBCCode and other IBC-related fields\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Perform the inner join between ageing_new_data and crm_consumer_dim_ on BillingMonth, Year, and AccountContract\n",
    "ibc_crm_ageing_dim_33 = ageing_new_data.alias(\"ageing\").join(\n",
    "    ibc_crm_dim_.alias(\"ibc_crm\"), \n",
    "    (F.col(\"ageing.IBC\") == F.col(\"ibc_crm.IBCCode\")) &\n",
    "    (F.col(\"ageing.Month\") == F.col(\"ibc_crm.Month\")) &\n",
    "    (F.col(\"ageing.Year\") == F.col(\"ibc_crm.Year\")),\n",
    "    how='inner'\n",
    ").groupBy(\"ageing.IBC\", \"ageing.Month\", \"ageing.Year\").agg(\n",
    "    # First value of relevant Ageing columns (from ageing_new_data)\n",
    "    F.first(\"ageing.IBCName\").alias(\"IBC Name\"),\n",
    "    F.first(\"ageing.Region\").alias(\"Region\"),\n",
    "    F.first(\"ageing.BillingMonth\").alias(\"Billing Month\"),\n",
    "    \n",
    "    # Aggregations from Ageing Data (IRB related columns)\n",
    "    F.first(\"ibc_crm.Complaint Counts\").alias(\"Complaint Counts\"),\n",
    "    F.first(\"ibc_crm.Average Complaint Resolution Time (mins)\").alias(\"Average Complaint Resolution Time (mins)\"),\n",
    "    \n",
    "    F.sum(\"ageing.CurrentAmount\").alias(\"Total Current Amount\"),\n",
    "    F.avg(\"ageing.CurrentAmount\").alias(\"Average Current Amount\"),\n",
    "    F.sum(\"ageing.CurrentUnits\").alias(\"Total Current Units\"),\n",
    "    F.avg(\"ageing.CurrentUnits\").alias(\"Average Current Units\"),\n",
    "\n",
    "    F.sum(\"ageing.Sanctioned Load\").alias(\"Total Sanctioned Load\"),\n",
    "    F.avg(\"ageing.Sanctioned Load\").alias(\"Average Sanctioned Load\"),\n",
    "    F.sum(\"ageing.Connected Load\").alias(\"Total Connected Load\"),\n",
    "    F.avg(\"ageing.Connected Load\").alias(\"Average Connected Load\"),\n",
    "    \n",
    "    F.sum(\"ageing.IRBAmount\").alias(\"Total IRB Amount\"),\n",
    "    F.avg(\"ageing.IRBAmount\").alias(\"Average IRB Amount\"),\n",
    "    F.sum(\"ageing.IRBUnits\").alias(\"Total IRB_Units\"),\n",
    "    F.avg(\"ageing.IRBUnits\").alias(\"Average IRB Units\"),\n",
    "    # Aggregated values from CRM\n",
    "    # F.first(\"crm.Month\").alias(\"Month\"),\n",
    "    # F.first(\"crm.Year\").alias(\"Year\"),\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "# ibc_crm_ageing_dim_33.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f34bfc33-c862-4ac1-a5d8-aab60ff59c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP TABLE nessie.starschema.ibc_crm_ageing_dim_33\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8605d5c-443b-45f8-9159-4c3cb959cc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ibc_crm_ageing_dim_33.writeTo(\"nessie.starschema.ibc_crm_ageing_dim_33\").createOrReplace()\n",
    "# ibc_crm_ageing_dim_33.writeTo(\"nessie.starschema.ibc_crm_ageing_dim_33\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338bb93-40f1-4a1f-b61b-c7f32e59f372",
   "metadata": {},
   "source": [
    "##### Now all three (ageing crm oms) joined 4 ibcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90b6975e-d9c2-4dd8-9f74-09c3635f6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "#for actual use it will be the data that we just read and cleaned\n",
    "ageing_new_data = df_updated\n",
    "#TEST ONLY \n",
    "# ageing_new_data = spark.read.table(\"nessie.starschema.ageing_fact\")\n",
    "ibc_oms_crm_dim = spark.read.table(\"nessie.starschema.ibc_oms_crm_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b3cbdc7-7aeb-4aa8-905b-b3ce438e40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Consumer_Dim\n",
    "# 2. IBC_Dim: Group by IBCCode and other IBC-related fields\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Perform the inner join between ageing_new_data and crm_consumer_dim_ on BillingMonth, Year, and AccountContract\n",
    "ibc_all_dim_4 = ageing_new_data.alias(\"ageing\").join(\n",
    "    ibc_oms_crm_dim.alias(\"ibc_crm_oms\"), \n",
    "    (F.col(\"ageing.IBC\") == F.col(\"ibc_crm_oms.IBCCode\")) &\n",
    "    (F.col(\"ageing.Month\") == F.col(\"ibc_crm_oms.Month\")) &\n",
    "    (F.col(\"ageing.Year\") == F.col(\"ibc_crm_oms.Year\")),\n",
    "    how='inner'\n",
    ").groupBy(\"ageing.IBC\", \"ageing.Month\", \"ageing.Year\").agg(\n",
    "    # First value of relevant Ageing columns (from ageing_new_data)\n",
    "    F.first(\"ageing.IBCName\").alias(\"IBC Name\"),\n",
    "    F.first(\"ageing.Region\").alias(\"Region\"),\n",
    "    F.first(\"ageing.BillingMonth\").alias(\"Billing Month\"),\n",
    "    # Aggregations are already calculated in oms crm dim\n",
    "    F.first(\"ibc_crm_oms.Complaint Counts\").alias(\"Complaint Counts\"),\n",
    "    F.first(\"ibc_crm_oms.Average Complaint Resolution Time (mins)\").alias(\"Average Complaint Resolution Time (mins)\"),\n",
    "    F.first(\"ibc_crm_oms.Total Outages/Faults\").alias(\"Total Outages/Faults\"),\n",
    "    F.first(\"ibc_crm_oms.Most_Occurred_Fault\").alias(\"Most Occurred Fault\"),\n",
    "    F.first(\"ibc_crm_oms.Most_Occurred_Fault_Frequency\").alias(\"Most Occurred Fault Frequency\"),\n",
    "    F.first(\"ibc_crm_oms.Average Fault Turn-Around Time (mins)\").alias(\"Average Fault Turn-Around Time (mins)\"),\n",
    "    F.first(\"ibc_crm_oms.Average Fault Duration (mins)\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.first(\"ibc_crm_oms.Rain Frequency\").alias(\"Rain Frequency\"),\n",
    "\n",
    "    # Aggregations from Ageing Data (IRB related columns)\n",
    "    \n",
    "    F.sum(\"ageing.CurrentAmount\").alias(\"Total Current Amount\"),\n",
    "    F.avg(\"ageing.CurrentAmount\").alias(\"Average Current Amount\"),\n",
    "    F.sum(\"ageing.CurrentUnits\").alias(\"Total Current Units\"),\n",
    "    F.avg(\"ageing.CurrentUnits\").alias(\"Average Current Units\"),\n",
    "\n",
    "    F.sum(\"ageing.Sanctioned Load\").alias(\"Total Sanctioned Load\"),\n",
    "    F.avg(\"ageing.Sanctioned Load\").alias(\"Average Sanctioned Load\"),\n",
    "    F.sum(\"ageing.Connected Load\").alias(\"Total Connected Load\"),\n",
    "    F.avg(\"ageing.Connected Load\").alias(\"Average Connected Load\"),\n",
    "    \n",
    "    F.sum(\"ageing.IRBAmount\").alias(\"Total IRB Amount\"),\n",
    "    F.avg(\"ageing.IRBAmount\").alias(\"Average IRB Amount\"),\n",
    "    F.sum(\"ageing.IRBUnits\").alias(\"Total IRB_Units\"),\n",
    "    F.avg(\"ageing.IRBUnits\").alias(\"Average IRB Units\"),\n",
    "    # Aggregated values from CRM\n",
    "    # F.first(\"crm.Month\").alias(\"Month\"),\n",
    "    # F.first(\"crm.Year\").alias(\"Year\"),\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "# ibc_all_dim_4.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a92081f7-44ed-44fb-aa83-6921c2e2e889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE nessie.starschema.ibc_all_dim_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb968835-0066-468b-b753-df8de5c1ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ibc_all_dim_4.writeTo(\"nessie.starschema.ibc_all_dim_4\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec02e32d-a92e-45d9-8e5b-2d936350b3a8",
   "metadata": {},
   "source": [
    "#### Now IBC OMS x Ageing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0dad997b-d3a9-4c0c-90dd-97023311ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_oms_dim = spark.read.table(\"nessie.starschema.ibc_oms_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06121abf-8695-4812-a143-6a0de849340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Perform the inner join between ageing_new_data and crm_consumer_dim_ on BillingMonth, Year, and AccountContract\n",
    "ibc_oms_ageing_dim_4 = ageing_new_data.alias(\"ageing\").join(\n",
    "    ibc_oms_dim.alias(\"ibc_oms\"), \n",
    "    (F.col(\"ageing.IBCName\") == F.col(\"ibc_oms.IBC\")) & #oms does not have ibc codes\n",
    "    (F.col(\"ageing.Month\") == F.col(\"ibc_oms.Month\")) &\n",
    "    (F.col(\"ageing.Year\") == F.col(\"ibc_oms.Year\")),\n",
    "    how='inner'\n",
    ").groupBy(\"ageing.IBC\", \"ageing.Month\", \"ageing.Year\").agg(\n",
    "    # First value of relevant Ageing columns (from ageing_new_data)\n",
    "    F.first(\"ageing.IBCName\").alias(\"IBC Name\"),\n",
    "    F.first(\"ageing.Region\").alias(\"Region\"),\n",
    "    F.first(\"ageing.BillingMonth\").alias(\"Billing Month\"),\n",
    "    # Aggregations are already calculated in oms ibc dim\n",
    "    F.first(\"ibc_oms.Total Outages/Faults\").alias(\"Total Outages/Faults\"),\n",
    "    F.first(\"ibc_oms.Most_Occurred_Fault\").alias(\"Most Occurred Fault\"),\n",
    "    F.first(\"ibc_oms.Most_Occurred_Fault_Frequency\").alias(\"Most Occurred Fault Frequency\"),\n",
    "    F.first(\"ibc_oms.Average Fault Turn-Around Time (mins)\").alias(\"Average Fault Turn-Around Time (mins)\"),\n",
    "    F.first(\"ibc_oms.Average Fault Duration (mins)\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.first(\"ibc_oms.Rain Frequency\").alias(\"Rain Frequency\"),\n",
    "\n",
    "    # Aggregations from Ageing Data (IRB related columns)\n",
    "    \n",
    "    F.sum(\"ageing.CurrentAmount\").alias(\"Total Current Amount\"),\n",
    "    F.avg(\"ageing.CurrentAmount\").alias(\"Average Current Amount\"),\n",
    "    F.sum(\"ageing.CurrentUnits\").alias(\"Total Current Units\"),\n",
    "    F.avg(\"ageing.CurrentUnits\").alias(\"Average Current Units\"),\n",
    "\n",
    "    F.sum(\"ageing.Sanctioned Load\").alias(\"Total Sanctioned Load\"),\n",
    "    F.avg(\"ageing.Sanctioned Load\").alias(\"Average Sanctioned Load\"),\n",
    "    F.sum(\"ageing.Connected Load\").alias(\"Total Connected Load\"),\n",
    "    F.avg(\"ageing.Connected Load\").alias(\"Average Connected Load\"),\n",
    "    \n",
    "    F.sum(\"ageing.IRBAmount\").alias(\"Total IRB Amount\"),\n",
    "    F.avg(\"ageing.IRBAmount\").alias(\"Average IRB Amount\"),\n",
    "    F.sum(\"ageing.IRBUnits\").alias(\"Total IRB_Units\"),\n",
    "    F.avg(\"ageing.IRBUnits\").alias(\"Average IRB Units\"),\n",
    "    # Aggregated values from CRM\n",
    "    # F.first(\"crm.Month\").alias(\"Month\"),\n",
    "    # F.first(\"crm.Year\").alias(\"Year\"),\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "# ibc_oms_ageing_dim_4.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0cec2e0-6a67-4bde-8a66-7709f138ec8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE nessie.starschema.ibc_oms_ageing_dim_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fef847a9-df7f-435c-a9f0-0c57652dd4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ibc_oms_ageing_dim_4.writeTo(\"nessie.starschema.ibc_oms_ageing_dim_4\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e4428-2375-458c-8ee6-bef4782c06d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f8c3c6-3549-418e-816f-c70118646119",
   "metadata": {},
   "source": [
    "### PMT CRM Ageing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "092a7225-0ca4-4008-a28d-f42dab7ff279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "#for actual use it will be the data that we just read and cleaned\n",
    "ageing_new_data = df_updated\n",
    "#TEST ONLY \n",
    "# ageing_new_data = spark.read.table(\"nessie.starschema.ageing_fact\")\n",
    "pmt_crm_dim = spark.read.table(\"nessie.starschema.pmt_crm_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ca2ae6e-7601-4b24-a75c-d85eabf64bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Consumer_Dim\n",
    "# 2. IBC_Dim: Group by IBCCode and other IBC-related fields\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Perform the inner join between ageing_new_data and crm_consumer_dim_ on BillingMonth, Year, and AccountContract\n",
    "pmt_crm_ageing_dim = ageing_new_data.alias(\"ageing\").join(\n",
    "    pmt_crm_dim.alias(\"pmt_crm\"), \n",
    "    (F.col(\"ageing.PMT\") == F.col(\"pmt_crm.PMT\")) &\n",
    "    (F.col(\"ageing.Month\") == F.col(\"pmt_crm.Month\")) &\n",
    "    (F.col(\"ageing.Year\") == F.col(\"pmt_crm.Year\")),\n",
    "    how='inner'\n",
    ").groupBy(\"ageing.PMT\", \"ageing.Month\", \"ageing.Year\").agg(\n",
    "    # First value of relevant Ageing columns (from ageing_new_data)\n",
    "    F.first(\"pmt_crm.PMT Name\").alias(\"PMT Name\"),    \n",
    "    F.first(\"ageing.IBC\").alias(\"IBC Code\"),\n",
    "    F.first(\"ageing.IBCName\").alias(\"IBC Name\"),\n",
    "    F.first(\"ageing.Region\").alias(\"Region\"),\n",
    "    F.first(\"ageing.BillingMonth\").alias(\"Billing Month\"),\n",
    "    # Aggregations are already calculated in oms crm dim\n",
    "    F.first(\"pmt_crm.Complaint Counts\").alias(\"Complaint Counts\"),\n",
    "    F.first(\"pmt_crm.Average Complaint Resolution Time (mins)\").alias(\"Average Complaint Resolution Time (mins)\"),\n",
    "\n",
    "    # Aggregations from Ageing Data (IRB related columns)\n",
    "    \n",
    "    F.sum(\"ageing.CurrentAmount\").alias(\"Total Current Amount\"),\n",
    "    F.avg(\"ageing.CurrentAmount\").alias(\"Average Current Amount\"),\n",
    "    F.sum(\"ageing.CurrentUnits\").alias(\"Total Current Units\"),\n",
    "    F.avg(\"ageing.CurrentUnits\").alias(\"Average Current Units\"),\n",
    "\n",
    "    F.sum(\"ageing.Sanctioned Load\").alias(\"Total Sanctioned Load\"),\n",
    "    F.avg(\"ageing.Sanctioned Load\").alias(\"Average Sanctioned Load\"),\n",
    "    F.sum(\"ageing.Connected Load\").alias(\"Total Connected Load\"),\n",
    "    F.avg(\"ageing.Connected Load\").alias(\"Average Connected Load\"),\n",
    "    \n",
    "    F.sum(\"ageing.IRBAmount\").alias(\"Total IRB Amount\"),\n",
    "    F.avg(\"ageing.IRBAmount\").alias(\"Average IRB Amount\"),\n",
    "    F.sum(\"ageing.IRBUnits\").alias(\"Total IRB_Units\"),\n",
    "    F.avg(\"ageing.IRBUnits\").alias(\"Average IRB Units\"),\n",
    "    # Aggregated values from CRM\n",
    "    # F.first(\"crm.Month\").alias(\"Month\"),\n",
    "    # F.first(\"crm.Year\").alias(\"Year\"),\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "# pmt_crm_ageing_dim.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a2e74d4-bc79-4177-bf56-f4e8d48168f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE nessie.starschema.pmt_crm_ageing_dim\")\n",
    "pmt_crm_ageing_dim.writeTo(\"nessie.starschema.pmt_crm_ageing_dim\").createOrReplace()\n",
    "# pmt_crm_ageing_dim.writeTo(\"nessie.starschema.pmt_crm_ageing_dim\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f63f9aa0-4875-4740-bdba-fb49dd3d53f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a689ebd8-4f56-434f-a48e-562b245dcb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c581e09-67ed-4a43-a890-1579cc65a700",
   "metadata": {},
   "source": [
    "## CRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c5a5bdc5-2f1f-4279-af28-e5825f4a40ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.master(\"local[*]\").config(conf=conf).getOrCreate()\n",
    "# print(\"Spark Session Started\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# print(\"Spark Session Started\")\n",
    "# spark = SparkSession.builder.master(\"local[*]\").config(conf=conf).getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "433fc247-c6b1-43d4-bd87-e995e1c0ab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import lit\n",
    "csv = \"/mnt/HabibData/CRM_202301-202501.csv\"\n",
    "# match = re.search(r'_(\\d{6})', csv)# Extract month and year from the filename\n",
    "# billing_month = match.group(1) if match else \"Unknown\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "crm_df = spark.read \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237c29a9-a111-4369-800e-796fe4f6ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------------+--------------------+----------+------------+----------+------------------+------------------+------------+--------------------+--------------------+-------+-----------------+------------------+------------------+---------------------------+------------------+------------------+---------------------+--------------------+-------------------+-------+------------+-----------+----------+------+----------+---------+----+-------------------+-----------+----------+------------+----------------+--------------------+-----------------+--------+--------+---------------+----+-----+---------+---------+-----------+----------+-------------------+----------+-----------+-----------------------+----------------+--------+---+---------+-----------+-------------+---------+-------------+-------------+-------------+--------------------+--------------+---------+------+------+-------------+---------+------------+-------------+---------------+-------------+-------+------------+---------+---------+-------------------+-----------------+---------------+----------+--------------+----------------+----------+\n",
      "|     AccountContract|Ticket No.|Business_Partner|                GUID|   IBCName|PROCESS_TYPE|CREATED_BY|        CREATED_AT|         UDATETIME|LatestStatus|          Supervisor|             Lineman|  MTLNo|CompletionRemarks| CompletedDateTime|    ClosedDateTime|ClosedOnInteractionDateTime| InProcessDateTime|      OpenDateTime|ForwardedToHTDateTime|HTDispatchedDateTime|Medium Of Complaint|IBCCode|CREATED_DATE|     Source|Created By|Region|Subject_ID|Reason_ID| Key|      Subject Group|     Reason|   Subject|GroundTATHrs|    GroundTATNum|TimeElapsedCompleted|TimeElapsedClosed|  O to I|  I to C|ReOpenCountTemp|Year|Month|GroundTAT|TargetTAT|ReOpenCount|Closed Tag|       %IntervalKey| TATBusted|SDR/Not SDR|TimeElapsedAtLastReload|Contract Account|Contract|OIP|  MNCcode|Postal Code|Billing Class|Cycle Day|Rate Category|Consumer Type|Customer Name|    Customer Address|ConsumerStatus|PMT Count| Phase|   PMT|     PMT Name|Feeder ID| Feeder Name|Mobile Number|LandLine Number|Email Address|Remarks|StarCustomer|  DueDate|IssueDate|Last Payment Amount|Last Payment Date|LastBillingMode|LastDCDate|LastDCRecovery|LastSIRCreatedOn|UnitBilled|\n",
      "+--------------------+----------+----------------+--------------------+----------+------------+----------+------------------+------------------+------------+--------------------+--------------------+-------+-----------------+------------------+------------------+---------------------------+------------------+------------------+---------------------+--------------------+-------------------+-------+------------+-----------+----------+------+----------+---------+----+-------------------+-----------+----------+------------+----------------+--------------------+-----------------+--------+--------+---------------+----+-----+---------+---------+-----------+----------+-------------------+----------+-----------+-----------------------+----------------+--------+---+---------+-----------+-------------+---------+-------------+-------------+-------------+--------------------+--------------+---------+------+------+-------------+---------+------------+-------------+---------------+-------------+-------+------------+---------+---------+-------------------+-----------------+---------------+----------+--------------+----------------+----------+\n",
      "|40002933016432866064|6018670040|      0102934244|75BA21661686067EE...|Liaqatabad|        YSVR|     CSR22|18-Apr-24 21:51:36|19-Apr-24 11:05:14|      Closed|MAZHAR BILAL - LI...|MUHAMMAD ADEEL SH...|JU-3184|             Null|19-Apr-24 01:05:10|19-Apr-24 11:05:14|                       NULL|18-Apr-24 23:23:43|18-Apr-24 21:52:30|                 NULL|                NULL|     Telephone call|    127|   18-Apr-24|Call Center|     CSR22|    R4|      YS28|     YR97|NULL|Technical Complaint|Feeder Trip|Supply Off|    03:13:34|0.13442129629402|            03:13:34|         13:13:38|01:31:13|01:41:27|              0|2024|  Apr|     3.23|     4.25|          0|    Closed|Technical Complaint|Within TAT|    Non-SDR|                   3.23|    400029330164|32866064|ORD|Undefined|        144|         A1-R|        8|         A1-R|         NULL|   NASIR KHAN|PLOT NO. 944 , BL...|           ACT|        1|SINGLE|500530|MADINI MASJID|     3107|Sarafa Bazar|   3131032682|           Null|         NULL|   NULL|        NULL|5/30/2019|5/16/2019|              1,500|      29-Mar-2021|         Normal|      NULL|          NULL|            NULL|       280|\n",
      "+--------------------+----------+----------------+--------------------+----------+------------+----------+------------------+------------------+------------+--------------------+--------------------+-------+-----------------+------------------+------------------+---------------------------+------------------+------------------+---------------------+--------------------+-------------------+-------+------------+-----------+----------+------+----------+---------+----+-------------------+-----------+----------+------------+----------------+--------------------+-----------------+--------+--------+---------------+----+-----+---------+---------+-----------+----------+-------------------+----------+-----------+-----------------------+----------------+--------+---+---------+-----------+-------------+---------+-------------+-------------+-------------+--------------------+--------------+---------+------+------+-------------+---------+------------+-------------+---------------+-------------+-------+------------+---------+---------+-------------------+-----------------+---------------+----------+--------------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crm_df.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b1a72faa-70e0-4ff5-8a09-c25d5398fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULL values\n",
    "columns_to_remove = [\n",
    "      \"LastDCDate\",\n",
    "    \"LastDCRecovery\",\n",
    "    \"HTDispatchedDateTime\",\n",
    "    \"ForwardedToHTDateTime\",\n",
    "    \"Key\",\n",
    "    \"LastSIRCreatedOn\",\n",
    "    \"Remarks\",\n",
    "    \"ClosedOnInteractionDateTime\",\n",
    "    \"CompletionRemarks\",\n",
    "    \"%IntervalKey\",\n",
    "    \"MNCcode\",\n",
    "    \"Subject Group\",\n",
    "    \"Contract\",\n",
    "    \"Contract Account\",\n",
    "    \"GroundTATNum\"]# fazool hai\n",
    "    \n",
    "crm_df_reduced = crm_df.drop(*columns_to_remove)    # Drop the columns from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0a9512b-0b25-4ae1-b564-f0a851f687fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 67\n"
     ]
    }
   ],
   "source": [
    "num_columns = len(crm_df_reduced.columns)\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "49c40f75-696f-4944-ba4e-2c69f22ee94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df_reduced = crm_df_reduced.filter(crm_df_reduced.ConsumerStatus == 'ACT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7e7e4d85-3079-4342-828c-66d03dd051f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df_reduced = crm_df_reduced.drop('CustomerStatus')    # Drop the columns from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9c0ab1d9-f120-4b51-944a-22d309a1d7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AccountContract', 'string'),\n",
       " ('Business Partner', 'string'),\n",
       " ('Consumer Number', 'string'),\n",
       " ('Contract Account', 'bigint'),\n",
       " ('Billing Class', 'string'),\n",
       " ('Rate Category', 'string'),\n",
       " ('Region', 'string'),\n",
       " ('IBC', 'string'),\n",
       " ('IBCName', 'string'),\n",
       " ('Postal Code', 'string'),\n",
       " ('OIP', 'string'),\n",
       " ('Phase', 'string'),\n",
       " ('Cycle Day', 'string'),\n",
       " ('MRU', 'string'),\n",
       " ('Sanctioned Load', 'double'),\n",
       " ('Connected Load', 'double'),\n",
       " ('Last DC Date', 'string'),\n",
       " ('Last DC Reason', 'int'),\n",
       " ('Premise Type', 'string'),\n",
       " ('Customer Name', 'string'),\n",
       " ('PMT', 'int'),\n",
       " ('PSC Consumer Region', 'string'),\n",
       " ('Strategic/Non-Strategic', 'string'),\n",
       " ('Consumer Type', 'string'),\n",
       " ('Industry Classification', 'string'),\n",
       " ('Last Payment Date', 'timestamp'),\n",
       " ('Last Payment Amount', 'double'),\n",
       " ('Meter Number', 'string'),\n",
       " ('Meter Make', 'string'),\n",
       " ('Device Category', 'string'),\n",
       " ('BillingMonth', 'date'),\n",
       " ('BillType', 'string'),\n",
       " ('BCM', 'string'),\n",
       " ('NormalUnits', 'double'),\n",
       " ('NormalAmount', 'double'),\n",
       " ('RegularUnits', 'double'),\n",
       " ('RegularAmount', 'double'),\n",
       " ('IRBDetectionUnits', 'double'),\n",
       " ('IRBDetectionAmount', 'double'),\n",
       " ('IRBRevisedUnits', 'double'),\n",
       " ('IRBRevisedAmount', 'double'),\n",
       " ('CurrentUnits', 'double'),\n",
       " ('CurrentAmount', 'double'),\n",
       " ('12MonthsAvgUnits', 'double'),\n",
       " ('12MonthsAvgAmount', 'double'),\n",
       " ('UnitBilledYTD', 'double'),\n",
       " ('12MonthsUnits', 'double'),\n",
       " ('12MonthsAmount', 'double'),\n",
       " ('AmountBilled', 'double'),\n",
       " ('LPSBilled', 'double'),\n",
       " ('LPSWaived', 'double'),\n",
       " ('Payment', 'double'),\n",
       " ('ClosingBalance', 'double'),\n",
       " ('SecurityDeposit', 'double'),\n",
       " ('GrossBilledYTD', 'double'),\n",
       " ('NetCreditYTD', 'double'),\n",
       " ('12MonthsGrossBilled', 'double'),\n",
       " ('12MonthsNetCredit', 'double'),\n",
       " ('12MonthsAvgGrossBilled', 'double'),\n",
       " ('12MonthsAvgNetCredit', 'double'),\n",
       " ('IRBAmount', 'double'),\n",
       " ('IRBUnits', 'double'),\n",
       " ('LastPaymentDate', 'date'),\n",
       " ('PaymentYear', 'int'),\n",
       " ('PaymentMonth', 'int'),\n",
       " ('Year', 'int'),\n",
       " ('Month', 'int')]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ageing_new_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1ab80faa-cd8a-4692-bcdd-0e9dae27a01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AccountContract', 'decimal(20,0)'),\n",
       " ('Ticket No.', 'bigint'),\n",
       " ('Business_Partner', 'string'),\n",
       " ('GUID', 'string'),\n",
       " ('IBCName', 'string'),\n",
       " ('PROCESS_TYPE', 'string'),\n",
       " ('CREATED_BY', 'string'),\n",
       " ('CREATED_AT', 'string'),\n",
       " ('UDATETIME', 'string'),\n",
       " ('LatestStatus', 'string'),\n",
       " ('Supervisor', 'string'),\n",
       " ('Lineman', 'string'),\n",
       " ('MTLNo', 'string'),\n",
       " ('CompletionRemarks', 'string'),\n",
       " ('CompletedDateTime', 'string'),\n",
       " ('ClosedDateTime', 'string'),\n",
       " ('ClosedOnInteractionDateTime', 'string'),\n",
       " ('InProcessDateTime', 'string'),\n",
       " ('OpenDateTime', 'string'),\n",
       " ('ForwardedToHTDateTime', 'string'),\n",
       " ('HTDispatchedDateTime', 'string'),\n",
       " ('Medium Of Complaint', 'string'),\n",
       " ('IBCCode', 'string'),\n",
       " ('CREATED_DATE', 'string'),\n",
       " ('Source', 'string'),\n",
       " ('Created By', 'string'),\n",
       " ('Region', 'string'),\n",
       " ('Subject_ID', 'string'),\n",
       " ('Reason_ID', 'string'),\n",
       " ('Key', 'string'),\n",
       " ('Subject Group', 'string'),\n",
       " ('Reason', 'string'),\n",
       " ('Subject', 'string'),\n",
       " ('GroundTATHrs', 'string'),\n",
       " ('GroundTATNum', 'double'),\n",
       " ('TimeElapsedCompleted', 'string'),\n",
       " ('TimeElapsedClosed', 'string'),\n",
       " ('O to I', 'string'),\n",
       " ('I to C', 'string'),\n",
       " ('ReOpenCountTemp', 'int'),\n",
       " ('Year', 'int'),\n",
       " ('Month', 'string'),\n",
       " ('GroundTAT', 'double'),\n",
       " ('TargetTAT', 'string'),\n",
       " ('ReOpenCount', 'int'),\n",
       " ('Closed Tag', 'string'),\n",
       " ('%IntervalKey', 'string'),\n",
       " ('TATBusted', 'string'),\n",
       " ('SDR/Not SDR', 'string'),\n",
       " ('TimeElapsedAtLastReload', 'double'),\n",
       " ('Contract Account', 'bigint'),\n",
       " ('Contract', 'int'),\n",
       " ('OIP', 'string'),\n",
       " ('MNCcode', 'string'),\n",
       " ('Postal Code', 'string'),\n",
       " ('Billing Class', 'string'),\n",
       " ('Cycle Day', 'int'),\n",
       " ('Rate Category', 'string'),\n",
       " ('Consumer Type', 'string'),\n",
       " ('Customer Name', 'string'),\n",
       " ('Customer Address', 'string'),\n",
       " ('ConsumerStatus', 'string'),\n",
       " ('PMT Count', 'int'),\n",
       " ('Phase', 'string'),\n",
       " ('PMT', 'string'),\n",
       " ('PMT Name', 'string'),\n",
       " ('Feeder ID', 'string'),\n",
       " ('Feeder Name', 'string'),\n",
       " ('Mobile Number', 'string'),\n",
       " ('LandLine Number', 'string'),\n",
       " ('Email Address', 'string'),\n",
       " ('Remarks', 'string'),\n",
       " ('StarCustomer', 'string'),\n",
       " ('DueDate', 'string'),\n",
       " ('IssueDate', 'string'),\n",
       " ('Last Payment Amount', 'string'),\n",
       " ('Last Payment Date', 'string'),\n",
       " ('LastBillingMode', 'string'),\n",
       " ('LastDCDate', 'string'),\n",
       " ('LastDCRecovery', 'string'),\n",
       " ('LastSIRCreatedOn', 'string'),\n",
       " ('UnitBilled', 'string')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_reduced.count()\n",
    "crm_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4ab2b43d-e7b9-48a8-bdca-0953f35d52ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     GroundTATNum|\n",
      "+-----------------+\n",
      "| 0.13442129629402|\n",
      "|0.034247685187438|\n",
      "| 0.32202546296321|\n",
      "| 0.76576388889225|\n",
      "|  1.2027893518534|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crm_df.select('GroundTATNum').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "329c9b2d-928d-45c5-a21f-f70621d55f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df_reduced = crm_df_reduced.withColumn(\"AccountContract\", F.col(\"AccountContract\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb85c8-35c8-4ead-a1bc-3418dae3681f",
   "metadata": {},
   "source": [
    "### dividing date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ae21834c-b2a7-4060-bdd7-54820a9bc9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, col, date_format, lit, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ddfc5ec9-d020-451a-942a-e997bdf0e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------------+--------------------+----------+------------+----------+-------------------+-------------------+------------+--------------------+--------------------+-------+-------------------+-------------------+-------------------+-------------------+-------------------+-------+------------+-----------+----------+------+----------+---------+-----------+----------+------------+--------------------+-----------------+--------+--------+---------------+----+-----+---------+---------+-----------+----------+----------+-----------+-----------------------+---+-----------+-------------+---------+-------------+-------------+-------------+--------------------+--------------+---------+------+------+-------------+---------+------------+-------------+---------------+-------------+------------+---------+---------+-------------------+-----------------+---------------+----------+\n",
      "|     AccountContract|Ticket No.|Business_Partner|                GUID|   IBCName|PROCESS_TYPE|CREATED_BY|         CREATED_AT|          UDATETIME|LatestStatus|          Supervisor|             Lineman|  MTLNo|  CompletedDateTime|     ClosedDateTime|  InProcessDateTime|       OpenDateTime|Medium Of Complaint|IBCCode|CREATED_DATE|     Source|Created By|Region|Subject_ID|Reason_ID|     Reason|   Subject|GroundTATHrs|TimeElapsedCompleted|TimeElapsedClosed|  O to I|  I to C|ReOpenCountTemp|Year|Month|GroundTAT|TargetTAT|ReOpenCount|Closed Tag| TATBusted|SDR/Not SDR|TimeElapsedAtLastReload|OIP|Postal Code|Billing Class|Cycle Day|Rate Category|Consumer Type|Customer Name|    Customer Address|ConsumerStatus|PMT Count| Phase|   PMT|     PMT Name|Feeder ID| Feeder Name|Mobile Number|LandLine Number|Email Address|StarCustomer|  DueDate|IssueDate|Last Payment Amount|Last Payment Date|LastBillingMode|UnitBilled|\n",
      "+--------------------+----------+----------------+--------------------+----------+------------+----------+-------------------+-------------------+------------+--------------------+--------------------+-------+-------------------+-------------------+-------------------+-------------------+-------------------+-------+------------+-----------+----------+------+----------+---------+-----------+----------+------------+--------------------+-----------------+--------+--------+---------------+----+-----+---------+---------+-----------+----------+----------+-----------+-----------------------+---+-----------+-------------+---------+-------------+-------------+-------------+--------------------+--------------+---------+------+------+-------------+---------+------------+-------------+---------------+-------------+------------+---------+---------+-------------------+-----------------+---------------+----------+\n",
      "|40002933016432866064|6018670040|      0102934244|75BA21661686067EE...|Liaqatabad|        YSVR|     CSR22|2024-04-18 21:51:36|2024-04-19 11:05:14|      Closed|MAZHAR BILAL - LI...|MUHAMMAD ADEEL SH...|JU-3184|2024-04-19 01:05:10|2024-04-19 11:05:14|2024-04-18 23:23:43|2024-04-18 21:52:30|     Telephone call|    127|   18-Apr-24|Call Center|     CSR22|    R4|      YS28|     YR97|Feeder Trip|Supply Off|    03:13:34|            03:13:34|         13:13:38|01:31:13|01:41:27|              0|2024|  Apr|     3.23|     4.25|          0|    Closed|Within TAT|    Non-SDR|                   3.23|ORD|        144|         A1-R|        8|         A1-R|         NULL|   NASIR KHAN|PLOT NO. 944 , BL...|           ACT|        1|SINGLE|500530|MADINI MASJID|     3107|Sarafa Bazar|   3131032682|           Null|         NULL|        NULL|5/30/2019|5/16/2019|              1,500|      29-Mar-2021|         Normal|       280|\n",
      "+--------------------+----------+----------------+--------------------+----------+------------+----------+-------------------+-------------------+------------+--------------------+--------------------+-------+-------------------+-------------------+-------------------+-------------------+-------------------+-------+------------+-----------+----------+------+----------+---------+-----------+----------+------------+--------------------+-----------------+--------+--------+---------------+----+-----+---------+---------+-----------+----------+----------+-----------+-----------------------+---+-----------+-------------+---------+-------------+-------------+-------------+--------------------+--------------+---------+------+------+-------------+---------+------------+-------------+---------------+-------------+------------+---------+---------+-------------------+-----------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [  \"UDATETIME\",\n",
    "    \n",
    "    \"CompletedDateTime\",\n",
    "\"ClosedDateTime\",\n",
    " \"InProcessDateTime\",\n",
    "\"OpenDateTime\",\n",
    "\"CREATED_AT\"\n",
    "         ]:\n",
    "    # Apply the function to split the datetime\n",
    "    crm_df_reduced = crm_df_reduced.withColumn(i, to_timestamp(col(i), 'dd-MMM-yy HH:mm:ss'))\n",
    "\n",
    "\n",
    "# Show the updated dataframe\n",
    "crm_df_reduced.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "bc9aaa24-49bc-47f0-a826-d38bf9e480f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimestampType()\n"
     ]
    }
   ],
   "source": [
    "datesinMonthformat = [\"CREATED_DATE\"]\n",
    "for date in datesinMonthformat:\n",
    "    crm_df_reduced = crm_df_reduced.withColumn(date, to_timestamp(col(date), 'dd-MMM-yy'))\n",
    "    print(crm_df_reduced.schema[date].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "552b564e-dfb7-48eb-a4f6-68f9716d8ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       CREATED_DATE|\n",
      "+-------------------+\n",
      "|2024-04-18 00:00:00|\n",
      "|2024-07-17 00:00:00|\n",
      "|2024-08-21 00:00:00|\n",
      "|2023-08-21 00:00:00|\n",
      "|2023-06-22 00:00:00|\n",
      "|2023-06-22 00:00:00|\n",
      "|2024-05-02 00:00:00|\n",
      "|2023-08-21 00:00:00|\n",
      "|2024-05-16 00:00:00|\n",
      "|2023-04-06 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "#CA Creation Date (MM/DD/YYYY), Contract Creation Date(DD/MM/YYYY), Last DC Date (DD-mon-YYYY) (same as last payment)\n",
    "crm_df_reduced.filter(crm_df_reduced[\"CREATED_DATE\"].isNotNull()).select(\"CREATED_DATE\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ff83111d-687c-45df-b7f2-36095eb3e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of invalid entries before updating casting dates and duration hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "107a20df-77c3-47f5-b983-35630f1a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strings that contain both date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b9cb82ce-46e5-4087-891f-b3154e0d321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b2ac3748-6f93-40de-b1fe-e58180b7cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to convert hh:mm:ss to minutes\n",
    "def hhmmss_to_minutes(time_str):\n",
    "    try:\n",
    "        if type(time_str)!=str:\n",
    "            return time_str\n",
    "        h, m, s = map(int, time_str.split(\":\"))\n",
    "        return h * 60 + m + s / 60\n",
    "    except Exception as e:\n",
    "        return None  # In case of an error (e.g., non-time values)\n",
    "# Register the function as a UDF (User Defined Function)\n",
    "hhmmss_to_minutes_udf = udf(hhmmss_to_minutes, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8031d066-7313-428b-8a38-4c7fa672662f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------------+--------------------------+---------------+---------------+\n",
      "|GroundTATHrs(minutes)|TimeElapsedCompleted(minutes)|TimeElapsedClosed(minutes)|O to I(minutes)|I to C(minutes)|\n",
      "+---------------------+-----------------------------+--------------------------+---------------+---------------+\n",
      "|            193.56667|                    193.56667|                 793.63336|       91.21667|         101.45|\n",
      "|            49.316666|                    49.316666|                  529.4167|      11.916667|           37.4|\n",
      "|                 89.7|                         89.7|                 689.76666|      13.566667|       74.78333|\n",
      "|            116.86667|                    116.86667|                    766.45|      14.366667|     100.916664|\n",
      "|                 91.7|                         91.7|                    2025.5|       4.116667|      85.583336|\n",
      "+---------------------+-----------------------------+--------------------------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hrstomins = [\"GroundTATHrs\", \"TimeElapsedCompleted\", \"TimeElapsedClosed\",\"O to I\",\"I to C\"]\n",
    "for j in hrstomins:\n",
    "    crm_df_reduced = crm_df_reduced.withColumn(j + \"(minutes)\", hhmmss_to_minutes_udf(col(j)))\n",
    "    # df_reduced = df_reduced.withColumn(i, to_timestamp(col(i), 'dd-MMM-yy HH:mm:ss'))\n",
    "columns_to_show = [col + \"(minutes)\" for col in hrstomins]\n",
    "crm_df_reduced.select(columns_to_show).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ced230c5-4116-4b8e-b83b-bcdd1ebe1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df_reduced = crm_df_reduced.drop(*hrstomins)    # Drop the columns from the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11381a30-c3da-4d3d-bf02-0321c7338f49",
   "metadata": {},
   "source": [
    "## cleaning before handling target TAT because its string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fb1ee7f1-b55b-4714-a7ab-43983bc51b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df_reduced = replace_invalid_strings_with_null(crm_df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cb14ba12-81b2-4644-8fe4-323b068225ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_to_double(df, column_name):\n",
    "    # Convert the specified column to double (float)\n",
    "    df_updated = df.withColumn(column_name, col(column_name).cast(\"double\"))\n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cf1fbbe9-2922-427b-bc1d-3d7146984b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df_reduced = convert_column_to_double(crm_df_reduced, \"TargetTAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c5240c21-aca2-43cc-9e2c-6a9ce0f3610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoubleType()\n"
     ]
    }
   ],
   "source": [
    "print(crm_df_reduced.schema[\"TargetTAT\"].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "952d1ef6-9358-49e4-88be-7401deb0c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "doubletohours = [\n",
    "\"GroundTAT\",\n",
    "\"TargetTAT\"]\n",
    "# removed \"GroundTATNum\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "78d743ed-04a8-4da9-867f-c5d662b6449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert a double (hours) to minutes\n",
    "def hours_to_minutes(hours):\n",
    "    try:\n",
    "        return hours * 60\n",
    "    except Exception as e:\n",
    "        return None  # In case of any invalid value\n",
    "\n",
    "# Register the function as a UDF (User Defined Function)\n",
    "hours_to_minutes_udf = udf(hours_to_minutes, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "550d9234-54bd-4140-9062-6417326899bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|GroundTAT(minutes)|TargetTAT(minutes)|\n",
      "+------------------+------------------+\n",
      "|             193.8|             255.0|\n",
      "|              49.2|             300.0|\n",
      "|              89.4|              NULL|\n",
      "|             117.0|              NULL|\n",
      "|              91.8|              NULL|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in doubletohours:\n",
    "    crm_df_reduced = crm_df_reduced.withColumn(k + \"(minutes)\", hours_to_minutes_udf(col(k)))\n",
    "\n",
    "# Show the updated dataframe with the new converted column\n",
    "columns_to_show = [col + \"(minutes)\" for col in doubletohours]\n",
    "crm_df_reduced.select(columns_to_show).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f4ee119d-7752-4155-870d-56a875b859e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|TargetTAT|\n",
      "+---------+\n",
      "|     4.25|\n",
      "|      5.0|\n",
      "|     2.75|\n",
      "|      3.5|\n",
      "|      5.0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crm_df_reduced.filter(crm_df_reduced[\"TargetTAT\"].isNotNull()).select(\"TargetTAT\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d805cb22-d1ab-4695-9a98-d3ea7e5227db",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df_reduced = crm_df_reduced.drop(*doubletohours)    # Drop the columns from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ee048adf-db66-453f-83f2-0fecf340258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Month\" #alpha to num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5809b3d3-3a40-47c3-9e33-db4b03e39546",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_mapping = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "def month_name_to_number(month_name):\n",
    "    try:\n",
    "        return month_mapping.get(month_name, None)  # Return None if the month is invalid\n",
    "    except Exception as e:\n",
    "        return None\n",
    "month_name_to_number_udf = udf(month_name_to_number, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8fa9582d-87dd-46d6-a9d2-c18c3a27d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df_reduced = crm_df_reduced.withColumn('Month', month_name_to_number_udf(col('Month')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a149f84b-8537-4c38-b46e-88e6c2bb9e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Month|\n",
      "+-----+\n",
      "|    4|\n",
      "|    7|\n",
      "|    8|\n",
      "|    8|\n",
      "|    6|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crm_df_reduced.select('Month').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cf77b181-6e65-44c7-b885-cf5cd78a5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"DueDate\", \"IssueDate\" \"DD/MM/YYYY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0dc65ed6-ef2d-43a0-bdf1-7c2fe296842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimestampType()\n",
      "TimestampType()\n"
     ]
    }
   ],
   "source": [
    "datesindiffMonthformat = [\"DueDate\",\"IssueDate\"]\n",
    "for date in datesindiffMonthformat:\n",
    "    crm_df_reduced = crm_df_reduced.withColumn(date, to_timestamp(col(date), 'dd/MM/yy'))\n",
    "    print(crm_df_reduced.schema[date].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fb509288-055b-4729-af9d-55ef16f8cae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          IssueDate|\n",
      "+-------------------+\n",
      "|2019-08-05 00:00:00|\n",
      "|2019-07-05 00:00:00|\n",
      "|2019-09-05 00:00:00|\n",
      "|2019-08-05 00:00:00|\n",
      "|2019-08-05 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_show = [col for col in datesindiffMonthformat]\n",
    "crm_df_reduced.filter(crm_df_reduced[\"IssueDate\"].isNotNull()).select(\"IssueDate\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0119da4a-72cf-478e-810c-049687e7f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "def remove_commas_and_convert_to_numerical(df, column_name):\n",
    "    # Remove commas from the specified column and convert the result to float\n",
    "    df_updated = df.withColumn(column_name, regexp_replace(col(column_name), \",\", \"\").cast(\"double\"))\n",
    "    return df_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2508ac41-76b1-4916-8ed3-887e43270299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|UnitBilled|\n",
      "+----------+\n",
      "|       280|\n",
      "|       422|\n",
      "|       154|\n",
      "|       250|\n",
      "|       495|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crm_df_reduced.filter(crm_df_reduced[\"UnitBilled\"].isNotNull()).select(\"UnitBilled\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b8d0397a-5236-4586-bc27-cee9854fa912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoubleType()\n",
      "DoubleType()\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "numericalcolumnstofix = [\"UnitBilled\", \"Last Payment Amount\"]\n",
    "for r in numericalcolumnstofix:\n",
    "    crm_df_reduced = remove_commas_and_convert_to_numerical(crm_df_reduced, r)\n",
    "    print(crm_df_reduced.schema[r].dataType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dd7d33fc-b897-4fee-b05a-354003162c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|Last Payment Amount|     AccountContract|\n",
      "+-------------------+--------------------+\n",
      "|             1500.0|40002933016432866064|\n",
      "|             6878.0|40002709964332629117|\n",
      "|             4149.0|40003394143733313978|\n",
      "|             1802.0|40002814258532732331|\n",
      "|             1413.0|40002905226132829687|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crm_df_reduced.filter(crm_df_reduced[\"Last Payment Amount\"].isNotNull()).select(\"Last Payment Amount\", \"AccountContract\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064af81-b2c8-46e4-89fd-085eb47b1a49",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9672cb1b-a0d4-456d-8844-8d198526f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4f5d7a0-f33a-4df2-b700-3f8a96d129c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.starschema\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "0ffd1fbf-4101-410b-974c-f0d0665b389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df_reduced = crm_df_reduced.repartition(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f251734c-07dc-4d73-9095-65cf877a6036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".  (11 + 1) / 12]\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crm_df_reduced.writeTo(\"nessie.starschema.crm_fact\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4bdd0-22a7-451f-878d-7853ce4b1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Customer Status from crm.fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4498d-558f-4af4-bb53-113834b19be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crm\n",
    "# Total complaints\n",
    "# Most Occured Subject \n",
    "\n",
    "# Average Complaint Resolution Time (hrs)\n",
    "# oms\n",
    "# Average Fault Duration Time (hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba10a9b0-6961-4d08-9932-d31fa82d84ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "#TEST ONLY \n",
    "crm_df_reduced = spark.read.table(\"nessie.starschema.crm_fact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42bada99-015f-46c3-9815-7f7c225222c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AccountContract', 'decimal(20,0)'),\n",
       " ('Ticket No.', 'bigint'),\n",
       " ('Business_Partner', 'string'),\n",
       " ('GUID', 'string'),\n",
       " ('IBCName', 'string'),\n",
       " ('PROCESS_TYPE', 'string'),\n",
       " ('CREATED_BY', 'string'),\n",
       " ('CREATED_AT', 'timestamp'),\n",
       " ('UDATETIME', 'timestamp'),\n",
       " ('LatestStatus', 'string'),\n",
       " ('Supervisor', 'string'),\n",
       " ('Lineman', 'string'),\n",
       " ('MTLNo', 'string'),\n",
       " ('CompletedDateTime', 'timestamp'),\n",
       " ('ClosedDateTime', 'timestamp'),\n",
       " ('InProcessDateTime', 'timestamp'),\n",
       " ('OpenDateTime', 'timestamp'),\n",
       " ('Medium Of Complaint', 'string'),\n",
       " ('IBCCode', 'string'),\n",
       " ('CREATED_DATE', 'timestamp'),\n",
       " ('Source', 'string'),\n",
       " ('Created By', 'string'),\n",
       " ('Region', 'string'),\n",
       " ('Subject_ID', 'string'),\n",
       " ('Reason_ID', 'string'),\n",
       " ('Reason', 'string'),\n",
       " ('Subject', 'string'),\n",
       " ('ReOpenCountTemp', 'int'),\n",
       " ('Year', 'int'),\n",
       " ('Month', 'int'),\n",
       " ('ReOpenCount', 'int'),\n",
       " ('Closed Tag', 'string'),\n",
       " ('TATBusted', 'string'),\n",
       " ('SDR/Not SDR', 'string'),\n",
       " ('TimeElapsedAtLastReload', 'double'),\n",
       " ('OIP', 'string'),\n",
       " ('Postal Code', 'string'),\n",
       " ('Billing Class', 'string'),\n",
       " ('Cycle Day', 'int'),\n",
       " ('Rate Category', 'string'),\n",
       " ('Consumer Type', 'string'),\n",
       " ('Customer Name', 'string'),\n",
       " ('Customer Address', 'string'),\n",
       " ('ConsumerStatus', 'string'),\n",
       " ('PMT Count', 'int'),\n",
       " ('Phase', 'string'),\n",
       " ('PMT', 'string'),\n",
       " ('PMT Name', 'string'),\n",
       " ('Feeder ID', 'string'),\n",
       " ('Feeder Name', 'string'),\n",
       " ('Mobile Number', 'string'),\n",
       " ('LandLine Number', 'string'),\n",
       " ('Email Address', 'string'),\n",
       " ('StarCustomer', 'string'),\n",
       " ('DueDate', 'timestamp'),\n",
       " ('IssueDate', 'timestamp'),\n",
       " ('Last Payment Amount', 'double'),\n",
       " ('Last Payment Date', 'string'),\n",
       " ('LastBillingMode', 'string'),\n",
       " ('UnitBilled', 'double'),\n",
       " ('GroundTATHrs(minutes)', 'float'),\n",
       " ('TimeElapsedCompleted(minutes)', 'float'),\n",
       " ('TimeElapsedClosed(minutes)', 'float'),\n",
       " ('O to I(minutes)', 'float'),\n",
       " ('I to C(minutes)', 'float'),\n",
       " ('GroundTATNum(minutes)', 'float'),\n",
       " ('GroundTAT(minutes)', 'float'),\n",
       " ('TargetTAT(minutes)', 'float')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_reduced.count()\n",
    "crm_df_reduced.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8f4088-ca3b-4890-8ec5-93d056962b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----+--------------------------------+----------------------------------------+------------+\n",
      "|AccountContract|Month|Year|Complaint Counts (Apr-Sep_23-24)|Average Complaint Resolution Time (mins)|StarCustomer|\n",
      "+---------------+-----+----+--------------------------------+----------------------------------------+------------+\n",
      "|           NULL|    5|2023|                               0|                      197.22461641751804|        NULL|\n",
      "|           NULL|    8|2023|                               0|                      147.53877491853675|        NULL|\n",
      "|           NULL|    8|2024|                               0|                      367.84687323868275|        NULL|\n",
      "|           NULL|    9|2023|                               0|                       193.8000005086263|        NULL|\n",
      "|   400000646456|    8|2023|                               1|                                   342.0|      Others|\n",
      "+---------------+-----+----+--------------------------------+----------------------------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2. Consumer_Dim\n",
    "crm_consumer_dim_ = crm_df_reduced.groupBy(\"AccountContract\", \"Month\", \"Year\").agg(\n",
    "    F.count(\"AccountContract\").alias(\"Complaint Counts (Apr-Sep_23-24)\"),  # Count number of records per AccountContract\n",
    "    F.avg(\"GroundTAT(minutes)\").alias(\"Average Complaint Resolution Time (mins)\"), F.first(\"StarCustomer\").alias(\"StarCustomer\")\n",
    ")\n",
    "crm_consumer_dim_.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a63c35-690e-423a-b5cd-9407a4f0e002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"DROP TABLE nessie.starschema.crm_consumer_dim\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d735556-2112-4b97-84ce-c179ea1529f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crm_consumer_dim_.writeTo(\"nessie.starschema.crm_consumer_dim\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2f3ce780-98e2-44d2-83a0-28f61984fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ONLY \n",
    "ageing_new_data = spark.read.table(\"nessie.starschema.ageing_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ab3eb-d866-4c65-a8c6-bdc7178ef7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ageing_new_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bccf45f5-70a6-413a-8125-43063c5cce3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consumer_ageing_crm_dim.writeTo(\"nessie.starschema.consumer_ageing_crm_dim\").createOrReplace()\n",
    "# would be\n",
    "# consumer_ageing_crm_dim.writeTo(\"nessie.starschema.consumer_ageing_crm_dim\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc42cc4-9652-4c1b-8c25-369de5b86799",
   "metadata": {},
   "source": [
    "### IBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "42ec61a9-83d8-4018-8989-ab107b66ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ONLY \n",
    "crm_df_reduced = spark.read.table(\"nessie.starschema.crm_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4d15e9c4-f1e8-4415-a0b5-5f20414f80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Grouping and aggregating the IBC data\n",
    "crm_ibc_dim = crm_df_reduced.groupBy(\"IBCName\", \"IBCCode\",\"Year\", \"Month\").agg(\n",
    "    F.count(\"IBCName\").alias(\"Complaint Counts\"),  # Count the occurrences of IBCName\n",
    "    F.avg(\"GroundTAT(minutes)\").alias(\"Average Complaint Resolution Time (mins)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "73c2f38e-255e-4bba-b864-b52a5124e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_ibc_dim = crm_ibc_dim.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2f1eee85-faf2-4d1c-a87a-f6155d69b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. OMS fact\n",
    "crm_ibc_dim = crm_ibc_dim.filter(\n",
    "    (col(\"Month\").isNotNull()) & (col(\"Year\").isNotNull() & (col(\"IBCName\").isNotNull()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "17d0ff9f-355e-4360-9468-e16d995832a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS nessie.starschema.ibc_crm_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "58175190-e50b-4f03-afee-bca6248b29b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crm_ibc_dim.writeTo(\"nessie.starschema.ibc_crm_dim\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7d856-5f61-42b7-b571-095ca87802ac",
   "metadata": {},
   "source": [
    "#### CRM PMT dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a15d70bf-8148-4a14-8116-93c1620301c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pmt_crm_dim = crm_df_reduced.groupBy(\"PMT\", \"PMT Name\",\"Year\", \"Month\").agg(\n",
    "    F.count(\"IBCName\").alias(\"Complaint Counts\"),  # Count the occurrences of IBCName\n",
    "    F.avg(\"GroundTAT(minutes)\").alias(\"Average Complaint Resolution Time (mins)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6ebb4598-b9b9-42db-866f-c7b6edfd8ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS nessie.starschema.pmt_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c77a50bd-5a4b-472e-9cba-ca845cec53ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crm_pmt_dim.writeTo(\"nessie.starschema.pmt_crm_dim\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9933f3f2-9012-48b5-81b3-80ad5bb0a329",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a883ce3-111f-4dba-af37-e55c44f91aa0",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0ce443b6-377d-4835-954d-dd38b9724213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by the date part of CREATED_DATE\n",
    "crm_date_dim = crm_df_reduced.groupBy(F.to_date(\"CREATED_DATE\").alias(\"Date\"),\"IBCCode\",\"IBCName\").agg(\n",
    "    F.count(\"CREATED_DATE\").alias(\"Complaint Counts\"),\n",
    "    F.avg(\"GroundTAT(minutes)\").alias(\"Average Complaint Resolution Time (mins)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e4afa70d-4f56-4258-bc6b-109438351937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:=====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+----------------+----------------------------------------+\n",
      "|      Date|IBCCode|IBCName|Complaint Counts|Average Complaint Resolution Time (mins)|\n",
      "+----------+-------+-------+----------------+----------------------------------------+\n",
      "|2023-08-14|    138| Garden|             312|                      177.06153851289017|\n",
      "+----------+-------+-------+----------------+----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crm_date_dim.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fc14cc7e-85a6-4224-834e-ca5126ed43c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS nessie.starschema.crm_date_dim\")\n",
    "crm_date_dim.writeTo(\"nessie.starschema.crm_date_dim\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a64681-4329-4379-834d-8e4bea2dbc0a",
   "metadata": {},
   "source": [
    "#### Feeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6a4d02bc-0a94-4c6b-81ed-44d8b59c9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by the date part of CREATED_DATE\n",
    "crm_feeder_dim = crm_df_reduced.groupBy(F.to_date(\"CREATED_DATE\").alias(\"Date\"), \"Feeder ID\", \"Feeder Name\").agg(\n",
    "    F.count(\"Feeder ID\").alias(\"Complaint Counts\"),\n",
    "    F.avg(\"GroundTAT(minutes)\").alias(\"Average Complaint Resolution Time (mins)\")\n",
    ")\n",
    "crm_feeder_dim.writeTo(\"nessie.starschema.crm_feeder_dim\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17ca1ac0-9154-456a-b7c2-67b199f8f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0fef6d-2528-4640-9d61-342aebef41f5",
   "metadata": {},
   "source": [
    "## OMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2baa5-4a9a-4136-a95e-b49c8197cd3d",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e72021b4-9b3b-40b4-a0bf-0f6b68ed21ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 56816)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(conf=conf).getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b8aa728-105d-4bae-b856-9dc0d8297787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0663a9c6-df3a-4ca9-b304-f76db5a0bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b999bd7-5a89-4e1c-aded-352750336b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import when, col\n",
    "csv = \"/mnt/HabibData/OMSRequestedData.csv\"\n",
    "# match = re.search(r'_(\\d{6})', csv)# Extract month and year from the filename\n",
    "# billing_month = match.group(1) if match else \"Unknown\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "oms_df = spark.read \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc056cc-85cd-4d54-a046-3bf0049d8e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    Createdon|\n",
      "+-------------+\n",
      "|1/1/2023 0:09|\n",
      "|1/1/2023 1:21|\n",
      "|1/1/2023 1:21|\n",
      "|1/1/2023 1:46|\n",
      "|1/1/2023 2:34|\n",
      "|         NULL|\n",
      "|           LL|\n",
      "|1/1/2023 5:34|\n",
      "|1/1/2023 5:35|\n",
      "|1/1/2023 7:14|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oms_df.select(\"Createdon\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7755c247-3224-4125-a168-a2246d4f3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULL values\n",
    "columns_to_remove = [\"pt_name\",\"load\", \"Relay\", \"planned_sd_Id\",\"faultstr\", \n",
    "\"consumer_count_fdr\", \"saifi\", \"saidi\", \"_c36\",\"_c37\", \"_c38\", \"_c39\",\"_c40\", \"_c41\", \"_c42\", \"_c43\", \"_c44\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8299857-7785-4a62-9652-ad3598dc07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the DataFrame for outage_Id == 786900\n",
    "# punching_delay = oms_df.filter(oms_df.outage_Id == '786900').select(\"PunchingDelay\")\n",
    "\n",
    "# # Show the result\n",
    "# punching_delay.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb298de-0c7b-4c95-b552-9715aede39bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 28\n"
     ]
    }
   ],
   "source": [
    "oms_df_reduced = oms_df.drop(*columns_to_remove)    # Drop the columns from the DataFrame\n",
    "num_columns = len(oms_df_reduced.columns)\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f9c5170-54bd-480c-87c4-eab729983747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88487"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms_df_reduced.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe477671-69bc-4dbd-a60a-3f87f2863a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filter for rows where \"TAT\" is either a number (matching the regex) or null\n",
    "oms_df_reduced = oms_df_reduced.filter(\n",
    "    (F.col(\"TAT\").rlike(\"^[0-9]+$\")) | (F.col(\"TAT\").isNull())\n",
    ")\n",
    "\n",
    "\n",
    "oms_df_reduced = oms_df_reduced.withColumn(\n",
    "    \"TAT\",(col(\"TAT\").cast(\"double\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86845165-2611-4839-aa67-098f894841f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82956"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms_df_reduced.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9146acb5-0850-4f3e-a2dd-3b5f96144ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outage_Id', 'string'),\n",
       " ('NetworkLevel', 'string'),\n",
       " ('OutageType', 'string'),\n",
       " ('outageSubType', 'string'),\n",
       " ('htclosingtype', 'string'),\n",
       " ('htclosingsubtype', 'string'),\n",
       " ('htclosingreason', 'string'),\n",
       " ('initialoffreason', 'string'),\n",
       " ('grid_name', 'string'),\n",
       " ('fdr_Id', 'string'),\n",
       " ('fdr_name', 'string'),\n",
       " ('dts_Id', 'string'),\n",
       " ('dts_name', 'string'),\n",
       " ('cluster', 'string'),\n",
       " ('IBC', 'string'),\n",
       " ('TAT', 'double'),\n",
       " ('PunchingDelay', 'string'),\n",
       " ('OutageStatus', 'string'),\n",
       " ('Createdon', 'string'),\n",
       " ('punchCreateAt', 'string'),\n",
       " ('punchCloseAt', 'string'),\n",
       " ('duration', 'string'),\n",
       " ('isemergency', 'string'),\n",
       " ('israintripping', 'string'),\n",
       " ('cluster_Id', 'string'),\n",
       " ('closing_remarks', 'string'),\n",
       " ('loss_category', 'string'),\n",
       " ('lastupdatedon', 'string')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms_df_reduced.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81418fa3-fe28-4d87-9cd8-63c67ce62f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "oms_df_reduced = oms_df_reduced.repartition(100)  # or less/more based on your cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00140cb4-99c6-439b-aef0-103bcce3d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "oms_df_reduced = replace_invalid_strings_with_null(oms_df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c174514-199d-4247-8bcb-a09bab4e3ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82956"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms_df_reduced.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ca23115-c464-4658-a30e-01a05c71982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|PunchingDelay|\n",
      "+-------------+\n",
      "|            9|\n",
      "|           12|\n",
      "|            4|\n",
      "|           40|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|         NULL|\n",
      "|           11|\n",
      "|            6|\n",
      "|            8|\n",
      "|            3|\n",
      "|            3|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "oms_df_reduced.select(\"PunchingDelay\").limit(150).show()\n",
    "# oms_df_reduced.filter(oms_df_reduced[\"lastupdatedon\"].isNotNull()).select(\"lastupdatedon\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42a0710e-b560-47c7-9f04-c77f106195da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+-------------+-------------------+--------------------------------------------+---------------------------------+-----------------+----------+------+-----------------------------+------+----------------------------------------+-------+-----------+------+-------------+------------+-------------------+-------------------+-------------------+--------+-----------+--------------+----------+-------------------------------------------------------------------------------------------------+-------------+-------------+\n",
      "|outage_Id|NetworkLevel|OutageType|outageSubType|htclosingtype      |htclosingsubtype                            |htclosingreason                  |initialoffreason |grid_name |fdr_Id|fdr_name                     |dts_Id|dts_name                                |cluster|IBC        |TAT   |PunchingDelay|OutageStatus|Createdon          |punchCreateAt      |punchCloseAt       |duration|isemergency|israintripping|cluster_Id|closing_remarks                                                                                  |loss_category|lastupdatedon|\n",
      "+---------+------------+----------+-------------+-------------------+--------------------------------------------+---------------------------------+-----------------+----------+------+-----------------------------+------+----------------------------------------+-------+-----------+------+-------------+------------+-------------------+-------------------+-------------------+--------+-----------+--------------+----------+-------------------------------------------------------------------------------------------------+-------------+-------------+\n",
      "|1068252  |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Shutdown         |NULL      |220   |EASTERN SERVICES             |500276|P-2/8 SANAULLAH AVENUE                  |Gulshan|FB Area    |20.0  |1            |Completed   |2023-04-05 12:36:00|2023-04-05 12:35:00|2023-04-05 12:56:00|0:21:19 |NULL       |NO            |7         |NULL                                                                                             |LL           |NULL         |\n",
      "|1302402  |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |186   |BASRA SOAP                   |4319  |P-20, TAYABA SCHOOL                     |Gulshan|FB Area    |701.0 |1            |Completed   |2023-05-27 05:24:00|2023-05-27 05:23:00|2023-05-27 17:05:00|11:42:41|NULL       |NO            |7         |NULL                                                                                             |VHL          |NULL         |\n",
      "|812453   |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |DC - Disconnected|NULL      |543   |TIMBER MARKET                |172350|JANU T/PMT MIDDLE (FOOT BALL GROUND-II) |Gulshan|Liaquatabad|691.0 |1            |Completed   |2023-02-09 23:59:00|2023-02-09 23:58:00|2023-02-10 11:30:00|11:31:42|NULL       |NO            |7         |DC - Disconnected                                                                                |HL           |NULL         |\n",
      "|878061   |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |557   |APWA COLLEGE(OLD GOVT SCHOOL)|525412|CHILDREN IDEAL SCHOOL D/PMT (RHS)       |Gulshan|Liaquatabad|817.0 |0            |Completed   |2023-02-25 05:44:00|2023-02-25 05:44:00|2023-02-25 19:21:00|13:36:43|NULL       |NO            |7         |NULL                                                                                             |VHL          |NULL         |\n",
      "|1099949  |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Job of Operation |NULL      |1215  |ERUM DEVELOPER               |10374 |BL.18 PMT B-187                         |Gulshan|Gulshan    |61.0  |1            |Completed   |2023-04-13 00:43:00|2023-04-13 00:42:00|2023-04-13 01:44:00|1:02:12 |NULL       |NO            |7         |PMT Earthing .                                                                                   |LL           |NULL         |\n",
      "|756529   |HT          |Loop      |Loop Off     |Load Management    |Operations                                  |NULL                             |Forced Outage    |AZIZABAD  |3904  |MANZAR-E-JILANI              |NULL  |NULL                                    |Gulshan|FB Area    |19.0  |0            |Completed   |2023-01-22 21:18:00|2023-01-22 21:18:00|2023-01-22 21:37:00|0:19:25 |0          |NO            |7         |.HTLCF TO BE N/D                                                                                 |LL           |NULL         |\n",
      "|1018315  |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Lead Burnt Out   |NULL      |351   |ASIF & ZAFAR                 |10437 |LATEEF PLAZA SS                         |Gulshan|Gulshan    |68.0  |0            |Completed   |2023-03-26 16:48:00|2023-03-26 16:48:00|2023-03-26 17:56:00|1:07:48 |NULL       |NO            |7         |NULL                                                                                             |LL           |NULL         |\n",
      "|1283032  |HT          |Feeder    |OTFS         |Shutdown           |Fault / Shutdown Normalization              |NULL                             |Forced Outage    |AZIZABAD  |2113  |ALI SQUARE                   |NULL  |NULL                                    |Gulshan|FB Area    |10.0  |1            |Completed   |2023-05-23 19:41:00|2023-05-23 19:40:00|2023-05-23 19:47:00|0:06:58 |0          |NO            |7         |SSM forced shutdown to be normalized on Prince Corner PMT. Feeder put Off against safety measures|LL           |NULL         |\n",
      "|1223175  |HT          |Loop      |Loop Off     |Fault              |NULL                                        |HT Jumper                        |Forced Outage    |GULSHAN   |334   |SHANTI NAGAR                 |NULL  |NULL                                    |Gulshan|Gulshan    |70.0  |0            |Completed   |2023-05-11 17:30:00|2023-05-11 17:30:00|2023-05-11 18:18:00|0:48:21 |0          |NO            |7         |.JUMPER REPAIR AT 535777 - JAMALI SHOPPING CENTRE PMT                                            |CLL          |NULL         |\n",
      "|1276960  |HT          |Loop      |Loop Off     |Grid Planned Outage|NULL                                        |NULL                             |Grid Level Outage|AZIZABAD  |2034  |GULNOOR ICE                  |NULL  |NULL                                    |Gulshan|FB Area    |39.0  |1            |Completed   |2023-05-22 18:36:00|2023-05-22 18:35:00|2023-05-22 19:15:00|0:40:38 |0          |NO            |7         |.GRID PSD B/F TO BE N/D OPERATION                                                                |LL           |NULL         |\n",
      "|1239274  |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Job of Operation |NULL      |3439  |SADIQ NAGAR                  |1435  |ISLAM SB, BL-5 D/PMT-I, (COSMOS SCHOOL) |Gulshan|FB Area    |172.0 |0            |Completed   |2023-05-15 08:40:00|2023-05-15 08:40:00|2023-05-15 11:32:00|2:51:54 |NULL       |NO            |7         |NULL                                                                                             |CLL          |NULL         |\n",
      "|1248748  |HT          |Feeder    |Feeder Trip  |Non-Transient      |Minor Fault                                 |VIR / Wire / Cloth etc. grounding|NULL             |LIAQUTABAD|560   |FATIMA BAROCHA               |NULL  |NULL                                    |Gulshan|Nazimabad  |120.0 |0            |Completed   |2023-05-17 07:14:00|2023-05-17 07:14:00|2023-05-17 09:10:00|1:56:20 |0          |NO            |7         |.VIR  REMMOVED @ 7978 - BABOO JEE CENTER , Supervisor Name:HAMMAD                                |LL           |NULL         |\n",
      "|846511   |LT          |DTS       |DTS Trip     |NULL               |NULL                                        |NULL                             |Lead Burnt Out   |NULL      |556   |ABC COMPUTER                 |11    |SHARIF & PARTNER BL- 4 FBA              |Gulshan|FB Area    |76.0  |1            |Completed   |2023-02-19 00:38:00|2023-02-19 00:37:00|2023-02-19 01:54:00|1:17:30 |NULL       |NO            |7         |NULL                                                                                             |LL           |NULL         |\n",
      "|851971   |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |3502  |TOOBA MASJID                 |10666 |IBRAHIM PCO (QUSIA MASJID PMT)          |Gulshan|Nazimabad  |1309.0|1            |Completed   |2023-02-20 06:29:00|2023-02-20 06:28:00|2023-02-21 04:15:00|21:47:18|NULL       |NO            |7         |N.D                                                                                              |VHL          |NULL         |\n",
      "|1331331  |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |2047  |NAGWAN VILLAGE (ANGARA GOTH) |7961  |KASOO RASHAN SHOP                       |Gulshan|Liaquatabad|296.0 |0            |Completed   |2023-06-01 05:59:00|2023-06-01 05:59:00|2023-06-01 10:30:00|4:31:07 |NULL       |NO            |7         |NULL                                                                                             |HL           |NULL         |\n",
      "|956137   |HT          |Loop      |Loop Off     |Fault              |NULL                                        |HT Jumper                        |Forced Outage    |GULSHAN   |330   |SHAMOONABAD                  |NULL  |NULL                                    |Gulshan|Gulshan    |102.0 |0            |Completed   |2023-03-12 18:43:00|2023-03-12 18:43:00|2023-03-12 20:25:00|1:42:15 |0          |NO            |7         |.HT JUMPER REPAIR AT 9477 - TAXAS SCHOOL                                                         |LL           |NULL         |\n",
      "|843525   |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |2156  |MEMON MASJID                 |500390|NOORI MASJID- D/B PMT-2     , R-1548 (RH|Gulshan|FB Area    |491.0 |0            |Completed   |2023-02-18 05:40:00|2023-02-18 05:40:00|2023-02-18 13:51:00|8:10:57 |NULL       |NO            |7         |NULL                                                                                             |LL           |NULL         |\n",
      "|725893   |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Lead Burnt Out   |NULL      |197   |KMC WATER PUMP               |500399|P-10/1 SA CNG                           |Gulshan|FB Area    |579.0 |1            |Completed   |2023-01-03 10:16:00|2023-01-03 10:15:00|2023-01-03 19:55:00|9:40:21 |NULL       |NO            |7         |LT Leads Stolen                                                                                  |LL           |NULL         |\n",
      "|1092314  |HT          |Feeder    |OTFS         |Operation          |NULL                                        |NULL                             |Planned Outage   |JAIL      |2021  |HAJI MUREED GOTH             |NULL  |NULL                                    |Gulshan|Nazimabad  |12.0  |7            |Completed   |2023-04-10 16:58:00|2023-04-10 16:51:00|2023-04-10 17:06:00|0:14:34 |0          |NO            |7         |ID # 116419, Planned Shutdown to CCR has been Normalized.                                        |VHL          |NULL         |\n",
      "|989025   |HT          |Loop      |Loop Off     |Shutdown           |Shutdown against Fault / Job / Safety Hazard|NULL                             |Forced Outage    |LIAQUTABAD|NULL  |NULL                         |NULL  |NULL                                    |NULL   |NULL       |NULL  |NULL         |NULL        |NULL               |NULL               |NULL               |NULL    |NULL       |NULL          |NULL      |NULL                                                                                             |NULL         |NULL         |\n",
      "+---------+------------+----------+-------------+-------------------+--------------------------------------------+---------------------------------+-----------------+----------+------+-----------------------------+------+----------------------------------------+-------+-----------+------+-------------+------------+-------------------+-------------------+-------------------+--------+-----------+--------------+----------+-------------------------------------------------------------------------------------------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, to_timestamp\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_timestamp, col, when\n",
    "\n",
    "\n",
    "# # Clean non-datetime rows, e.g., \"LL\" or empty\n",
    "# oms_df_reduced = oms_df_reduced.withColumn(\n",
    "#     \"lastupdatedon\",\n",
    "#     when(col(\"lastupdatedon\").rlike(\"^[0-9]{1,2}/[0-9]{1,2}/[0-9]{4} [0-9]{1,2}:[0-9]{2}$\"), \n",
    "#          to_timestamp(col(\"lastupdatedon\"), \"M/d/yy h:mm\"))\n",
    "#     .otherwise(None)  # Will set to null for invalid formats\n",
    "# )\n",
    "\n",
    "# Repeat for other columns if needed\n",
    "datetime_cols = [\"punchCreateAt\", \"punchCloseAt\", \"Createdon\",\"lastupdatedon\",\"lastupdatedon\"]\n",
    "\n",
    "# Loop through datetime columns\n",
    "for col_name in datetime_cols:\n",
    "    oms_df_reduced = oms_df_reduced.withColumn(\n",
    "        col_name,\n",
    "    when(\n",
    "        col(col_name).rlike(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}$\"),  # M/d/yyyy h:mm or MM/dd/yyyy HH:mm\n",
    "        to_timestamp(col(col_name), \"M/d/yyyy H:mm\")\n",
    "    )\n",
    "    .when(\n",
    "        col(col_name).rlike(r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\"),  # yyyy-MM-dd HH:mm:ss\n",
    "        to_timestamp(col(col_name), \"yyyy-MM-dd HH:mm:ss\")\n",
    "    )\n",
    "        # when(\n",
    "        #     col(col_name).rlike(\"^[0-9]{1,2}/[0-9]{1,2}/[0-9]{4} [0-9]{1,2}:[0-9]{2}$\"),  # M/d/yyyy h:mm format\n",
    "        #     to_timestamp(col(col_name), \"M/d/yyyy h:mm\")\n",
    "        # )\n",
    "        # .when(\n",
    "        #     col(col_name).rlike(\"^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}$\"),  # MM/d/yyyy h:mm format\n",
    "        #     to_timestamp(col(col_name), \"yyyy-MM-dd HH:mm:ss\")\n",
    "        # )\n",
    "        # .when(\n",
    "        #     col(col_name).rlike(\"^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}$\"),  # M/dd/yyyy h:mm format\n",
    "        #     to_timestamp(col(col_name), \"yyyy-MM-dd HH:mm:ss\")\n",
    "        # )\n",
    "        # .when(\n",
    "        #     col(col_name).rlike(\"^[0-9]{1,2}/[0-9]{1,2}/[0-9]{4} [0-9]{1,2}:[0-9]{2}$\"),  # MM/dd/yyyy h:mm format\n",
    "        #     to_timestamp(col(col_name), \"M/d/yyyy h:mm\")\n",
    "        # )\n",
    "        # .when(\n",
    "        #     col(col_name).rlike(\"^[0-9]{1,2}/[0-9]{1,2}/[0-9]{4} [0-9]{1,2}:[0-9]{2}$\"),  # M/d/yyyy HH:mm format\n",
    "        #     to_timestamp(col(col_name), \"M/d/yyyy h:mm\")\n",
    "        # )\n",
    "        # .when(\n",
    "        #     col(col_name).rlike(\"^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}$\"),  # MM/d/yyyy HH:mm format\n",
    "        #     to_timestamp(col(col_name), \"yyyy-MM-dd HH:mm:ss\")\n",
    "        # )\n",
    "        # .when(\n",
    "        #     col(col_name).rlike(\"^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}$\"),  # M/dd/yyyy HH:mm format\n",
    "        #     to_timestamp(col(col_name), \"yyyy-MM-dd HH:mm:ss\")\n",
    "        # )\n",
    "        # .when(\n",
    "        #     col(col_name).rlike(\"^[0-9]{1,2}/[0-9]{1,2}/[0-9]{4} [0-9]{1,2}:[0-9]{2}$\"),  # MM/dd/yyyy HH:mm format\n",
    "        #     to_timestamp(col(col_name), \"M/d/yyyy h:mm\")\n",
    "        # )\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "# Show the updated DataFrame\n",
    "oms_df_reduced.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51a4abda-0d80-4ef7-9a67-4a020ec71b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82956"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms_df_reduced.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "756e07b8-3dfd-42ab-a6f1-c93e4e62e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,when\n",
    "\n",
    "\n",
    "# Keep only rows where PunchingDelay is NULL or fully numeric\n",
    "oms_df_reduced = oms_df_reduced.filter(\n",
    "    col(\"PunchingDelay\").rlike(\"^[0-9]+$\") | col(\"PunchingDelay\").isNull()\n",
    ")\n",
    "\n",
    "oms_df_reduced = oms_df_reduced.withColumn(\n",
    "    \"PunchingDelay\",(col(\"PunchingDelay\").cast(\"double\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ab9c98c-d5a7-461a-984c-d0213de890bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 234:===================>                                     (2 + 4) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+-------------+-------------------+--------------------------------------------+---------------------------------+-----------------+----------+------+-----------------------------+------+----------------------------------------+-------+----------+------+-------------+------------+-------------------+-------------------+-------------------+------------------+-----------+--------------+----------+-------------------------------------------------------------------------------------------------+-------------+-------------+----+-----+\n",
      "|outage_Id|NetworkLevel|OutageType|outageSubType|htclosingtype      |htclosingsubtype                            |htclosingreason                  |initialoffreason |grid_name |fdr_Id|fdr_name                     |dts_Id|dts_name                                |cluster|IBC       |TAT   |PunchingDelay|OutageStatus|Createdon          |punchCreateAt      |punchCloseAt       |duration          |isemergency|israintripping|cluster_Id|closing_remarks                                                                                  |loss_category|lastupdatedon|Year|Month|\n",
      "+---------+------------+----------+-------------+-------------------+--------------------------------------------+---------------------------------+-----------------+----------+------+-----------------------------+------+----------------------------------------+-------+----------+------+-------------+------------+-------------------+-------------------+-------------------+------------------+-----------+--------------+----------+-------------------------------------------------------------------------------------------------+-------------+-------------+----+-----+\n",
      "|1068252  |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Shutdown         |NULL      |220   |EASTERN SERVICES             |500276|P-2/8 SANAULLAH AVENUE                  |Gulshan|F.B. Area |20.0  |1.0          |Completed   |2023-04-05 12:36:00|2023-04-05 12:35:00|2023-04-05 12:56:00|21.316667556762695|NULL       |0             |7         |NULL                                                                                             |LL           |NULL         |2023|4    |\n",
      "|1302402  |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |186   |BASRA SOAP                   |4319  |P-20, TAYABA SCHOOL                     |Gulshan|F.B. Area |701.0 |1.0          |Completed   |2023-05-27 05:24:00|2023-05-27 05:23:00|2023-05-27 17:05:00|702.683349609375  |NULL       |0             |7         |NULL                                                                                             |VHL          |NULL         |2023|5    |\n",
      "|812453   |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |DC - Disconnected|NULL      |543   |TIMBER MARKET                |172350|JANU T/PMT MIDDLE (FOOT BALL GROUND-II) |Gulshan|Liaqatabad|691.0 |1.0          |Completed   |2023-02-09 23:59:00|2023-02-09 23:58:00|2023-02-10 11:30:00|691.7000122070312 |NULL       |0             |7         |DC - Disconnected                                                                                |HL           |NULL         |2023|2    |\n",
      "|878061   |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |557   |APWA COLLEGE(OLD GOVT SCHOOL)|525412|CHILDREN IDEAL SCHOOL D/PMT (RHS)       |Gulshan|Liaqatabad|817.0 |0.0          |Completed   |2023-02-25 05:44:00|2023-02-25 05:44:00|2023-02-25 19:21:00|816.7166748046875 |NULL       |0             |7         |NULL                                                                                             |VHL          |NULL         |2023|2    |\n",
      "|1099949  |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Job of Operation |NULL      |1215  |ERUM DEVELOPER               |10374 |BL.18 PMT B-187                         |Gulshan|Gulshan   |61.0  |1.0          |Completed   |2023-04-13 00:43:00|2023-04-13 00:42:00|2023-04-13 01:44:00|62.20000076293945 |NULL       |0             |7         |PMT Earthing .                                                                                   |LL           |NULL         |2023|4    |\n",
      "|756529   |HT          |Loop      |Loop Off     |Load Management    |Operations                                  |NULL                             |Forced Outage    |AZIZABAD  |3904  |MANZAR-E-JILANI              |NULL  |NULL                                    |Gulshan|F.B. Area |19.0  |0.0          |Completed   |2023-01-22 21:18:00|2023-01-22 21:18:00|2023-01-22 21:37:00|19.41666603088379 |0          |0             |7         |.HTLCF TO BE N/D                                                                                 |LL           |NULL         |2023|1    |\n",
      "|1018315  |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Lead Burnt Out   |NULL      |351   |ASIF & ZAFAR                 |10437 |LATEEF PLAZA SS                         |Gulshan|Gulshan   |68.0  |0.0          |Completed   |2023-03-26 16:48:00|2023-03-26 16:48:00|2023-03-26 17:56:00|67.80000305175781 |NULL       |0             |7         |NULL                                                                                             |LL           |NULL         |2023|3    |\n",
      "|1283032  |HT          |Feeder    |OTFS         |Shutdown           |Fault / Shutdown Normalization              |NULL                             |Forced Outage    |AZIZABAD  |2113  |ALI SQUARE                   |NULL  |NULL                                    |Gulshan|F.B. Area |10.0  |1.0          |Completed   |2023-05-23 19:41:00|2023-05-23 19:40:00|2023-05-23 19:47:00|6.9666666984558105|0          |0             |7         |SSM forced shutdown to be normalized on Prince Corner PMT. Feeder put Off against safety measures|LL           |NULL         |2023|5    |\n",
      "|1223175  |HT          |Loop      |Loop Off     |Fault              |NULL                                        |HT Jumper                        |Forced Outage    |GULSHAN   |334   |SHANTI NAGAR                 |NULL  |NULL                                    |Gulshan|Gulshan   |70.0  |0.0          |Completed   |2023-05-11 17:30:00|2023-05-11 17:30:00|2023-05-11 18:18:00|48.349998474121094|0          |0             |7         |.JUMPER REPAIR AT 535777 - JAMALI SHOPPING CENTRE PMT                                            |CLL          |NULL         |2023|5    |\n",
      "|1276960  |HT          |Loop      |Loop Off     |Grid Planned Outage|NULL                                        |NULL                             |Grid Level Outage|AZIZABAD  |2034  |GULNOOR ICE                  |NULL  |NULL                                    |Gulshan|F.B. Area |39.0  |1.0          |Completed   |2023-05-22 18:36:00|2023-05-22 18:35:00|2023-05-22 19:15:00|40.63333511352539 |0          |0             |7         |.GRID PSD B/F TO BE N/D OPERATION                                                                |LL           |NULL         |2023|5    |\n",
      "|1239274  |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Job of Operation |NULL      |3439  |SADIQ NAGAR                  |1435  |ISLAM SB, BL-5 D/PMT-I, (COSMOS SCHOOL) |Gulshan|F.B. Area |172.0 |0.0          |Completed   |2023-05-15 08:40:00|2023-05-15 08:40:00|2023-05-15 11:32:00|171.89999389648438|NULL       |0             |7         |NULL                                                                                             |CLL          |NULL         |2023|5    |\n",
      "|1248748  |HT          |Feeder    |Feeder Trip  |Non-Transient      |Minor Fault                                 |VIR / Wire / Cloth etc. grounding|NULL             |LIAQUTABAD|560   |FATIMA BAROCHA               |NULL  |NULL                                    |Gulshan|Nazimabad |120.0 |0.0          |Completed   |2023-05-17 07:14:00|2023-05-17 07:14:00|2023-05-17 09:10:00|116.33333587646484|0          |0             |7         |.VIR  REMMOVED @ 7978 - BABOO JEE CENTER , Supervisor Name:HAMMAD                                |LL           |NULL         |2023|5    |\n",
      "|846511   |LT          |DTS       |DTS Trip     |NULL               |NULL                                        |NULL                             |Lead Burnt Out   |NULL      |556   |ABC COMPUTER                 |11    |SHARIF & PARTNER BL- 4 FBA              |Gulshan|F.B. Area |76.0  |1.0          |Completed   |2023-02-19 00:38:00|2023-02-19 00:37:00|2023-02-19 01:54:00|77.5              |NULL       |0             |7         |NULL                                                                                             |LL           |NULL         |2023|2    |\n",
      "|851971   |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |3502  |TOOBA MASJID                 |10666 |IBRAHIM PCO (QUSIA MASJID PMT)          |Gulshan|Nazimabad |1309.0|1.0          |Completed   |2023-02-20 06:29:00|2023-02-20 06:28:00|2023-02-21 04:15:00|1307.300048828125 |NULL       |0             |7         |N.D                                                                                              |VHL          |NULL         |2023|2    |\n",
      "|1331331  |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |2047  |NAGWAN VILLAGE (ANGARA GOTH) |7961  |KASOO RASHAN SHOP                       |Gulshan|Liaqatabad|296.0 |0.0          |Completed   |2023-06-01 05:59:00|2023-06-01 05:59:00|2023-06-01 10:30:00|271.1166687011719 |NULL       |0             |7         |NULL                                                                                             |HL           |NULL         |2023|6    |\n",
      "|956137   |HT          |Loop      |Loop Off     |Fault              |NULL                                        |HT Jumper                        |Forced Outage    |GULSHAN   |330   |SHAMOONABAD                  |NULL  |NULL                                    |Gulshan|Gulshan   |102.0 |0.0          |Completed   |2023-03-12 18:43:00|2023-03-12 18:43:00|2023-03-12 20:25:00|102.25            |0          |0             |7         |.HT JUMPER REPAIR AT 9477 - TAXAS SCHOOL                                                         |LL           |NULL         |2023|3    |\n",
      "|843525   |RCDC        |DTS       |DTS DC       |NULL               |NULL                                        |NULL                             |Commercial       |NULL      |2156  |MEMON MASJID                 |500390|NOORI MASJID- D/B PMT-2     , R-1548 (RH|Gulshan|F.B. Area |491.0 |0.0          |Completed   |2023-02-18 05:40:00|2023-02-18 05:40:00|2023-02-18 13:51:00|490.95001220703125|NULL       |0             |7         |NULL                                                                                             |LL           |NULL         |2023|2    |\n",
      "|725893   |LT          |DTS       |DTS Off      |NULL               |NULL                                        |NULL                             |Lead Burnt Out   |NULL      |197   |KMC WATER PUMP               |500399|P-10/1 SA CNG                           |Gulshan|F.B. Area |579.0 |1.0          |Completed   |2023-01-03 10:16:00|2023-01-03 10:15:00|2023-01-03 19:55:00|580.3499755859375 |NULL       |0             |7         |LT Leads Stolen                                                                                  |LL           |NULL         |2023|1    |\n",
      "|1092314  |HT          |Feeder    |OTFS         |Operation          |NULL                                        |NULL                             |Planned Outage   |JAIL      |2021  |HAJI MUREED GOTH             |NULL  |NULL                                    |Gulshan|Nazimabad |12.0  |7.0          |Completed   |2023-04-10 16:58:00|2023-04-10 16:51:00|2023-04-10 17:06:00|14.566666603088379|0          |0             |7         |ID # 116419, Planned Shutdown to CCR has been Normalized.                                        |VHL          |NULL         |2023|4    |\n",
      "|989025   |HT          |Loop      |Loop Off     |Shutdown           |Shutdown against Fault / Job / Safety Hazard|NULL                             |Forced Outage    |LIAQUTABAD|NULL  |NULL                         |NULL  |NULL                                    |NULL   |NULL      |NULL  |NULL         |NULL        |NULL               |NULL               |NULL               |NULL              |0          |0             |NULL      |NULL                                                                                             |NULL         |NULL         |NULL|NULL |\n",
      "+---------+------------+----------+-------------+-------------------+--------------------------------------------+---------------------------------+-----------------+----------+------+-----------------------------+------+----------------------------------------+-------+----------+------+-------------+------------+-------------------+-------------------+-------------------+------------------+-----------+--------------+----------+-------------------------------------------------------------------------------------------------+-------------+-------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#homogenizing name because oms does not have ibc code for join\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Update the 'IBC' column to replace the specific strings\n",
    "oms_df_reduced = oms_df_reduced.withColumn(\n",
    "    \"IBC\", \n",
    "    F.when(F.col(\"IBC\") == \"Liaquatabad\", \"Liaqatabad\")\n",
    "     .when(F.col(\"IBC\") == \"FB Area\", \"F.B. Area\")\n",
    "     .otherwise(F.col(\"IBC\"))  # Keep other values unchanged\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "oms_df_reduced.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c356ba02-0dbf-4ed1-9f8c-f0b9045735fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77385"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "#CA Creation Date (MM/DD/YYYY), Contract Creation Date(DD/MM/YYYY), Last DC Date (DD-mon-YYYY) (same as last payment)\n",
    "oms_df_reduced.filter(oms_df_reduced[\"Createdon\"].isNotNull()).select(\"Createdon\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bff3d1e2-5b2c-4274-802a-2d087badbe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  duration|\n",
      "+----------+\n",
      "| 55.366665|\n",
      "| 38.516666|\n",
      "|  837.9667|\n",
      "| 65.433334|\n",
      "|107.166664|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert duration to minutes hh:mm:ss\n",
    "oms_df_reduced = oms_df_reduced.withColumn(\"duration\", hhmmss_to_minutes_udf(col(\"duration\"))) \n",
    "oms_df_reduced.select(\"duration\").limit(5).show()\n",
    "oms_df_reduced = convert_column_to_double(oms_df_reduced,\"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a07ba489-0141-40c1-aa88-1e53b448dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          duration|\n",
      "+------------------+\n",
      "| 55.36666488647461|\n",
      "|38.516666412353516|\n",
      "| 837.9666748046875|\n",
      "| 65.43333435058594|\n",
      "|107.16666412353516|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "oms_df_reduced.select(\"duration\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0cd55e54-eabb-4a45-85d6-944ec8181bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# # Filter out rows where 'isemergency' is numeric, null, or contains only whitespace\n",
    "# check = df_reduced.filter(\n",
    "#     (col(\"isemergency\").rlike(\"^[0-9]+$\")) | \n",
    "#     (col(\"isemergency\").isNull()) | \n",
    "#     (col(\"isemergency\").rlike(\"^\\\\s*$\"))  # matches whitespace-only strings\n",
    "# )\n",
    "\n",
    "# Replace invalid values (including whitespace) with 0, then cast to int\n",
    "oms_df_reduced = oms_df_reduced.withColumn(\n",
    "    \"isemergency\",\n",
    "    when(col(\"isemergency\").isNull(), 0)\n",
    "    .when(col(\"isemergency\").rlike(\"^\\\\s*$\"), 0)  # Replace whitespace with 0\n",
    "    .otherwise(col(\"isemergency\").cast(\"int\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "32464803-3496-4c07-b1c1-aac189f6082e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82951"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms_df_reduced.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "954ceefa-63be-4d7d-8fce-a6497c557bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "oms_df_reduced = oms_df_reduced.withColumn(\"israintripping\", when(col(\"israintripping\") == \"YES\", 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7e707d4-9f40-4c5e-8561-b59df44e0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 255:=========>                                               (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+-------------+-------------------+------------------------------+---------------+-------------------+------------+------+-------------------------------------+------+----------------------------------------+-------+----------+--------+-------------+------------+-------------------+-------------------+-------------------+------------------+-----------+--------------+----------+----------------------------------------------------------------------+-------------+-------------+----+-----+\n",
      "|outage_Id|NetworkLevel|OutageType|outageSubType|htclosingtype      |htclosingsubtype              |htclosingreason|initialoffreason   |grid_name   |fdr_Id|fdr_name                             |dts_Id|dts_name                                |cluster|IBC       |TAT     |PunchingDelay|OutageStatus|Createdon          |punchCreateAt      |punchCloseAt       |duration          |isemergency|israintripping|cluster_Id|closing_remarks                                                       |loss_category|lastupdatedon|Year|Month|\n",
      "+---------+------------+----------+-------------+-------------------+------------------------------+---------------+-------------------+------------+------+-------------------------------------+------+----------------------------------------+-------+----------+--------+-------------+------------+-------------------+-------------------+-------------------+------------------+-----------+--------------+----------+----------------------------------------------------------------------+-------------+-------------+----+-----+\n",
      "|861624   |RCDC        |DTS       |DTS DC       |NULL               |NULL                          |NULL           |Commercial         |NULL        |2021  |HAJI MUREED GOTH                     |500571|ASHRAFIA MAZAR PMT ( HAJI MUREED GOTH)  |Gulshan|Nazimabad |852.0   |1.0          |Completed   |2023-02-22 04:56:00|2023-02-22 04:55:00|2023-02-22 19:08:00|853.3333129882812 |0          |0             |7         |NULL                                                                  |VHL          |NULL         |2023|2    |\n",
      "|794002   |LT          |DTS       |DTS Trip     |NULL               |NULL                          |NULL           |DC - Disconnected  |NULL        |2101  |USMAN HOSPITAL                       |527163|CUE SNOOKER CLUB, BLOCK-2 F.B AREA      |Gulshan|F.B. Area |716.0   |0.0          |Completed   |2023-02-04 05:53:00|2023-02-04 05:53:00|2023-02-04 17:49:00|715.7333374023438 |0          |0             |7         |NULL                                                                  |LL           |NULL         |2023|2    |\n",
      "|839610   |HT          |Feeder    |OTFS         |Load Management    |Operations                    |NULL           |Forced Outage      |LIAQUTABAD  |551   |SHAKEEL CORPORATION(OLD MOOSA COLONY)|NULL  |NULL                                    |Gulshan|F.B. Area |18.0    |1.0          |Completed   |2023-02-17 02:53:00|2023-02-17 02:52:00|2023-02-17 03:05:00|13.600000381469727|0          |0             |7         |.htlcf to be n/d                                                      |LL           |NULL         |2023|2    |\n",
      "|1307687  |HT          |Loop      |Loop Off     |Load Management    |Operations                    |NULL           |Forced Outage      |LIAQUTABAD  |NULL  |NULL                                 |NULL  |NULL                                    |NULL   |NULL      |NULL    |NULL         |NULL        |NULL               |NULL               |NULL               |NULL              |0          |0             |NULL      |NULL                                                                  |NULL         |NULL         |NULL|NULL |\n",
      "|820860   |RCDC        |DTS       |DTS DC       |NULL               |NULL                          |NULL           |Commercial         |NULL        |3502  |TOOBA MASJID                         |170626|MUSARAT COLONY (CINEMA) PMT-1           |Gulshan|Nazimabad |93.0    |0.0          |Completed   |2023-02-12 08:00:00|2023-02-12 08:00:00|2023-02-12 09:33:00|93.0999984741211  |0          |0             |7         |NULL                                                                  |VHL          |NULL         |2023|2    |\n",
      "|958418   |HT          |Feeder    |Feeder Trip  |Non-Transient      |Major Fault With Carry Forward|NULL           |NULL               |AZIZABAD    |3341  |BAKHSHI SQUARE                       |NULL  |NULL                                    |Gulshan|F.B. Area |145.0   |3.0          |Completed   |2023-03-13 07:34:00|2023-03-13 07:31:00|2023-03-13 09:25:00|114.01667022705078|0          |0             |7         |.C/F TO MCF                                                           |LL           |NULL         |2023|3    |\n",
      "|1184747  |HT          |Loop      |Loop Off     |NULL               |NULL                          |NULL           |Forced Outage      |LIAQUTABAD  |551   |SHAKEEL CORPORATION(OLD MOOSA COLONY)|NULL  |NULL                                    |Gulshan|F.B. Area |930415.0|0.0          |Cancelled   |2023-05-03 13:29:00|2023-05-03 13:29:00|NULL               |NULL              |0          |0             |7         |NULL                                                                  |LL           |NULL         |2023|5    |\n",
      "|1293072  |LT          |DTS       |DTS Trip     |NULL               |NULL                          |NULL           |Job of Operation   |NULL        |552   |CAF MILAT (NAIRANG CINEMA)          |971   |LIAQUATABAD # 4 SS                      |Gulshan|Liaqatabad|493.0   |60.0         |Completed   |2023-05-25 13:44:00|2023-05-25 12:44:00|2023-05-25 21:57:00|553.0833129882812 |0          |0             |7         |NULL                                                                  |ML           |NULL         |2023|5    |\n",
      "|1103155  |LT          |DTS       |DTS Off      |NULL               |NULL                          |NULL           |Supply off from s/s|NULL        |558   |TAYYA HOTEL (OLD LIAQATABAD-1)       |526240|OPP. LIAQUATABAD-1 SS PMT-2             |Gulshan|Liaqatabad|249.0   |1.0          |Completed   |2023-04-13 18:46:00|2023-04-13 18:45:00|2023-04-13 22:55:00|250.36666870117188|0          |0             |7         |NULL                                                                  |ML           |NULL         |2023|4    |\n",
      "|762231   |LT          |DTS       |DTS Off      |NULL               |NULL                          |NULL           |DC - Disconnected  |NULL        |195   |ABBAS SQUARE                         |1371  |AZIZABAD TEL NO. 1 BL-14 F.B.A S/S      |Gulshan|F.B. Area |36.0    |1.0          |Completed   |2023-01-25 17:18:00|2023-01-25 17:17:00|2023-01-25 17:54:00|37.349998474121094|0          |0             |7         |NULL                                                                  |LL           |NULL         |2023|1    |\n",
      "|1323526  |HT          |Loop      |Loop Off     |Load Management    |Operations                    |NULL           |Forced Outage      |GULSHAN     |343   |LUCKY CENTER                         |NULL  |NULL                                    |Gulshan|Gulshan   |16.0    |1.0          |Completed   |2023-05-30 20:13:00|2023-05-30 20:12:00|2023-05-30 20:29:00|17.316667556762695|0          |0             |7         |.MCF TO BE N/D                                                        |LL           |NULL         |2023|5    |\n",
      "|740626   |HT          |Loop      |Loop Off     |Grid Planned Outage|NULL                          |NULL           |Grid Level Outage  |LIAQUTABAD  |1269  |PAKISTAN COLLEGE (OLD NAZIMABAD-1)   |NULL  |NULL                                    |Gulshan|Nazimabad |8.0     |1.0          |Completed   |2023-01-13 19:44:00|2023-01-13 19:43:00|2023-01-13 19:52:00|9.133333206176758 |0          |0             |7         |.Grid Level Outage BF Operation.N/D                                   |LL           |NULL         |2023|1    |\n",
      "|1226865  |RCDC        |DTS       |DTS DC       |NULL               |NULL                          |NULL           |Commercial         |NULL        |559   |C-1 AREA                             |552   |DAK KHANA CHOWK PMT-1                   |Gulshan|Liaqatabad|151.0   |0.0          |Completed   |2023-05-12 10:54:00|2023-05-12 10:54:00|2023-05-12 13:25:00|150.61666870117188|0          |0             |7         |NULL                                                                  |ML           |NULL         |2023|5    |\n",
      "|1319177  |LT          |DTS       |DTS Off      |NULL               |NULL                          |NULL           |Lead Burnt Out     |NULL        |2156  |MEMON MASJID                         |9712  |AZIZABAD POLICE STATION PMT-I           |Gulshan|F.B. Area |90.0    |0.0          |Completed   |2023-05-30 01:59:00|2023-05-30 01:59:00|2023-05-30 03:29:00|89.93333435058594 |0          |0             |7         |NULL                                                                  |LL           |NULL         |2023|5    |\n",
      "|880020   |HT          |Feeder    |Incoming Trip|Non-Transient      |Minor Fault                   |Misc.          |NULL               |LIAQUTABAD  |1269  |PAKISTAN COLLEGE (OLD NAZIMABAD-1)   |NULL  |NULL                                    |Gulshan|Nazimabad |63.0    |0.0          |Completed   |2023-02-25 18:27:00|2023-02-25 18:27:00|2023-02-25 19:30:00|63.03333282470703 |0          |0             |7         |.protection issue.E/W REPAIR AT 511172 - MUHAMMAD HANIF (HABIB GARDEN)|LL           |NULL         |2023|2    |\n",
      "|1242013  |LT          |DTS       |DTS Off      |NULL               |NULL                          |NULL           |Lead Burnt Out     |NULL        |3053  |HUSSAIN HAZARA                       |200024|POLE-8, HUSSAIN HAZARA GOTH (HYDRI DRY C|Gulshan|Gulshan   |195.0   |0.0          |Completed   |2023-05-15 19:56:00|2023-05-15 19:56:00|2023-05-15 23:11:00|194.78334045410156|0          |0             |7         |LEad Repaired .                                                       |HL           |NULL         |2023|5    |\n",
      "|826311   |HT          |Feeder    |OTFS         |Shutdown           |Fault / Shutdown Normalization|NULL           |Forced Outage      |LIAQUTABAD  |1269  |PAKISTAN COLLEGE (OLD NAZIMABAD-1)   |NULL  |NULL                                    |Gulshan|Nazimabad |25.0    |0.0          |Completed   |2023-02-13 17:33:00|2023-02-13 17:33:00|2023-02-13 17:45:00|11.866666793823242|1          |0             |7         |.EMERGENCY COMPLAINT                                                  |LL           |NULL         |2023|2    |\n",
      "|998380   |LT          |DTS       |DTS Off      |NULL               |NULL                          |NULL           |Voltage Fluctuation|NULL        |3549  |TALIB COLONY                         |172100|FAIZAN (NADDI WALI D/PMT) RHS           |Gulshan|Liaqatabad|409.0   |0.0          |Completed   |2023-03-21 21:14:00|2023-03-21 21:14:00|2023-03-22 04:03:00|409.26666259765625|0          |0             |7         |Voltage Fluctuation                                                   |HL           |NULL         |2023|3    |\n",
      "|1275968  |HT          |Feeder    |OTFS         |Grid Planned Outage|NULL                          |NULL           |Grid Level Outage  |CIVIC CENTRE|99    |BARADARI                             |NULL  |NULL                                    |Gulshan|Gulshan   |112.0   |1.0          |Completed   |2023-05-22 15:02:00|2023-05-22 15:01:00|2023-05-22 16:39:00|98.4000015258789  |0          |0             |7         |GRID LEVE FORCE S/D                                                   |LL           |NULL         |2023|5    |\n",
      "|1304503  |LT          |DTS       |DTS Off      |NULL               |NULL                          |NULL           |Job of Operation   |NULL        |1199  |GULBERG                              |500419|P-23 SAMANABD  MARKET WALI PMT (LHS)    |Gulshan|F.B. Area |337.0   |0.0          |Completed   |2023-05-27 12:34:00|2023-05-27 12:34:00|2023-05-27 18:11:00|336.8666687011719 |0          |0             |7         |NULL                                                                  |LL           |NULL         |2023|5    |\n",
      "+---------+------------+----------+-------------+-------------------+------------------------------+---------------+-------------------+------------+------+-------------------------------------+------+----------------------------------------+-------+----------+--------+-------------+------------+-------------------+-------------------+-------------------+------------------+-----------+--------------+----------+----------------------------------------------------------------------+-------------+-------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df_reduced.limit(10).show()\n",
    "# df_reduced.show()  # First 20 rows by default\n",
    "# df.show(50)  # Show 50 rows\n",
    "oms_df_reduced.show(truncate=False)  # Don't cut off long strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcc215-02c4-474e-a41d-b6e72c0f1ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddff864-a2d1-4009-af06-2a0dc952cbde",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8582ea5c-409a-4949-b148-d4098d5a9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, year, month, sum as _sum\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "89a802ad-53c9-4fbc-a8f3-098024262b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82951"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms_df_reduced.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88f1d7a9-2b74-41d9-8f06-8b3ae30ef135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP table nessie.starschema.oms_fact\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "786d161f-393c-422b-95da-dc98e11b2804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. OMS fact\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.starschema\")\n",
    "oms_df_reduced.writeTo(\"nessie.starschema.oms_fact\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6e1085c4-ba45-446f-854a-00be1e6ad19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ONLY \n",
    "oms_df_reduced = spark.read.table(\"nessie.starschema.oms_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ca6f1-d85d-478c-90a8-a998732ddffe",
   "metadata": {},
   "source": [
    "### IBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47f15a35-00cf-4f8b-a05d-dbbd7c7c8989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 272:====================================================> (97 + 3) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+-------------------------------------+-----------------------------+--------------------+--------------+-------------------+-----------------------------+\n",
      "|Month|Year|IBC       |Average Fault Turn-Around Time (mins)|Average Fault Duration (mins)|Total Outages/Faults|Rain Frequency|Most_Occurred_Fault|Most_Occurred_Fault_Frequency|\n",
      "+-----+----+----------+-------------------------------------+-----------------------------+--------------------+--------------+-------------------+-----------------------------+\n",
      "|3    |2024|F.B. Area |12548.455188679245                   |192.87223607655298           |424                 |0             |Job of Operation   |182                          |\n",
      "|2    |2023|Gulshan   |17831.50980392157                    |147.82355213657743           |357                 |0             |Shutdown           |88                           |\n",
      "|6    |2023|Gulshan   |17547.137339055793                   |130.56513362291955           |699                 |0             |Lead Burnt Out     |257                          |\n",
      "|4    |2024|Liaqatabad|565.8563428293158                    |267.3639089741234            |4107                |0             |Commercial         |3302                         |\n",
      "|7    |2023|Gulshan   |10498.176223776223                   |119.944239577727             |715                 |0             |Lead Burnt Out     |234                          |\n",
      "|3    |2024|Liaqatabad|909.8636833046471                    |305.8743477947997            |2905                |0             |Commercial         |2258                         |\n",
      "|12   |2024|F.B. Area |1631.6443418013857                   |309.5681220350857            |433                 |0             |NULL               |86                           |\n",
      "|10   |2024|F.B. Area |3687.4675572519086                   |188.31062358638115           |524                 |0             |Job of Operation   |231                          |\n",
      "|5    |2023|Nazimabad |6956.117206982543                    |81.41825798707991            |401                 |0             |NULL               |94                           |\n",
      "|10   |2024|Liaqatabad|1298.2018348623853                   |740.6502466491572            |545                 |0             |Voltage Fluctuation|165                          |\n",
      "|2    |2024|F.B. Area |13180.735459662288                   |495.621311710603             |533                 |0             |Job of Operation   |212                          |\n",
      "|9    |2023|Gulshan   |14497.959223300972                   |208.55132131960133           |515                 |0             |Lead Burnt Out     |116                          |\n",
      "|2    |2025|Gulshan   |236.75757575757575                   |128.78102847363087           |99                  |0             |Shutdown           |28                           |\n",
      "|9    |2024|Liaqatabad|1317.5057034220533                   |571.8835244680027            |789                 |0             |Commercial         |237                          |\n",
      "|1    |2025|Liaqatabad|725.9142156862745                    |768.2303079999252            |816                 |0             |Job of Operation   |256                          |\n",
      "|8    |2023|F.B. Area |15407.771739130434                   |126.29020756342656           |460                 |0             |NULL               |129                          |\n",
      "|1    |2025|F.B. Area |1077.921717171717                    |147.41332440801975           |396                 |0             |Job of Operation   |83                           |\n",
      "|11   |2024|Nazimabad |4548.313199105145                    |688.6068998248558            |447                 |0             |Forced Outage      |106                          |\n",
      "|7    |2024|Gulshan   |3995.960824742268                    |258.312526101717             |970                 |0             |Lead Burnt Out     |334                          |\n",
      "|6    |2023|F.B. Area |12717.103280680438                   |125.43956042824338           |823                 |0             |Job of Operation   |250                          |\n",
      "+-----+----+----------+-------------------------------------+-----------------------------+--------------------+--------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Extract Year and Month from the CreatedOn column\n",
    "oms_df_reduced = oms_df_reduced.withColumn(\"Year\", F.year(\"CreatedOn\")) \\\n",
    "    .withColumn(\"Month\", F.month(\"CreatedOn\"))\n",
    "\n",
    "# Group by IBC, Year, Month and calculate frequency of each fault (initialoffreason)\n",
    "most_frequent_fault = oms_df_reduced.groupBy(\"IBC\", \"Year\", \"Month\", \"initialoffreason\").agg(\n",
    "    F.count(\"initialoffreason\").alias(\"Most_Occurred_Fault_Frequency\")\n",
    ")\n",
    "\n",
    "# Now, for each IBC, Year, and Month, we want the fault with the highest count\n",
    "most_frequent_fault = most_frequent_fault.withColumn(\n",
    "    \"rank\", F.row_number().over(\n",
    "        Window.partitionBy(\"IBC\", \"Year\", \"Month\").orderBy(F.desc(\"Most_Occurred_Fault_Frequency\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter to get only the most frequent fault for each IBC, Year, and Month\n",
    "most_frequent_fault = most_frequent_fault.filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# # Rename IBC to IBC_Fault for clarity\n",
    "# most_frequent_fault = most_frequent_fault.withColumnRenamed(\"IBC\", \"IBC_Fault\")\n",
    "# most_frequent_fault = most_frequent_fault.withColumnRenamed(\"Month\", \"Month_Fault\")\n",
    "# most_frequent_fault = most_frequent_fault.withColumnRenamed(\"Year\", \"Year_Fault\")\n",
    "\n",
    "# Now, we will aggregate the other fields for the IBC and join with the most frequent fault\n",
    "oms_ibc_dim = oms_df_reduced.groupBy(\"IBC\", \"Year\", \"Month\").agg(\n",
    "    F.avg(\"TAT\").alias(\"Average Fault Turn-Around Time (mins)\"),\n",
    "    F.avg(\"duration\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.count(\"outage_Id\").alias(\"Total Outages/Faults\"),\n",
    "    F.sum(\"israintripping\").alias(\"Rain Frequency\")\n",
    ")\n",
    "\n",
    "# Join the most frequent fault with the rest of the aggregation\n",
    "oms_ibc_dim_j = oms_ibc_dim.join(most_frequent_fault, \n",
    "                       on=[\"IBC\",\"Year\",\"Month\"],\n",
    "                       how=\"left\")\n",
    "\n",
    "# # Rename columns to avoid ambiguity before the final select\n",
    "# oms_ibc_dim = oms_ibc_dim.withColumnRenamed(\"Year\", \"oms_Year\") \\\n",
    "#                          .withColumnRenamed(\"Month\", \"oms_Month\")\n",
    "\n",
    "# most_frequent_fault = most_frequent_fault.withColumnRenamed(\"Year\", \"fault_Year\") \\\n",
    "#                                            .withColumnRenamed(\"Month\", \"fault_Month\")\n",
    "\n",
    "# Final selection of columns, including the most frequent fault\n",
    "oms_ibc_dim_j = oms_ibc_dim_j.select(\n",
    "    # Rename columns for clarity\n",
    "    F.col(\"Month\").alias(\"Month\"),\n",
    "    F.col(\"Year\").alias(\"Year\"),\n",
    "    \"IBC\", \n",
    "    \"Average Fault Turn-Around Time (mins)\", \n",
    "    \"Average Fault Duration (mins)\", \n",
    "    \"Total Outages/Faults\", \n",
    "    \"Rain Frequency\", \n",
    "    F.col(\"initialoffreason\").alias(\"Most_Occurred_Fault\"), \n",
    "    \"Most_Occurred_Fault_Frequency\"\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "oms_ibc_dim_j.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f006f207-0ac8-40db-ac0f-8fb9a4c7528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:==============================================>       (87 + 12) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|Month|\n",
      "+----+-----+\n",
      "|2024|    3|\n",
      "|2023|    2|\n",
      "|2023|    2|\n",
      "|2023|    6|\n",
      "|2024|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "oms_ibc_dim_j.select(\"Year\",\"Month\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4de1ab24-9859-416a-97b0-d5d2a2655525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms_ibc_dim_j.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "840ddbf4-4c46-4633-81b0-194696d1fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. OMS fact\n",
    "oms_ibc_dim_j = oms_ibc_dim_j.filter(\n",
    "    (col(\"Month\").isNotNull()) & (col(\"Year\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "05e93747-5654-4eff-b745-8c61bf5e96fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE nessie.starschema.ibc_oms_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f69ef4a7-6ae3-4e6d-b75d-d3c52c0c6b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.starschema\")\n",
    "oms_ibc_dim_j.writeTo(\"nessie.starschema.ibc_oms_dim\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c39a73-1955-4897-a929-ff945086f851",
   "metadata": {},
   "source": [
    "#### With CRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed001ee-3c1e-4ffe-a84a-fe7c88a775ef",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "1734f38e-7ae5-46ef-beab-05785208521d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:===================================================>    (23 + 2) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------------------------+-----------------------------+--------------------+----------------------+--------------+-------------------+-----------------------------+\n",
      "|IBC       |Date      |Average Fault Turn-Around Time (mins)|Average Fault Duration (mins)|Total Outages/Faults|Average Punching Delay|Rain Frequency|Most_Occurred_Fault|Most_Occurred_Fault_Frequency|\n",
      "+----------+----------+-------------------------------------+-----------------------------+--------------------+----------------------+--------------+-------------------+-----------------------------+\n",
      "|Nazimabad |2024-04-02|49934.0                              |164.55208611488342           |9                   |0.1111111111111111    |0             |Breaker Tripped    |4                            |\n",
      "|F.B. Area |2024-08-18|80.4                                 |79.8008337020874             |20                  |2.45                  |0             |Job of Operation   |8                            |\n",
      "|Nazimabad |2024-07-30|144.54545454545453                   |144.87121119643703           |33                  |2.393939393939394     |0             |Breaker Tripped    |9                            |\n",
      "|Gulshan   |2024-10-28|7505.75                              |155.24736805966026           |20                  |0.3                   |0             |Lead Burnt Out     |5                            |\n",
      "|Liaqatabad|2024-11-25|1458.3658536585365                   |1502.4365720981505           |41                  |44.4390243902439      |0             |Voltage Fluctuation|21                           |\n",
      "|Nazimabad |2024-12-26|145.63636363636363                   |143.57575832713735           |11                  |2.5454545454545454    |0             |Forced Outage      |6                            |\n",
      "|Liaqatabad|2023-07-18|41369.7                              |348.6561397753264            |20                  |44.15                 |0             |Job of Operation   |5                            |\n",
      "|Liaqatabad|2024-02-16|557.88                               |655.079328918457             |25                  |97.08                 |0             |Voltage Fluctuation|12                           |\n",
      "|F.B. Area |2024-06-05|12816.714285714286                   |128.49135896894666           |28                  |2.5                   |0             |Job of Operation   |14                           |\n",
      "|F.B. Area |2024-07-13|161.83333333333334                   |163.9138894279798            |24                  |2.625                 |0             |Job of Operation   |19                           |\n",
      "|Liaqatabad|2024-10-15|378.2916666666667                    |409.06596997380257           |24                  |35.083333333333336    |0             |Forced Outage      |8                            |\n",
      "|Nazimabad |2024-11-03|10713.692307692309                   |96.46111122767131            |13                  |2.230769230769231     |0             |Breaker Tripped    |5                            |\n",
      "|Liaqatabad|2023-03-04|583.1575342465753                    |571.3390406925384            |146                 |1.9794520547945205    |0             |Commercial         |107                          |\n",
      "|Liaqatabad|2023-01-02|152.44444444444446                   |152.97222137451172           |9                   |0.7777777777777778    |0             |Voltage Fluctuation|5                            |\n",
      "|Liaqatabad|2023-03-02|216.8181818181818                    |220.1121227091009            |22                  |3.727272727272727     |0             |Shutdown           |7                            |\n",
      "|F.B. Area |2023-09-19|35859.76190476191                    |1155.2758346557616           |21                  |0.047619047619047616  |0             |Commercial         |9                            |\n",
      "|Nazimabad |2024-02-11|80.0                                 |75.98461664639987            |13                  |0.23076923076923078   |0             |Job of Operation   |5                            |\n",
      "|Gulshan   |2024-01-17|39979.0                              |183.3589715224046            |14                  |5.571428571428571     |0             |Shutdown           |4                            |\n",
      "|Liaqatabad|2025-02-04|438.0                                |464.2767673261238            |33                  |27.484848484848484    |0             |Job of Operation   |15                           |\n",
      "|Gulshan   |2024-04-02|226.30769230769232                   |224.08461922865646           |13                  |0.8461538461538461    |0             |Shutdown           |8                            |\n",
      "+----------+----------+-------------------------------------+-----------------------------+--------------------+----------------------+--------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Convert timestamp to date only\n",
    "oms_df_reduced = oms_df_reduced.withColumn(\"Date\", F.to_date(\"CreatedOn\"))\n",
    "\n",
    "# Group by IBC and Date to calculate frequency of each fault\n",
    "most_frequent_fault = oms_df_reduced.groupBy(\"IBC\", \"Date\", \"initialoffreason\").agg(\n",
    "    F.count(\"initialoffreason\").alias(\"Most_Occurred_Fault_Frequency\")\n",
    ")\n",
    "\n",
    "# Find the most frequent fault per IBC and Date\n",
    "most_frequent_fault = most_frequent_fault.withColumn(\n",
    "    \"rank\", F.row_number().over(\n",
    "        Window.partitionBy(\"IBC\", \"Date\").orderBy(F.desc(\"Most_Occurred_Fault_Frequency\"))\n",
    "    )\n",
    ").filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Aggregate the other fields for each IBC and Date\n",
    "oms_ibc_dim = oms_df_reduced.groupBy(\"IBC\", \"Date\").agg(\n",
    "    F.avg(\"TAT\").alias(\"Average Fault Turn-Around Time (mins)\"),\n",
    "    F.avg(\"duration\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.count(\"outage_Id\").alias(\"Total Outages/Faults\"),\n",
    "    F.avg(\"PunchingDelay\").alias(\"Average Punching Delay\"),\n",
    "\n",
    "    F.sum(\"israintripping\").alias(\"Rain Frequency\")\n",
    ")\n",
    "\n",
    "# Join both datasets\n",
    "oms_ibc_dim_j = oms_ibc_dim.join(most_frequent_fault, on=[\"IBC\", \"Date\"], how=\"left\")\n",
    "\n",
    "# Final selection\n",
    "oms_ibc_date_dim = oms_ibc_dim_j.select(\n",
    "    \"IBC\",\n",
    "    \"Date\",\n",
    "    \"Average Fault Turn-Around Time (mins)\",\n",
    "    \"Average Fault Duration (mins)\",\n",
    "    \"Total Outages/Faults\",\n",
    "    \"Average Punching Delay\",\n",
    "    \"Rain Frequency\",\n",
    "    F.col(\"initialoffreason\").alias(\"Most_Occurred_Fault\"),\n",
    "    \"Most_Occurred_Fault_Frequency\"\n",
    ")\n",
    "\n",
    "# Show result\n",
    "oms_ibc_date_dim.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "0cf58b7a-7c5a-4fc9-96aa-b3607e35c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading crm_date\n",
    "crm_date_dim = spark.read.table(\"nessie.starschema.crm_date_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "715f5b8c-2e78-439d-b5af-7d21d2e8f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e901a35a-6806-4c68-9040-7b106421ffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+-------------------+-----------------------------+-----------------------------+-------------------------------------+----------------------+--------------+----------------+----------------------------------------+\n",
      "|Date      |IBC       |Total Outages/Faults|Most_Occurred_Fault|Most_Occurred_Fault_Frequency|Average Fault Duration (mins)|Average Fault Turn-Around Time (mins)|Average Punching Delay|Rain Frequency|Complaint Counts|Average Complaint Resolution Time (mins)|\n",
      "+----------+----------+--------------------+-------------------+-----------------------------+-----------------------------+-------------------------------------+----------------------+--------------+----------------+----------------------------------------+\n",
      "|NULL      |NULL      |5561                |NULL               |NULL                         |NULL                         |NULL                                 |NULL                  |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|F.B. Area |11                  |NULL               |8                            |76.84333248138428            |100661.81818181818                   |0.2727272727272727    |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|Gulshan   |16                  |Lead Burnt Out     |10                           |68.47307634353638            |207484.375                           |0.3125                |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|Liaqatabad|11                  |Forced Outage      |3                            |78.93787930228494            |80.81818181818181                    |2.272727272727273     |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|Nazimabad |8                   |Forced Outage      |2                            |59.164584159851074           |60.0                                 |0.5                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|F.B. Area |7                   |NULL               |4                            |74.95714351109096            |77.57142857142857                    |0.14285714285714285   |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|Gulshan   |14                  |Shutdown           |6                            |186.66923357890204           |79087.71428571429                    |1.2142857142857142    |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|Liaqatabad|9                   |Voltage Fluctuation|5                            |152.97222137451172           |152.44444444444446                   |0.7777777777777778    |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|Nazimabad |7                   |Shutdown           |3                            |83.13809558323452            |84.42857142857143                    |1.4285714285714286    |0             |NULL            |NULL                                    |\n",
      "|2023-01-03|F.B. Area |31                  |Commercial         |21                           |534.5349517560775            |535.2903225806451                    |1.3548387096774193    |0             |NULL            |NULL                                    |\n",
      "|2023-01-03|Gulshan   |6                   |NULL               |2                            |103.01388812065125           |131.0                                |0.5                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-03|Liaqatabad|12                  |Shutdown           |11                           |112.42777570088704           |114.66666666666667                   |1.5833333333333333    |0             |NULL            |NULL                                    |\n",
      "|2023-01-03|Nazimabad |12                  |NULL               |4                            |64.5069436232249             |68.75                                |0.3333333333333333    |0             |NULL            |NULL                                    |\n",
      "|2023-01-04|F.B. Area |9                   |Forced Outage      |3                            |71.69999861717224            |122511.0                             |1.8888888888888888    |0             |NULL            |NULL                                    |\n",
      "|2023-01-04|Gulshan   |9                   |Shutdown           |6                            |119.1592583126492            |122.0                                |0.8888888888888888    |0             |NULL            |NULL                                    |\n",
      "|2023-01-04|Liaqatabad|10                  |Job of Operation   |3                            |83.35833311080933            |91.5                                 |0.7                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-04|Nazimabad |10                  |Shutdown           |3                            |81.14999976423051            |110224.7                             |0.4                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-05|F.B. Area |23                  |Forced Outage      |5                            |153.63478001304296           |162.56521739130434                   |0.391304347826087     |0             |NULL            |NULL                                    |\n",
      "|2023-01-05|Gulshan   |8                   |Forced Outage      |3                            |146.6520848274231            |150.125                              |0.375                 |0             |NULL            |NULL                                    |\n",
      "|2023-01-05|Liaqatabad|30                  |Commercial         |12                           |617.7716508865357            |622.7                                |0.6333333333333333    |0             |NULL            |NULL                                    |\n",
      "+----------+----------+--------------------+-------------------+-----------------------------+-----------------------------+-------------------------------------+----------------------+--------------+----------------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_ibc_crm_oms = oms_ibc_date_dim.alias(\"oms_date\").join(\n",
    "    crm_date_dim.alias(\"crm_date\"), \n",
    "    (F.col(\"oms_date.Date\") == F.col(\"crm_date.Date\")) &\n",
    "    (F.col(\"oms_date.IBC\") == F.col(\"crm_date.IBCName\")),#only ibc names in oms\n",
    "    how='outer'#retaining all info \n",
    "\n",
    "\n",
    "     \n",
    ").groupBy(\"oms_date.Date\",\"oms_date.IBC\" ).agg(\n",
    "    # First value of relevant oms and crm columns\n",
    "    F.first(\"oms_date.Total Outages/Faults\").alias(\"Total Outages/Faults\"),\n",
    "    F.first(\"oms_date.Most_Occurred_Fault\").alias(\"Most_Occurred_Fault\"),\n",
    "    F.first(\"oms_date.Most_Occurred_Fault_Frequency\").alias(\"Most_Occurred_Fault_Frequency\"),\n",
    "    F.first(\"oms_date.Average Fault Duration (mins)\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.first(\"oms_date.Average Fault Turn-Around Time (mins)\").alias(\"Average Fault Turn-Around Time (mins)\"),\n",
    "    F.first(\"oms_date.Average Punching Delay\").alias(\"Average Punching Delay\"),\n",
    "    F.first(\"oms_date.Rain Frequency\").alias(\"Rain Frequency\"),\n",
    "    F.first(\"crm_date.Complaint Counts\").alias(\"Complaint Counts\"),    \n",
    "    F.first(\"crm_date.Average Complaint Resolution Time (mins)\").alias(\"Average Complaint Resolution Time (mins)\")\n",
    " \n",
    ")\n",
    "    \n",
    "date_ibc_crm_oms.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "cd70f0e1-585f-4dc1-bc88-ebb3dd9135b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE nessie.starschema.date_ibc_crm_oms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "f1a5b53f-d7c2-416f-831c-48c0f9132a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "date_ibc_crm_oms.writeTo(\"nessie.starschema.date_ibc_crm_oms\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1817a-374c-4f22-99ec-3a5fcb8047e2",
   "metadata": {},
   "source": [
    "### Feeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd93703-7665-4b61-abb6-2ea7ccf15136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ONLY \n",
    "oms_df_reduced = spark.read.table(\"nessie.starschema.oms_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c8fe327c-d9a3-416f-bf58-d7ae5ed55669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-------------------------------------+-----------------------------+--------------------+----------------------+--------------+-------------------+-----------------------------+\n",
      "|Date      |Feeder ID|Feeder Name|Average Fault Turn-Around Time (mins)|Average Fault Duration (mins)|Total Outages/Faults|Average Punching Delay|Rain Frequency|Most_Occurred_Fault|Most_Occurred_Fault_Frequency|\n",
      "+----------+---------+-----------+-------------------------------------+-----------------------------+--------------------+----------------------+--------------+-------------------+-----------------------------+\n",
      "|2023-01-16|102      |EAGLE SQUAD|54.0                                 |51.849998474121094           |1                   |1.0                   |0             |NULL               |1                            |\n",
      "|2023-02-09|102      |EAGLE SQUAD|106.0                                |104.73332977294922           |1                   |0.0                   |0             |NULL               |1                            |\n",
      "|2023-04-19|102      |EAGLE SQUAD|100.33333333333333                   |109.62777900695801           |3                   |10.0                  |0             |Lead Burnt Out     |2                            |\n",
      "|2023-05-09|102      |EAGLE SQUAD|116.66666666666667                   |124.11666584014893           |3                   |8.333333333333334     |0             |Job of Operation   |1                            |\n",
      "|2023-05-27|102      |EAGLE SQUAD|15.0                                 |13.733333587646484           |1                   |1.0                   |0             |Forced Outage      |1                            |\n",
      "|2023-05-29|102      |EAGLE SQUAD|225.83333333333334                   |220.94166628519693           |6                   |0.3333333333333333    |0             |NULL               |3                            |\n",
      "|2023-05-30|102      |EAGLE SQUAD|24.0                                 |15.0                         |1                   |1.0                   |0             |Planned Outage     |1                            |\n",
      "|2023-06-02|102      |EAGLE SQUAD|40.0                                 |37.79999923706055            |1                   |0.0                   |0             |NULL               |1                            |\n",
      "|2023-06-04|102      |EAGLE SQUAD|121.0                                |100.60000165303548           |3                   |0.6666666666666666    |0             |NULL               |2                            |\n",
      "|2023-06-08|102      |EAGLE SQUAD|86.0                                 |67.46666717529297            |1                   |1.0                   |0             |NULL               |1                            |\n",
      "|2023-06-09|102      |EAGLE SQUAD|19.0                                 |6.25                         |1                   |2.0                   |0             |NULL               |1                            |\n",
      "|2023-06-21|102      |EAGLE SQUAD|193.5                                |187.78333282470703           |2                   |0.5                   |0             |Shutdown           |1                            |\n",
      "|2023-06-22|102      |EAGLE SQUAD|19.0                                 |12.583333492279053           |2                   |6.0                   |0             |Forced Outage      |2                            |\n",
      "|2023-07-15|102      |EAGLE SQUAD|52.0                                 |50.650001525878906           |1                   |1.0                   |0             |Forced Outage      |1                            |\n",
      "|2023-07-24|102      |EAGLE SQUAD|133.0                                |132.8000030517578            |1                   |0.0                   |0             |NULL               |1                            |\n",
      "|2023-08-07|102      |EAGLE SQUAD|308.0                                |305.2777786254883            |3                   |0.3333333333333333    |0             |Breaker Tripped    |1                            |\n",
      "|2023-08-18|102      |EAGLE SQUAD|68.0                                 |69.53333282470703            |1                   |1.0                   |0             |Forced Outage      |1                            |\n",
      "|2023-08-21|102      |EAGLE SQUAD|15.0                                 |14.283333778381348           |1                   |0.0                   |0             |Planned Outage     |1                            |\n",
      "|2023-08-27|102      |EAGLE SQUAD|112.0                                |61.04999923706055            |1                   |0.0                   |0             |NULL               |1                            |\n",
      "|2023-09-04|102      |EAGLE SQUAD|6.0                                  |6.449999809265137            |1                   |1.0                   |0             |Forced Outage      |1                            |\n",
      "+----------+---------+-----------+-------------------------------------+-----------------------------+--------------------+----------------------+--------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Convert timestamp to date only\n",
    "oms_df_reduced = oms_df_reduced.withColumn(\"Date\", F.to_date(\"CreatedOn\"))\n",
    "\n",
    "# Group by Feeder ID and Date to calculate frequency of each fault\n",
    "most_frequent_fault = oms_df_reduced.groupBy(\"fdr_Id\", \"Date\", \"initialoffreason\").agg(\n",
    "    F.count(\"initialoffreason\").alias(\"Most_Occurred_Fault_Frequency\")\n",
    ")\n",
    "\n",
    "# Find the most frequent fault per Feeder ID and Date\n",
    "most_frequent_fault = most_frequent_fault.withColumn(\n",
    "    \"rank\", F.row_number().over(\n",
    "        Window.partitionBy(\"fdr_Id\", \"Date\").orderBy(F.desc(\"Most_Occurred_Fault_Frequency\"))\n",
    "    )\n",
    ").filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Aggregate the other fields for each Feeder ID and Date\n",
    "oms_feeder_dim = oms_df_reduced.groupBy(\"fdr_Id\", \"Date\").agg(\n",
    "    F.first(\"fdr_name\").alias(\"Feeder Name\"),\n",
    "    F.avg(\"TAT\").alias(\"Average Fault Turn-Around Time (mins)\"),\n",
    "    F.avg(\"duration\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.count(\"outage_Id\").alias(\"Total Outages/Faults\"),\n",
    "    F.avg(\"PunchingDelay\").alias(\"Average Punching Delay\"),\n",
    "    F.sum(\"israintripping\").alias(\"Rain Frequency\")\n",
    ")\n",
    "\n",
    "# Join both datasets\n",
    "oms_feeder_dim_j = oms_feeder_dim.join(most_frequent_fault, on=[\"fdr_Id\", \"Date\"], how=\"left\")\n",
    "\n",
    "# Final selection\n",
    "oms_feeder_date_dim = oms_feeder_dim_j.select(\n",
    "    \"Date\",\n",
    "    F.col(\"fdr_Id\").alias(\"Feeder ID\"),\n",
    "    \"Feeder Name\",\n",
    "    \"Average Fault Turn-Around Time (mins)\",\n",
    "    \"Average Fault Duration (mins)\",\n",
    "    \"Total Outages/Faults\",\n",
    "    \"Average Punching Delay\",\n",
    "    \"Rain Frequency\",\n",
    "    F.col(\"initialoffreason\").alias(\"Most_Occurred_Fault\"),\n",
    "    \"Most_Occurred_Fault_Frequency\"\n",
    ")\n",
    "\n",
    "# Show result\n",
    "oms_feeder_date_dim.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "002a5969-baa5-4848-978a-3a2edefced48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading crm_date\n",
    "crm_feeder_dim = spark.read.table(\"nessie.starschema.crm_feeder_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "a2a6e02a-e25a-480c-a743-c7e773dfb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------------------+--------------------+-------------------+-----------------------------+-----------------------------+-------------------------------------+----------------------+--------------+----------------+----------------------------------------+\n",
      "|Date      |Feeder ID|Feeder Name                     |Total Outages/Faults|Most_Occurred_Fault|Most_Occurred_Fault_Frequency|Average Fault Duration (mins)|Average Fault Turn-Around Time (mins)|Average Punching Delay|Rain Frequency|Complaint Counts|Average Complaint Resolution Time (mins)|\n",
      "+----------+---------+--------------------------------+--------------------+-------------------+-----------------------------+-----------------------------+-------------------------------------+----------------------+--------------+----------------+----------------------------------------+\n",
      "|2023-01-01|109      |EXPO CENTER                     |1                   |NULL               |1                            |15.783333778381348           |35.0                                 |1.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|117      |QASIMABAD DEGREE COLLGE (QDC)   |1                   |Service Jumper     |1                            |158.31666564941406           |158.0                                |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|1213     |NESCOS                          |1                   |Shutdown           |1                            |127.78333282470703           |128.0                                |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|1266     |FED CAPITAL-4                   |1                   |Lead Burnt Out     |1                            |11.666666984558105           |12.0                                 |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|2023     |BANA PALACE                     |1                   |NULL               |1                            |73.73332977294922            |77.0                                 |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|2139     |TRIMAX HOSPITAL(OLD COMP SCHOOL)|2                   |Forced Outage      |2                            |14.816666603088379           |17.5                                 |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|316      |ORIENT BUILDERS                 |1                   |Forced Outage      |1                            |31.600000381469727           |42.0                                 |1.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|3461     |ISMAILIA FLAT                   |2                   |Shutdown           |1                            |37.875000953674316           |37.5                                 |0.5                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|347      |AL-NAJEEB                       |2                   |Lead Burnt Out     |2                            |67.7249984741211             |68.0                                 |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|558      |TAYYA HOTEL (OLD LIAQATABAD-1)  |2                   |Forced Outage      |1                            |38.508334159851074           |54.5                                 |1.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|657      |PAPOSH NAGAR                    |2                   |NULL               |1                            |37.15000057220459            |37.0                                 |0.5                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|791      |USMANIA COLONY                  |1                   |NULL               |1                            |71.36666870117188            |75.0                                 |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|813      |SHAH-E-KARBALA                  |2                   |Lead Burnt Out     |1                            |114.15833377838135           |114.5                                |0.5                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-01|99       |BARADARI                        |4                   |Lead Burnt Out     |4                            |9.116666555404663            |553141.25                            |0.25                  |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|117      |QASIMABAD DEGREE COLLGE (QDC)   |2                   |Voltage Fluctuation|2                            |134.19166564941406           |134.0                                |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|2012     |DEHLI COLLEGE                   |1                   |NULL               |1                            |42.96666717529297            |44.0                                 |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|2156     |MEMON MASJID                    |1                   |Forced Outage      |1                            |41.20000076293945            |47.0                                 |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|3177     |PAK SWISS                       |1                   |LT Cable Fault     |1                            |540.6333618164062            |541.0                                |0.0                   |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|326      |EXHIBITION                      |1                   |Forced Outage      |1                            |5.7166666984558105           |1.0                                  |14.0                  |0             |NULL            |NULL                                    |\n",
      "|2023-01-02|334      |SHANTI NAGAR                    |1                   |NULL               |1                            |12.949999809265137           |19.0                                 |0.0                   |0             |NULL            |NULL                                    |\n",
      "+----------+---------+--------------------------------+--------------------+-------------------+-----------------------------+-----------------------------+-------------------------------------+----------------------+--------------+----------------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_feeder_crm_oms = oms_feeder_date_dim.alias(\"oms_feeder\").join(\n",
    "    crm_feeder_dim.alias(\"crm_feeder\"), \n",
    "    (F.col(\"oms_feeder.Date\") == F.col(\"crm_feeder.Date\")) &\n",
    "    (F.col(\"oms_feeder.Feeder ID\") == F.col(\"crm_feeder.Feeder ID\")),#only ibc names in oms\n",
    "    how='outer'#retaining all info \n",
    "\n",
    "\n",
    "     \n",
    ").groupBy(\"oms_feeder.Date\",\"oms_feeder.Feeder ID\" ).agg(\n",
    "    # First value of relevant oms and crm columns\n",
    "    F.first(\"oms_feeder.Feeder Name\").alias(\"Feeder Name\"),\n",
    "    F.first(\"oms_feeder.Total Outages/Faults\").alias(\"Total Outages/Faults\"),\n",
    "    F.first(\"oms_feeder.Most_Occurred_Fault\").alias(\"Most_Occurred_Fault\"),\n",
    "    F.first(\"oms_feeder.Most_Occurred_Fault_Frequency\").alias(\"Most_Occurred_Fault_Frequency\"),\n",
    "    F.first(\"oms_feeder.Average Fault Duration (mins)\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.first(\"oms_feeder.Average Fault Turn-Around Time (mins)\").alias(\"Average Fault Turn-Around Time (mins)\"),\n",
    "    F.first(\"oms_feeder.Average Punching Delay\").alias(\"Average Punching Delay\"),\n",
    "    F.first(\"oms_feeder.Rain Frequency\").alias(\"Rain Frequency\"),\n",
    "    F.first(\"crm_feeder.Complaint Counts\").alias(\"Complaint Counts\"),    \n",
    "    F.first(\"crm_feeder.Average Complaint Resolution Time (mins)\").alias(\"Average Complaint Resolution Time (mins)\")\n",
    " \n",
    ")\n",
    "    \n",
    "date_feeder_crm_oms.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9a020751-dbee-4246-ade4-578db6b26ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE nessie.starschema.date_feeder_crm_oms\")\n",
    "date_feeder_crm_oms.writeTo(\"nessie.starschema.date_feeder_crm_oms\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f8cbd-1527-435d-9987-085ab2c69667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a66c3a2-88ae-411d-b063-a34ebcf24090",
   "metadata": {},
   "source": [
    "#### merging crm ibc with customer IBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4471f80d-621c-4b85-8869-ff43e1f61a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "crm_ibc_dim = spark.read.table(\"nessie.starschema.ibc_crm_dim\")\n",
    "oms_ibc_dim_j = spark.read.table(\"nessie.starschema.ibc_oms_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af2f3fdc-1228-49ae-80b6-5fc626d06e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so its easier to drop\n",
    "crm_ibc_dim = crm_ibc_dim \\\n",
    "    .withColumnRenamed(\"Year\", \"Year1\") \\\n",
    "    .withColumnRenamed(\"Month\", \"Month1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1257fdb5-a9b3-41c4-aef7-7aa99867f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a full outer join to get rows from both crm_ibc_dim and oms_ibc_dim\n",
    "# oms is much lesser than crm \n",
    "\n",
    "ibc_oms_crm_dim = oms_ibc_dim_j.join(\n",
    "    crm_ibc_dim,\n",
    "    on=[oms_ibc_dim_j.IBC==crm_ibc_dim.IBCName,oms_ibc_dim_j.Year==crm_ibc_dim.Year1,oms_ibc_dim_j.Month==crm_ibc_dim.Month1],  # Ensure you're joining on the IBCName column\n",
    "    how=\"inner\"      # Use \"left\" because oms has lesser data than crm  and we only want mapping of ibcs that exist in both \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fffcf05d-76ae-4a37-9474-6589d1f1277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+-------------------------------------+-----------------------------+--------------------+--------------+-------------------+-----------------------------+-------+----------------+----------------------------------------+\n",
      "|Month|Year|IBC       |Average Fault Turn-Around Time (mins)|Average Fault Duration (mins)|Total Outages/Faults|Rain Frequency|Most_Occurred_Fault|Most_Occurred_Fault_Frequency|IBCCode|Complaint Counts|Average Complaint Resolution Time (mins)|\n",
      "+-----+----+----------+-------------------------------------+-----------------------------+--------------------+--------------+-------------------+-----------------------------+-------+----------------+----------------------------------------+\n",
      "|8    |2023|Nazimabad |16946.074162679426                   |75.52139364193879            |418                 |0             |Forced Outage      |144                          |136    |2730            |110.82857149376537                      |\n",
      "|5    |2023|Nazimabad |6956.117206982543                    |81.41825798707991            |401                 |0             |NULL               |94                           |136    |3606            |115.84658907207594                      |\n",
      "|9    |2023|Liaqatabad|8922.616174582798                    |493.45591046036066           |779                 |0             |Job of Operation   |126                          |127    |4413            |133.8341264748233                       |\n",
      "|8    |2024|Gulshan   |3166.9110122358175                   |216.62205297197843           |899                 |0             |Lead Burnt Out     |210                          |123    |6453            |323.6304260506944                       |\n",
      "|6    |2024|Liaqatabad|420.91503084688725                   |325.2773448057546            |3566                |0             |Commercial         |2470                         |127    |5588            |338.9302084408407                       |\n",
      "|5    |2024|Nazimabad |3207.5613519470976                   |122.3295554905026            |1361                |0             |Commercial         |848                          |136    |4535            |151.09230436193116                      |\n",
      "|6    |2023|Nazimabad |9132.02823179792                     |155.2396145800839            |673                 |0             |Forced Outage      |153                          |136    |4815            |120.12186911472527                      |\n",
      "|6    |2024|Nazimabad |4507.503456221199                    |130.3563982380452            |868                 |0             |Breaker Tripped    |213                          |136    |5975            |183.64629344505246                      |\n",
      "|4    |2023|Nazimabad |3060.5333333333333                   |276.3817872363468            |345                 |0             |Commercial         |142                          |136    |2276            |99.8783829894133                        |\n",
      "|4    |2023|Liaqatabad|9679.94453507341                     |341.0218026043703            |613                 |0             |Commercial         |160                          |127    |1987            |149.03673886748854                      |\n",
      "|7    |2023|Gulshan   |10498.176223776223                   |119.944239577727             |715                 |0             |Lead Burnt Out     |234                          |123    |5143            |152.6985417374975                       |\n",
      "|9    |2023|F.B. Area |21062.67391304348                    |251.5752162145332            |598                 |0             |Job of Operation   |167                          |132    |3118            |130.3227709563135                       |\n",
      "|9    |2024|Gulshan   |2338.9684210526316                   |202.59784208755966           |570                 |0             |Lead Burnt Out     |141                          |123    |4061            |356.35892674905915                      |\n",
      "|7    |2023|Liaqatabad|3886.7138364779876                   |505.7660868140898            |954                 |0             |Job of Operation   |176                          |127    |5326            |161.66124668508107                      |\n",
      "|6    |2023|F.B. Area |12717.103280680438                   |125.43956042824338           |823                 |0             |Job of Operation   |250                          |132    |6226            |130.2874076129193                       |\n",
      "|7    |2023|Nazimabad |12150.481178396072                   |84.47472314333599            |611                 |0             |NULL               |152                          |136    |4900            |112.85497943201844                      |\n",
      "|6    |2023|Liaqatabad|9810.945874934314                    |362.7612014443759            |1903                |0             |Commercial         |1000                         |127    |4470            |156.31476499668972                      |\n",
      "|4    |2024|Nazimabad |2937.603058994902                    |126.3896260085868            |1373                |0             |Commercial         |1113                         |136    |2495            |112.30725442818506                      |\n",
      "|8    |2024|Nazimabad |11063.99436936937                    |143.19730963927415           |888                 |0             |Commercial         |297                          |136    |4415            |157.8477227234824                       |\n",
      "|5    |2024|Gulshan   |6812.077694235589                    |208.54272937821224           |798                 |0             |Lead Burnt Out     |291                          |123    |6187            |248.04457733934407                      |\n",
      "+-----+----+----------+-------------------------------------+-----------------------------+--------------------+--------------+-------------------+-----------------------------+-------+----------------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the final merged result\n",
    "ibc_oms_crm_dim.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c76dab68-c10c-4842-a03a-7f11b9fba21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_oms_crm_dim = ibc_oms_crm_dim.drop(\"Year1\", \"Month1\", \"IBCName\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9518f236-e05a-437e-956d-a3511ab0cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_oms_crm_dim = ibc_oms_crm_dim.repartition(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5518a3ee-084d-4557-aecb-8274996a6fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_oms_crm_dim.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7a34110-ddfb-4bf2-a93d-7c10899e176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE nessie.starschema.ibc_oms_crm_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40c53349-cbab-4744-91f7-ca47bac40254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.starschema\")\n",
    "ibc_oms_crm_dim.writeTo(\"nessie.starschema.ibc_oms_crm_dim\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "b40616bb-7dcd-410b-ba4f-9ea863883e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining ageing columns \n",
    "merged_ibc_dim_omscrm = merged_ibc_dim_omscrm.withColumn(\n",
    "    \"Total_IRB_Amount\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Total_IRB_Units\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Average_IRB_Amount\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Average_IRB_Units\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Total_Sanctioned_Load\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Total_Connected_Load\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Average_Sanctioned_Load\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Average_Connected_Load\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Total_Current_Amount\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Average_Current_Amount\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Total_Current_Units\", F.lit(0).cast(\"double\")\n",
    ").withColumn(\n",
    "    \"Average_Current_Units\", F.lit(0).cast(\"double\")\n",
    ")\n",
    "\n",
    "# should be able to add ageing with\n",
    "# merged_ibc_dim_ageing = ageing_ibc_dim.join(\n",
    "#     merged_ibc_dim_omscrm,\n",
    "#     on=[\"IBCName\"],  # Ensure you're joining on the IBCName column\n",
    "#     how=\"left\"       # Use \"left\" join to keep all rows from oms_ibc_dim and match with crm_ibc_dim\n",
    "# )\n",
    "\n",
    "# # Display the final merged result\n",
    "# merged_ibc_dim_omscrm.show(truncate=False)\n",
    "\n",
    "##ingest it back into ibc_dim\n",
    "# merged_ibc_dim_omscrm.write.mode(\"overwrite\").saveAsTable(\"nessie.starschema.ibc_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "16779784-41af-4606-ad3a-c9cc77368d49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      IBCName|\n",
      "+-------------+\n",
      "|      Defence|\n",
      "|       Saddar|\n",
      "|  NEW KARACHI|\n",
      "|      Liyar-I|\n",
      "|     Liyar-II|\n",
      "|      Gulshan|\n",
      "|         KIMZ|\n",
      "|        Gadap|\n",
      "|       Landhi|\n",
      "|         SIMZ|\n",
      "|    F.B. Area|\n",
      "|  N.Nazimabad|\n",
      "|  Shah Faisal|\n",
      "|     Orangi-I|\n",
      "|    Nazimabad|\n",
      "|      Korangi|\n",
      "|          PSC|\n",
      "|      Clifton|\n",
      "|North Karachi|\n",
      "|     Jauhar-I|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+\n",
      "|        IBC|\n",
      "+-----------+\n",
      "|Liaquatabad|\n",
      "|    Gulshan|\n",
      "|  Nazimabad|\n",
      "|    FB Area|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming crm_df and oms_df are Spark DataFrames\n",
    "\n",
    "distinct_ibcs_crm = crm_df_reduced.select(\"IBCName\").distinct()\n",
    "distinct_ibcs_oms = oms_df_reduced.select(\"IBC\").distinct()\n",
    "\n",
    "# Show the results\n",
    "distinct_ibcs_crm.show(), distinct_ibcs_oms.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7eb0c-98f4-414b-a0e7-c139b74dd07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "1d8a07a7-3149-409a-b0cd-af2e5bcd2d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    }
   ],
   "source": [
    "# add to starschema as ibc_dim\n",
    "merged_ibc_dim_omscrm.write.mode(\"overwrite\").saveAsTable(\"nessie.starschema.ibc_dim\")\n",
    "# merged_ibc_dim_omscrm.writeTo(\"nessie.starschema.ibc_dim\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5ab4548b-b3fa-44ab-8ef2-fe1b5d36523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+-------------+-------------+----------------+---------------+-------------------+--------+---------+-------+------+--------------+------+----------------+-------+-------+----+-----+---+-------------+------------+-------------+-------------+-------------+--------+-----------+--------------+----------+---------------+-----+-----+-------------+------------------+-------------+-------------+----+----+----+----+----+----+----+----+----+-----+----+\n",
      "|outage_Id|NetworkLevel|OutageType|outageSubType|htclosingtype|htclosingsubtype|htclosingreason|   initialoffreason|faultstr|grid_name|pt_name|fdr_Id|      fdr_name|dts_Id|        dts_name|cluster|    IBC|load|Relay|TAT|PunchingDelay|OutageStatus|    Createdon|punchCreateAt| punchCloseAt|duration|isemergency|israintripping|cluster_Id|closing_remarks|saifi|saidi|loss_category|consumer_count_fdr|lastupdatedon|planned_sd_Id|_c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|_c44|month|year|\n",
      "+---------+------------+----------+-------------+-------------+----------------+---------------+-------------------+--------+---------+-------+------+--------------+------+----------------+-------+-------+----+-----+---+-------------+------------+-------------+-------------+-------------+--------+-----------+--------------+----------+---------------+-----+-----+-------------+------------------+-------------+-------------+----+----+----+----+----+----+----+----+----+-----+----+\n",
      "|   721295|          LT|       DTS|      DTS Off|         NULL|            NULL|           NULL|Voltage Fluctuation|    NULL|     NULL|   NULL|  3146|GALLANT SUMMIT|529533|COUNCILLOR SAQIB|Gulshan|Gulshan|NULL| NULL| 96|            0|   Completed|1/1/2023 0:09|1/1/2023 0:09|1/1/2023 1:45| 1:36:06|       NULL|            NO|         7|           NULL| NULL| NULL|           LL|              NULL|  1/1/23 1:45|         NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL|NULL|\n",
      "+---------+------------+----------+-------------+-------------+----------------+---------------+-------------------+--------+---------+-------+------+--------------+------+----------------+-------+-------+----+-----+---+-------------+------------+-------------+-------------+-------------+--------+-----------+--------------+----------+---------------+-----+-----+-------------+------------------+-------------+-------------+----+----+----+----+----+----+----+----+----+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 22:39:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: outage_Id, NetworkLevel, OutageType, outageSubType, htclosingtype, htclosingsubtype, htclosingreason, initialoffreason, faultstr, grid_name, pt_name, fdr_Id, fdr_name, dts_Id, dts_name, cluster, IBC, load, Relay, TAT, PunchingDelay, OutageStatus, Createdon, punchCreateAt, punchCloseAt, duration, isemergency, israintripping, cluster_Id, closing_remarks, saifi, saidi, loss_category, consumer_count_fdr, lastupdatedon, planned_sd_Id, , , , , , , , , \n",
      " Schema: outage_Id, NetworkLevel, OutageType, outageSubType, htclosingtype, htclosingsubtype, htclosingreason, initialoffreason, faultstr, grid_name, pt_name, fdr_Id, fdr_name, dts_Id, dts_name, cluster, IBC, load, Relay, TAT, PunchingDelay, OutageStatus, Createdon, punchCreateAt, punchCloseAt, duration, isemergency, israintripping, cluster_Id, closing_remarks, saifi, saidi, loss_category, consumer_count_fdr, lastupdatedon, planned_sd_Id, _c36, _c37, _c38, _c39, _c40, _c41, _c42, _c43, _c44\n",
      "Expected: _c36 but found: \n",
      "CSV file: file:///mnt/HabibData/OMSRequestedData.csv\n"
     ]
    }
   ],
   "source": [
    "df_reduced.withColumn(\"month\", F.month(\"Createdon\")).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "3287e427-cac0-4078-bbf4-6f784c9e342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 733:=================================================>    (92 + 8) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------------------+-----------------------------+--------------------+----------------------+-------------------+-----------------------------+\n",
      "|Month_|Year_|Average Turn-Around Time (mins)|Average Fault Duration (mins)|Total Outages/Faults|Rain Frequency (23-24)|Most_Occurred_Fault|Most_Occurred_Fault_Frequency|\n",
      "+------+-----+-------------------------------+-----------------------------+--------------------+----------------------+-------------------+-----------------------------+\n",
      "|7     |2024 |3018.4881141045958             |330.58424819474504           |1893                |5                     |Job of Operation   |468                          |\n",
      "|12    |2024 |2401.005395683453              |1079.3308405302034           |1112                |0                     |Forced Outage      |292                          |\n",
      "|8     |2023 |10472.540659340659             |180.1895697877508            |910                 |5                     |NULL               |217                          |\n",
      "|9     |2023 |15033.29887054735              |384.6883152108243            |1151                |1                     |NULL               |218                          |\n",
      "|3     |2024 |3090.1300448430493             |215.43662015764542           |3122                |23                    |Commercial         |2266                         |\n",
      "|7     |2023 |12057.780625                   |218.7580107434469            |1600                |27                    |NULL               |308                          |\n",
      "|NULL  |NULL |9511.906790004497              |319.570328393235             |31134               |834                   |NULL               |NULL                         |\n",
      "|5     |2024 |1844.2611903590753             |179.05412984896716           |4066                |0                     |Commercial         |2671                         |\n",
      "|6     |2023 |11079.120853983988             |246.9802826210396            |2623                |23                    |Commercial         |1087                         |\n",
      "|9     |2024 |2372.57343550447               |249.57855942710754           |1566                |0                     |Commercial         |479                          |\n",
      "|10    |2024 |3077.9882671480145             |259.78838355252236           |1108                |0                     |Forced Outage      |224                          |\n",
      "|2     |2024 |12209.880658436214             |524.0680871341417            |972                 |21                    |Job of Operation   |161                          |\n",
      "|3     |2023 |10007.93779264214              |608.7746276847067            |1495                |47                    |Commercial         |464                          |\n",
      "|2     |2023 |8379.639916839917              |588.8006832313728            |2405                |0                     |Commercial         |1422                         |\n",
      "|11    |2023 |14441.666338582678             |463.4474859758762            |1016                |13                    |NULL               |205                          |\n",
      "|4     |2023 |6882.463654223969              |293.3567288501371            |1018                |10                    |Commercial         |352                          |\n",
      "|5     |2023 |9601.952288218112              |300.54738496597975           |2054                |0                     |Commercial         |765                          |\n",
      "|1     |2024 |15701.134730538923             |411.8980196385735            |1002                |8                     |NULL               |164                          |\n",
      "|11    |2024 |3035.161666666667              |567.0919550120324            |1200                |0                     |Forced Outage      |287                          |\n",
      "|10    |2023 |18658.2860727729               |459.1528896304308            |1594                |0                     |NULL               |234                          |\n",
      "+------+-----+-------------------------------+-----------------------------+--------------------+----------------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#month\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window  # Window specification for ranking\n",
    "\n",
    "# Extract month and year from Createdon column\n",
    "temp_df = df_reduced.withColumn(\"month\", F.month(\"Createdon\")) \\\n",
    "               .withColumn(\"year\", F.year(\"Createdon\"))\n",
    "\n",
    "# First, calculate the most frequent fault in \"initialoffreason\" for each \"month\"\n",
    "most_frequent_fault_month = temp_df.groupBy(\"year\", \"month\", \"initialoffreason\").agg(\n",
    "    F.count(\"initialoffreason\").alias(\"Most_Occurred_Fault_Frequency\")\n",
    ")\n",
    "\n",
    "# Now, for each year-month, we want the fault with the highest count\n",
    "most_frequent_fault_month = most_frequent_fault_month.withColumn(\n",
    "    \"rank\", F.row_number().over(\n",
    "        Window.partitionBy(\"year\", \"month\").orderBy(F.desc(\"Most_Occurred_Fault_Frequency\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter to get only the most frequent fault for each year-month\n",
    "most_frequent_fault_month = most_frequent_fault_month.filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "most_frequent_fault_month = most_frequent_fault_month.withColumnRenamed(\"month\", \"month_Fault\") \\\n",
    "                                                   .withColumnRenamed(\"year\", \"year_Fault\")\n",
    "\n",
    "# Now, create the dimension (dim) for Month of `Createdon`\n",
    "oms_dim_month = temp_df.groupBy(\"year\", \"month\").agg(\n",
    "    F.first(\"month\").alias(\"Month_\"),\n",
    "    F.first(\"year\").alias(\"Year_\"),\n",
    "    F.avg(\"TAT\").alias(\"Average Turn-Around Time (mins)\"),\n",
    "    F.avg(\"duration\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.count(\"outage_Id\").alias(\"Total Outages/Faults\"),\n",
    "    F.sum(\"israintripping\").alias(\"Rain Frequency (23-24)\")\n",
    ")\n",
    "\n",
    "# Join the most frequent fault with the Month-based aggregation\n",
    "oms_dim_month = oms_dim_month.join(most_frequent_fault_month, \n",
    "                       on=[oms_dim_month.Month_ == most_frequent_fault_month.month_Fault, \n",
    "                           oms_dim_month.Year_ == most_frequent_fault_month.year_Fault], \n",
    "                       how=\"left\")\n",
    "oms_dim_month = oms_dim_month.select(\n",
    "    F.col(\"month_Fault\").alias(\"Month_\"), \n",
    "    F.col(\"year_Fault\").alias(\"Year_\"),\n",
    "    \"Average Turn-Around Time (mins)\", \"Average Fault Duration (mins)\", \n",
    "    \"Total Outages/Faults\", \"Rain Frequency (23-24)\", \n",
    "     F.col(\"initialoffreason\").alias(\"Most_Occurred_Fault\"), \n",
    "    \"Most_Occurred_Fault_Frequency\"\n",
    ")\n",
    "\n",
    "# Display the final result for the Month of Createdon dimension\n",
    "oms_dim_month.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "f273c97e-a63b-4f79-afd5-b635c3a472e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Month_', 'int'),\n",
       " ('Year_', 'int'),\n",
       " ('Average Turn-Around Time (mins)', 'double'),\n",
       " ('Average Fault Duration (mins)', 'double'),\n",
       " ('Total Outages/Faults', 'bigint'),\n",
       " ('Rain Frequency (23-24)', 'bigint'),\n",
       " ('Most_Occurred_Fault', 'string'),\n",
       " ('Most_Occurred_Fault_Frequency', 'bigint')]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oms_dim_month.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "d2044e6d-e5dd-48c9-815a-b483dd4ee616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 669:=======================================>             (75 + 12) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------------+-----------------------------+--------------------+----------------------+-------------------+-----------------------------+\n",
      "|Date      |Average Turn-Around Time (mins)|Average Fault Duration (mins)|Total Outages/Faults|Rain Frequency (23-24)|Most_Occurred_Fault|Most_Occurred_Fault_Frequency|\n",
      "+----------+-------------------------------+-----------------------------+--------------------+----------------------+-------------------+-----------------------------+\n",
      "|2023-06-22|6418.269503546099              |287.53844794780014           |141                 |0                     |Commercial         |54                           |\n",
      "|2023-11-08|370.27272727272725             |380.2393975691362            |33                  |0                     |Planned Outage     |6                            |\n",
      "|2024-05-30|8481.203007518798              |265.40641169181237           |133                 |0                     |Commercial         |73                           |\n",
      "|2024-06-12|10589.26                       |200.14914342054388           |100                 |0                     |Commercial         |55                           |\n",
      "|2023-02-25|6914.865853658536              |650.5460117572655            |164                 |0                     |Commercial         |136                          |\n",
      "|2023-06-18|8891.838383838383              |163.17108758751834           |99                  |0                     |Commercial         |55                           |\n",
      "|2024-10-24|3921.3333333333335             |282.77479613699563           |42                  |0                     |Shutdown           |8                            |\n",
      "|2024-11-02|363.741935483871               |359.5758070710205            |31                  |0                     |Shutdown           |11                           |\n",
      "|2023-02-08|295.4727272727273              |259.3663667053887            |165                 |0                     |Commercial         |128                          |\n",
      "|2023-06-23|264.4166666666667              |263.6861061652501            |72                  |0                     |Commercial         |21                           |\n",
      "|2024-04-20|128.01149425287358             |129.88052410184196           |261                 |0                     |Commercial         |239                          |\n",
      "|2024-08-30|188.48979591836735             |185.64455746631234           |49                  |0                     |NULL               |15                           |\n",
      "|2023-03-24|35376.03571428572              |95.49259316479719            |28                  |0                     |Shutdown           |10                           |\n",
      "|2023-09-27|898.3636363636364              |911.2824268254367            |55                  |0                     |DC - Disconnected  |16                           |\n",
      "|2024-09-10|387.35714285714283             |386.74643365542096           |42                  |0                     |Voltage Fluctuation|12                           |\n",
      "|2024-11-29|4029.1428571428573             |484.38764955379344           |28                  |0                     |Forced Outage      |9                            |\n",
      "|2024-08-05|308.66265060240966             |313.492371351604             |83                  |5                     |Job of Operation   |23                           |\n",
      "|2023-05-03|13431.972222222223             |355.8145658797781            |72                  |0                     |Commercial         |42                           |\n",
      "|2024-05-19|2973.4710144927535             |115.28345539622063           |138                 |0                     |Commercial         |85                           |\n",
      "|2025-02-06|159.90625                      |154.16353986412287           |32                  |0                     |Shutdown           |11                           |\n",
      "+----------+-------------------------------+-----------------------------+--------------------+----------------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window  # Window specification for ranking\n",
    "\n",
    "# Extract date from Createdon column\n",
    "temp_df = df_reduced.withColumn(\"date\", F.to_date(\"Createdon\"))\n",
    "\n",
    "# First, calculate the most frequent fault in \"initialoffreason\" for each \"date\"\n",
    "most_frequent_fault_date = temp_df.groupBy(\"date\", \"initialoffreason\").agg(\n",
    "    F.count(\"initialoffreason\").alias(\"Most_Occurred_Fault_Frequency\")\n",
    ")\n",
    "\n",
    "# Now, for each date, we want the fault with the highest count\n",
    "most_frequent_fault_date = most_frequent_fault_date.withColumn(\n",
    "    \"rank\", F.row_number().over(\n",
    "        Window.partitionBy(\"date\").orderBy(F.desc(\"Most_Occurred_Fault_Frequency\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter to get only the most frequent fault for each date\n",
    "most_frequent_fault_date = most_frequent_fault_date.filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "most_frequent_fault_date = most_frequent_fault_date.withColumnRenamed(\"date\", \"Date_diff\") \n",
    "# Now, create the dimension (dim) for Date of Createdon\n",
    "oms_dim_date = temp_df.groupBy(\"date\").agg(\n",
    "    F.first(\"date\").alias(\"Date_\"),\n",
    "    F.avg(\"TAT\").alias(\"Average Turn-Around Time (mins)\"),\n",
    "    F.avg(\"duration\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.count(\"outage_Id\").alias(\"Total Outages/Faults\"),\n",
    "    F.sum(\"israintripping\").alias(\"Rain Frequency (23-24)\")\n",
    ")\n",
    "\n",
    "# Join the most frequent fault with the Date-based aggregation\n",
    "oms_dim_date = oms_dim_date.join(most_frequent_fault_date, \n",
    "                       on=[oms_dim_date.Date_ == most_frequent_fault_date.Date_diff], \n",
    "                       how=\"left\")\n",
    "\n",
    "\n",
    "oms_dim_date = oms_dim_date.select(\n",
    "    F.col(\"Date_diff\").alias(\"Date\"),\n",
    "    \"Average Turn-Around Time (mins)\", \"Average Fault Duration (mins)\", \n",
    "    \"Total Outages/Faults\", \"Rain Frequency (23-24)\", \n",
    "     F.col(\"initialoffreason\").alias(\"Most_Occurred_Fault\"), \n",
    "    \"Most_Occurred_Fault_Frequency\"\n",
    ")\n",
    "\n",
    "# Display the final result for the Date of Createdon dimension\n",
    "oms_dim_date.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "daf9b0e9-5f65-42a3-a4e1-47f7b2b29d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 685:==============================================>      (87 + 12) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+-------------------------------+-----------------------------+--------------------+----------------------+-------------------+-----------------------------+\n",
      "|FDR_ID|FDR_Name                     |Average Turn-Around Time (mins)|Average Fault Duration (mins)|Total Outages/Faults|Rain Frequency (23-24)|Most_Occurred_Fault|Most_Occurred_Fault_Frequency|\n",
      "+------+-----------------------------+-------------------------------+-----------------------------+--------------------+----------------------+-------------------+-----------------------------+\n",
      "|102   |EAGLE SQUAD                  |4564.562256809339              |507.29518566068356           |514                 |5                     |Voltage Fluctuation|134                          |\n",
      "|103   |PARAS                        |5446.527876631079              |613.5865991824132            |843                 |5                     |Voltage Fluctuation|247                          |\n",
      "|109   |EXPO CENTER                  |38733.87719298246              |48.1768520358536             |57                  |4                     |NULL               |27                           |\n",
      "|111   |RUKKNUDDIN                   |277.32561728395063             |288.43107074830266           |648                 |4                     |Commercial         |352                          |\n",
      "|1123  |FALAK TOWER (GUL ICE)        |16876.566666666666             |107.74130630115268           |360                 |16                    |Job of Operation   |65                           |\n",
      "|1152  |GENERAL SOCIETY              |849.0841121495328              |185.82365094707126           |107                 |4                     |Shutdown           |26                           |\n",
      "|117   |QASIMABAD DEGREE COLLGE (QDC)|5638.694055944056              |591.2331132842138            |1144                |6                     |Voltage Fluctuation|269                          |\n",
      "|1176  |ARMY HOUSING-5               |5101.544776119403              |176.0078212426259            |134                 |2                     |Forced Outage      |39                           |\n",
      "|1177  |ARMY HOUSING-9               |13488.39393939394              |139.47025565505027           |66                  |1                     |NULL               |16                           |\n",
      "|1196  |WONDER BREAD(LAL SHAHBAZ)    |47937.192307692305             |95.32725674193352            |104                 |0                     |Forced Outage      |33                           |\n",
      "|1198  |U. K. SQUARE                 |13663.073979591836             |208.45833363433024           |392                 |7                     |Job of Operation   |174                          |\n",
      "|1199  |GULBERG                      |18946.38683127572              |235.10541354998563           |243                 |6                     |Job of Operation   |161                          |\n",
      "|1200  |TAHIR VILLA                  |19735.021505376346             |221.42246778582532           |279                 |8                     |Job of Operation   |128                          |\n",
      "|1201  |ABID PLAZA                   |6793.6120218579235             |362.6112041268084            |183                 |3                     |Lead Burnt Out     |68                           |\n",
      "|1203  |FAROOQ-E-AZAM(ZIAUDDIN)      |19936.58358662614              |1280.9046742877922           |329                 |9                     |Job of Operation   |180                          |\n",
      "|1205  |SHAMIM APPARTMENT            |12280.960655737705             |151.5515094266942            |305                 |4                     |Job of Operation   |91                           |\n",
      "|1212  |STATION ROAD                 |2900.89156626506               |295.4600540944922            |249                 |3                     |Lead Burnt Out     |97                           |\n",
      "|1213  |NESCOS                       |2666.926056338028              |314.8246754264663            |284                 |9                     |Lead Burnt Out     |96                           |\n",
      "|1214  |CIVIC VIEW                   |304.24175824175825             |308.5551279272352            |91                  |2                     |Lead Burnt Out     |28                           |\n",
      "|1215  |ERUM DEVELOPER               |217.55238095238096             |219.02825242962157           |105                 |7                     |Lead Burnt Out     |35                           |\n",
      "+------+-----------------------------+-------------------------------+-----------------------------+--------------------+----------------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window  # Window specification for ranking\n",
    "\n",
    "# First, calculate the most frequent fault in \"initialoffreason\" for each \"fdr_Id\" and \"fdr_name\"\n",
    "most_frequent_fault_fdr = df_reduced.groupBy(\"fdr_Id\", \"fdr_name\", \"initialoffreason\").agg(\n",
    "    F.count(\"initialoffreason\").alias(\"Most_Occurred_Fault_Frequency\")\n",
    ")\n",
    "\n",
    "# Now, for each \"fdr_Id\" and \"fdr_name\", we want the fault with the highest count\n",
    "most_frequent_fault_fdr = most_frequent_fault_fdr.withColumn(\n",
    "    \"rank\", F.row_number().over(\n",
    "        Window.partitionBy(\"fdr_Id\", \"fdr_name\").orderBy(F.desc(\"Most_Occurred_Fault_Frequency\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter to get only the most frequent fault for each \"fdr_Id\" and \"fdr_name\"\n",
    "most_frequent_fault_fdr = most_frequent_fault_fdr.filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Rename `fdr_Id` and `fdr_name` to avoid ambiguity\n",
    "most_frequent_fault_fdr = most_frequent_fault_fdr.withColumnRenamed(\"fdr_Id\", \"FDR_ID_Fault\") \\\n",
    "                                                   .withColumnRenamed(\"fdr_name\", \"FDR_Name_Fault\")\n",
    "\n",
    "# Now, create the dimension (dim) for \"fdr_Id\" and \"fdr_name\"\n",
    "oms_dim_fdr = df_reduced.groupBy(\"fdr_Id\", \"fdr_name\").agg(\n",
    "    F.first(\"fdr_Id\").alias(\"FDR_ID_\"),\n",
    "    F.first(\"fdr_name\").alias(\"FDR_Name_\"),\n",
    "    F.avg(\"TAT\").alias(\"Average Turn-Around Time (mins)\"),\n",
    "    F.avg(\"duration\").alias(\"Average Fault Duration (mins)\"),\n",
    "    F.count(\"outage_Id\").alias(\"Total Outages/Faults\"),\n",
    "    F.sum(\"israintripping\").alias(\"Rain Frequency (23-24)\")\n",
    ")\n",
    "\n",
    "# Join the most frequent fault with the \"fdr_Id\" and \"fdr_name\" aggregation\n",
    "oms_dim_fdr = oms_dim_fdr.join(most_frequent_fault_fdr, \n",
    "                       on=[oms_dim_fdr.FDR_ID_ == most_frequent_fault_fdr.FDR_ID_Fault, \n",
    "                           oms_dim_fdr.FDR_Name_ == most_frequent_fault_fdr.FDR_Name_Fault], \n",
    "                       how=\"left\")\n",
    "\n",
    "# Select distinct columns to avoid ambiguity\n",
    "oms_dim_fdr = oms_dim_fdr.select(\n",
    "    \"FDR_ID\", \"FDR_Name\", \n",
    "    \"Average Turn-Around Time (mins)\", \"Average Fault Duration (mins)\", \n",
    "    \"Total Outages/Faults\", \"Rain Frequency (23-24)\", \n",
    "     F.col(\"initialoffreason\").alias(\"Most_Occurred_Fault\"), \n",
    "    \"Most_Occurred_Fault_Frequency\"\n",
    ")\n",
    "\n",
    "# Display the final result for the \"fdr_Id\" and \"fdr_name\" dimension\n",
    "oms_dim_fdr.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "2633a079-cd8b-47f6-82e5-3126b03eec32",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/mnt/HabibData/fault_data.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[381], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/HabibData/fault_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# match = re.search(r'_(\\d{6})', csv)# Extract month and year from the filename\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# billing_month = match.group(1) if match else \"Unknown\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelimiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# df = df.withColumn(\"BillingMonth\", lit(billing_month))# Add the BillingMonth column\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Repartition the DataFrame\u001b[39;00m\n\u001b[1;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m200\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/mnt/HabibData/fault_data.csv."
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# from pyspark.sql.functions import lit\n",
    "# csv = \"/mnt/HabibData/fault_data.csv\"\n",
    "# # match = re.search(r'_(\\d{6})', csv)# Extract month and year from the filename\n",
    "# # billing_month = match.group(1) if match else \"Unknown\"\n",
    "\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = spark.read \\\n",
    "#     .option(\"delimiter\", \",\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"inferSchema\", \"true\") \\\n",
    "#     .csv(csv)\n",
    "\n",
    "# # df = df.withColumn(\"BillingMonth\", lit(billing_month))# Add the BillingMonth column\n",
    "\n",
    "# # Repartition the DataFrame\n",
    "# df = df.repartition(200)\n",
    "\n",
    "# # Write to Nessie table\n",
    "# df.writeTo(\"nessie.oms.oms_data_raw\").createOrReplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2bb5879-e59a-444c-953f-83317cbf1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "badf85ed-605e-4925-a4fa-b126c8f13375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o56.sql.\n: org.apache.iceberg.exceptions.NamespaceNotEmptyException: Namespace 'ageing' is not empty.\n\tat org.apache.iceberg.nessie.NessieIcebergClient.dropNamespace(NessieIcebergClient.java:337)\n\tat org.apache.iceberg.nessie.NessieCatalog.dropNamespace(NessieCatalog.java:316)\n\tat org.apache.iceberg.spark.SparkCatalog.dropNamespace(SparkCatalog.java:533)\n\tat org.apache.spark.sql.execution.datasources.v2.DropNamespaceExec.run(DropNamespaceExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.projectnessie.error.NessieReferenceConflictException: Namespace 'ageing' is not empty.\n\tat org.projectnessie.error.ErrorCode.lambda$asException$1(ErrorCode.java:66)\n\tat java.base/java.util.Optional.map(Optional.java:265)\n\tat org.projectnessie.error.ErrorCode.asException(ErrorCode.java:66)\n\tat org.projectnessie.client.rest.ResponseCheckFilter.checkResponse(ResponseCheckFilter.java:58)\n\tat org.projectnessie.client.rest.NessieHttpResponseFilter.filter(NessieHttpResponseFilter.java:29)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.lambda$executeRequest$1(JavaRequest.java:143)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1541)\n\tat java.base/java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1085)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.executeRequest(JavaRequest.java:143)\n\tat org.projectnessie.client.http.HttpRequest.post(HttpRequest.java:116)\n\tat org.projectnessie.client.rest.v1.RestV1TreeClient.commitMultipleOperations(RestV1TreeClient.java:204)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.projectnessie.client.rest.v1.RestV1Client$ExceptionRewriter.invoke(RestV1Client.java:84)\n\tat com.sun.proxy.$Proxy36.commitMultipleOperations(Unknown Source)\n\tat org.projectnessie.client.rest.v1.HttpCommitMultipleOperations.commit(HttpCommitMultipleOperations.java:34)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.lambda$commitRetry$14(NessieIcebergClient.java:772)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.commitRetry(NessieIcebergClient.java:764)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.commitRetry(NessieIcebergClient.java:748)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.dropNamespace(NessieIcebergClient.java:322)\n\t... 46 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP table nessie.ageing.consumer_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      6\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP table nessie.ageing.ageing_fact\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDROP schema nessie.ageing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.sql.\n: org.apache.iceberg.exceptions.NamespaceNotEmptyException: Namespace 'ageing' is not empty.\n\tat org.apache.iceberg.nessie.NessieIcebergClient.dropNamespace(NessieIcebergClient.java:337)\n\tat org.apache.iceberg.nessie.NessieCatalog.dropNamespace(NessieCatalog.java:316)\n\tat org.apache.iceberg.spark.SparkCatalog.dropNamespace(SparkCatalog.java:533)\n\tat org.apache.spark.sql.execution.datasources.v2.DropNamespaceExec.run(DropNamespaceExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.projectnessie.error.NessieReferenceConflictException: Namespace 'ageing' is not empty.\n\tat org.projectnessie.error.ErrorCode.lambda$asException$1(ErrorCode.java:66)\n\tat java.base/java.util.Optional.map(Optional.java:265)\n\tat org.projectnessie.error.ErrorCode.asException(ErrorCode.java:66)\n\tat org.projectnessie.client.rest.ResponseCheckFilter.checkResponse(ResponseCheckFilter.java:58)\n\tat org.projectnessie.client.rest.NessieHttpResponseFilter.filter(NessieHttpResponseFilter.java:29)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.lambda$executeRequest$1(JavaRequest.java:143)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1541)\n\tat java.base/java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1085)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.executeRequest(JavaRequest.java:143)\n\tat org.projectnessie.client.http.HttpRequest.post(HttpRequest.java:116)\n\tat org.projectnessie.client.rest.v1.RestV1TreeClient.commitMultipleOperations(RestV1TreeClient.java:204)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.projectnessie.client.rest.v1.RestV1Client$ExceptionRewriter.invoke(RestV1Client.java:84)\n\tat com.sun.proxy.$Proxy36.commitMultipleOperations(Unknown Source)\n\tat org.projectnessie.client.rest.v1.HttpCommitMultipleOperations.commit(HttpCommitMultipleOperations.java:34)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.lambda$commitRetry$14(NessieIcebergClient.java:772)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.commitRetry(NessieIcebergClient.java:764)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.commitRetry(NessieIcebergClient.java:748)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.dropNamespace(NessieIcebergClient.java:322)\n\t... 46 more\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"DROP table nessie.ageing.ageing_data_raw\").show()\n",
    "spark.sql(\"DROP table nessie.ageing.pmt_dim\").show()\n",
    "spark.sql(\"DROP table nessie.ageing.month_dim\").show()\n",
    "spark.sql(\"DROP table nessie.ageing.ibc_dim\").show()\n",
    "spark.sql(\"DROP table nessie.ageing.consumer_dim\").show()\n",
    "spark.sql(\"DROP table nessie.ageing.ageing_fact\").show()\n",
    "spark.sql(\"DROP schema nessie.ageing\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30cec25-d431-4baa-ac53-b9f5aaba7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select('Time').limit(1).show()\n",
    "df.limit(1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd85d4-44dc-4c70-b9e6-48807fb37cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP TABLE IF EXISTS nessie.power_report;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa859c-b0b9-47c1-9bdc-85838ce2dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE NAMESPACE nessie.feedervoltage;\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493f22a-e6d4-4fd7-97a9-434fa340a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# print(\"Spark Session Started\")\n",
    "df.writeTo(\"nessie.feedervoltage.feedervoltage_data_raw\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea22eb-c448-4569-a0f7-3ab0fe50e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table(\"nessie.feedervoltage.feedervoltage_data_raw\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0930e-b382-4f8f-bcb0-6fcc72ff16ed",
   "metadata": {},
   "source": [
    "### Appending values change some datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "1970b3d4-1b46-49b0-812c-15ce4356a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: \n",
      "    ALTER TABLE nessie.starschema.crm_fact \n",
      "    ALTER COLUMN `PMT` TYPE BIGINT\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 03:01:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[NOT_SUPPORTED_CHANGE_COLUMN] ALTER TABLE ALTER/CHANGE COLUMN is not supported for changing `nessie`.`starschema`.`crm_fact`'s column `PMT` with type \"STRING\" to `PMT` with type \"BIGINT\".; line 2 pos 4;\nAlterColumn resolvedfieldname(StructField(PMT,StringType,true)), LongType\n+- ResolvedTable org.apache.iceberg.spark.SparkCatalog@438000b7, starschema.crm_fact, nessie.starschema.crm_fact, [AccountContract#16147, Ticket No.#16148L, Business_Partner#16149, GUID#16150, IBCName#16151, PROCESS_TYPE#16152, CREATED_BY#16153, CREATED_AT#16154, UDATETIME#16155, LatestStatus#16156, Supervisor#16157, Lineman#16158, MTLNo#16159, CompletedDateTime#16160, ClosedDateTime#16161, InProcessDateTime#16162, OpenDateTime#16163, Medium Of Complaint#16164, IBCCode#16165, CREATED_DATE#16166, Source#16167, Created By#16168, Region#16169, Subject_ID#16170, ... 44 more fields]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m     alter_statement \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    ALTER TABLE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    ALTER COLUMN `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` TYPE BIGINT\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malter_statement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43malter_statement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Verify the schema after updates\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated Table Schema:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [NOT_SUPPORTED_CHANGE_COLUMN] ALTER TABLE ALTER/CHANGE COLUMN is not supported for changing `nessie`.`starschema`.`crm_fact`'s column `PMT` with type \"STRING\" to `PMT` with type \"BIGINT\".; line 2 pos 4;\nAlterColumn resolvedfieldname(StructField(PMT,StringType,true)), LongType\n+- ResolvedTable org.apache.iceberg.spark.SparkCatalog@438000b7, starschema.crm_fact, nessie.starschema.crm_fact, [AccountContract#16147, Ticket No.#16148L, Business_Partner#16149, GUID#16150, IBCName#16151, PROCESS_TYPE#16152, CREATED_BY#16153, CREATED_AT#16154, UDATETIME#16155, LatestStatus#16156, Supervisor#16157, Lineman#16158, MTLNo#16159, CompletedDateTime#16160, ClosedDateTime#16161, InProcessDateTime#16162, OpenDateTime#16163, Medium Of Complaint#16164, IBCCode#16165, CREATED_DATE#16166, Source#16167, Created By#16168, Region#16169, Subject_ID#16170, ... 44 more fields]\n"
     ]
    }
   ],
   "source": [
    "table_name = \"nessie.starschema.crm_fact\"\n",
    "\n",
    "# List of columns with `INT` type to be altered to `BIGINT`\n",
    "columns_to_alter = [\n",
    "    \"PMT\"\n",
    "]\n",
    "\n",
    "# Generate and execute ALTER TABLE commands for each column\n",
    "for column in columns_to_alter:\n",
    "    alter_statement = f\"\"\"\n",
    "    ALTER TABLE {table_name} \n",
    "    ALTER COLUMN `{column}` TYPE BIGINT\n",
    "    \"\"\"\n",
    "    print(f\"Executing: {alter_statement}\")\n",
    "    spark.sql(alter_statement)\n",
    "\n",
    "# Verify the schema after updates\n",
    "print(\"Updated Table Schema:\")\n",
    "spark.sql(f\"DESCRIBE {table_name}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b628a-8e7b-4e12-a4fe-3e1f82e61a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"Billing_part1.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "dfappend = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55b713-4cec-47d3-b2ef-291618c511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfappend.writeTo(\"nessie.Billing.Billing_data_raw\").append()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f4091-a5ac-49a2-a006-7819882f0fc6",
   "metadata": {},
   "source": [
    "### Accidentally appended twice, rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a06f93b-9525-4528-a849-d984f54ddcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                                                                    |summary                                                                                                                                                                                                                                                                                             |\n",
      "+-----------------------+-------------------+-------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2025-05-05 19:12:38.543|5017145433935738403|4068863865468229985|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-5017145433935738403-1-ec2d6771-5c98-4264-a8ab-85861d8891ee.avro|{spark.app.id -> local-1746472245547, added-data-files -> 1, added-records -> 4, added-files-size -> 8113, changed-partition-count -> 1, total-records -> 72, total-files-size -> 145814, total-data-files -> 18, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-05-05 19:06:54.132|4068863865468229985|5041133216343457244|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-4068863865468229985-1-21d3529d-c807-41df-9c5c-a018808a9c30.avro|{spark.app.id -> local-1746471897292, added-data-files -> 1, added-records -> 4, added-files-size -> 8113, changed-partition-count -> 1, total-records -> 68, total-files-size -> 137701, total-data-files -> 17, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-05-05 17:57:11.05 |187068459917935144 |8851235611705713215|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-187068459917935144-1-e9dbe2ae-0f42-4ea6-87dd-f913ab4362c1.avro |{spark.app.id -> local-1746467630891, added-data-files -> 1, added-records -> 4, added-files-size -> 8111, changed-partition-count -> 1, total-records -> 76, total-files-size -> 153904, total-data-files -> 19, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-05-05 17:53:08.838|8851235611705713215|6038822954880253741|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-8851235611705713215-1-8c649e22-16a2-49db-a716-22687f03353a.avro|{spark.app.id -> local-1746467417761, added-data-files -> 1, added-records -> 4, added-files-size -> 8113, changed-partition-count -> 1, total-records -> 72, total-files-size -> 145793, total-data-files -> 18, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-05-05 17:49:36.114|6038822954880253741|5041133216343457244|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-6038822954880253741-1-11d1129b-bb30-4715-8a6e-d3c9845d53f5.avro|{spark.app.id -> local-1746467162159, added-data-files -> 1, added-records -> 4, added-files-size -> 8092, changed-partition-count -> 1, total-records -> 68, total-files-size -> 137680, total-data-files -> 17, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-05-05 17:45:21.503|5041133216343457244|2934356492057782308|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-5041133216343457244-1-2955a685-50e6-4a4c-ad08-1906085d3689.avro|{spark.app.id -> local-1746466936785, added-data-files -> 1, added-records -> 4, added-files-size -> 8111, changed-partition-count -> 1, total-records -> 64, total-files-size -> 129588, total-data-files -> 16, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-05-05 17:24:09.01 |2934356492057782308|1936593057346956588|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-2934356492057782308-1-199deaab-59d5-4988-92cc-ca3254d65372.avro|{spark.app.id -> local-1746465699115, added-data-files -> 1, added-records -> 4, added-files-size -> 8134, changed-partition-count -> 1, total-records -> 60, total-files-size -> 121477, total-data-files -> 15, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-05-05 16:59:59.794|1936593057346956588|159497974480382378 |append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-1936593057346956588-1-f67f997d-6a9d-47cc-9d9f-28bdcba9722a.avro|{spark.app.id -> local-1746464240001, added-data-files -> 1, added-records -> 4, added-files-size -> 8103, changed-partition-count -> 1, total-records -> 56, total-files-size -> 113343, total-data-files -> 14, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-05-05 16:43:49.632|159497974480382378 |4264652847465242147|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-159497974480382378-1-125710b2-d644-48bf-a674-e9981415d9bb.avro |{spark.app.id -> local-1746463284653, added-data-files -> 1, added-records -> 4, added-files-size -> 8105, changed-partition-count -> 1, total-records -> 52, total-files-size -> 105240, total-data-files -> 13, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-05-05 16:36:33.003|4264652847465242147|711195174327855856 |append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-4264652847465242147-1-8cd95020-1b72-4a7a-8f3f-ead5389cea18.avro|{spark.app.id -> local-1746462862809, added-data-files -> 1, added-records -> 4, added-files-size -> 8122, changed-partition-count -> 1, total-records -> 48, total-files-size -> 97135, total-data-files -> 12, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2025-05-05 15:30:46.755|711195174327855856 |6874294259966954732|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-711195174327855856-1-e2e962c2-c785-4dfd-b3a2-671022f92350.avro |{spark.app.id -> local-1746458737049, added-data-files -> 1, added-records -> 4, added-files-size -> 8122, changed-partition-count -> 1, total-records -> 44, total-files-size -> 89013, total-data-files -> 11, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2025-05-05 04:32:21.079|6874294259966954732|1480533619151178561|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-6874294259966954732-1-69ab7142-293e-426f-81fd-0ab256d12778.avro|{spark.app.id -> local-1746419393072, added-data-files -> 1, added-records -> 4, added-files-size -> 8109, changed-partition-count -> 1, total-records -> 40, total-files-size -> 80891, total-data-files -> 10, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2025-05-05 04:27:34.661|1480533619151178561|5208398437095436650|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-1480533619151178561-1-3386c9d2-f926-44a6-8c2d-51c9fa366f2f.avro|{spark.app.id -> local-1746419026853, added-data-files -> 1, added-records -> 4, added-files-size -> 8069, changed-partition-count -> 1, total-records -> 36, total-files-size -> 72782, total-data-files -> 9, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "|2025-05-05 03:50:09.142|5208398437095436650|8523189718551279482|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-5208398437095436650-1-a737361a-b42f-4426-ae54-6a2b91c111bd.avro|{spark.app.id -> local-1746416762846, added-data-files -> 1, added-records -> 4, added-files-size -> 8111, changed-partition-count -> 1, total-records -> 32, total-files-size -> 64713, total-data-files -> 8, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "|2025-05-05 01:36:20.024|8750671667353740063|3181225141592882527|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-8750671667353740063-1-56fe2110-6a57-486e-b65b-9e6928e76c1a.avro|{spark.app.id -> local-1746408774542, added-data-files -> 1, added-records -> 4, added-files-size -> 8122, changed-partition-count -> 1, total-records -> 36, total-files-size -> 72846, total-data-files -> 9, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "|2025-05-05 01:29:27.285|3181225141592882527|5465752949077884188|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-3181225141592882527-1-f5e80bd6-1f7e-4e01-9c4f-68bfb9b8c4d0.avro|{spark.app.id -> local-1746408439245, added-data-files -> 1, added-records -> 4, added-files-size -> 8122, changed-partition-count -> 1, total-records -> 32, total-files-size -> 64724, total-data-files -> 8, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "|2025-05-05 00:01:51.725|5465752949077884188|8523189718551279482|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-5465752949077884188-1-ef71f078-962c-4c7d-a84f-598de2edecdb.avro|{spark.app.id -> local-1746403169182, changed-partition-count -> 0, total-records -> 28, total-files-size -> 56602, total-data-files -> 7, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                       |\n",
      "|2025-05-04 20:55:25.566|8523189718551279482|7229895021040156535|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-8523189718551279482-1-e2a841c8-9b11-4d79-816a-2c5b3adaabeb.avro|{spark.app.id -> local-1746391932578, added-data-files -> 1, added-records -> 4, added-files-size -> 8100, changed-partition-count -> 1, total-records -> 28, total-files-size -> 56602, total-data-files -> 7, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "|2025-05-04 20:12:29.257|7229895021040156535|3232036475364806518|append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-7229895021040156535-1-ca788856-e537-4210-a0b1-799511eae6da.avro|{spark.app.id -> local-1746389340284, added-data-files -> 1, added-records -> 4, added-files-size -> 8092, changed-partition-count -> 1, total-records -> 24, total-files-size -> 48502, total-data-files -> 6, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "|2025-05-04 20:04:02.765|3232036475364806518|409383330236466862 |append   |s3://warehouse/starschema/ibc_oms_ageing_dim_4_a862a398-fd2d-4fe7-899c-55b6a702a65f/metadata/snap-3232036475364806518-1-d6f31d24-b319-4608-ba42-840176584f1c.avro|{spark.app.id -> local-1746388895500, added-data-files -> 1, added-records -> 4, added-files-size -> 8062, changed-partition-count -> 1, total-records -> 20, total-files-size -> 40410, total-data-files -> 5, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "+-----------------------+-------------------+-------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM nessie.starschema.ibc_oms_ageing_dim_4.snapshots order by `committed_at` desc limit 100\n",
    "\"\"\").limit(100).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a39e2000-11ed-4144-9fae-e32170607ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[previous_snapshot_id: bigint, current_snapshot_id: bigint]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_snapshot_id = \"4068863865468229985\"  # Replace with the ID of the snapshot before the append\n",
    "spark.sql(f\"\"\"\n",
    "    CALL nessie.system.rollback_to_snapshot(\n",
    "        'nessie.starschema.ibc_oms_ageing_dim_4', \n",
    "        {previous_snapshot_id}\n",
    "    )\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1afb1fba-7363-4df1-ade1-e2e5de37dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls \"/mnt/inc_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c26a03-1f2d-4d9c-9674-d91b96f43913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "66871a4e-8d37-4b3e-8d8a-32dfd1344be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2c6b2-fca9-4024-ab9e-e6e1fe9a5121",
   "metadata": {},
   "source": [
    "## Getting views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a90995-d715-410d-8693-79287d38233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ageing = spark.read.table(\"nessie.ageing.ageing_data_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49219165-c7ac-42c8-b2d1-9b61dbac3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ageing.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba8bdb-f842-4bcd-8f8f-d2e6a699e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"nessie.ageing.ageing_data_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5ec76-b492-4163-862b-1510cb0b43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = ageing.filter(\"status LIKE 'ACT'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227787f7-7dd7-4ff5-8f8a-2f25c624253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitioned = df_filtered.repartition(400)  # Adjust number of partitions based on your data size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b881b6-42cb-4d6e-86e3-61da08a5a97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repartition the filtered DataFrame for performance optimization\n",
    "df_repartitioned.cache()\n",
    "df_repartitioned.limit(1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01928ca2-2ba6-4573-abd0-3e45d2fa5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Repartition the data to optimize subsequent operations\n",
    "df = spark.sql(\"SELECT * FROM nessie.ageing.ageing_data_raw\")\n",
    "df_repartitioned = df.repartition(200)  # Adjust the number of partitions based on your data size\n",
    "\n",
    "df_repartitioned.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae7d79-d8bf-4987-9bae-116df8b17228",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE NAMESPACE nessie.ageing_active\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48238aca-bf5e-41d1-a2f1-fcd5b5139f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    DELETE FROM nessie.ageing.ageing_data_raw\n",
    "    WHERE status NOT LIKE 'ACT'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760114d-8c34-4be8-b84d-931870d7fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active = df_active.repartition(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a1f37-ace6-4c1c-80f6-6ec9adcf13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active.writeTo(\"nessie.ageing_active.ageing_active_data_raw\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b7227-fbb2-4d78-a388-44ab8cfff880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active.write.repartition(100) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"nessie.ageing.ageing_active\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c086fa7-89bf-476c-b44d-7e62993a2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Load the data as a DataFrame\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    WEEKOFYEAR(`Last Payment Date`) AS week_number,\n",
    "    YEAR(`Last Payment Date`) AS `year`,\n",
    "    SUM(`RegularUnits`) AS total_regular_units,\n",
    "    SUM(`Last Payment Amount`) AS total_last_payment\n",
    "FROM nessie.ageing_active__.ageing_active\n",
    "GROUP BY YEAR(`Last Payment Date`), WEEKOFYEAR(`Last Payment Date`)\n",
    "ORDER BY YEAR(`Last Payment Date`), WEEKOFYEAR(`Last Payment Date`);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a3ee0-6599-4088-a512-684bca8007f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, regexp_replace\n",
    "\n",
    "df_updated = df.withColumn(\n",
    "    \"Last Payment Date\", \n",
    "    to_timestamp(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    regexp_replace(\n",
    "                        regexp_replace(\n",
    "                            regexp_replace(\n",
    "                                regexp_replace(\n",
    "                                    regexp_replace(\n",
    "                                        regexp_replace(\n",
    "                                            regexp_replace(\n",
    "                                                regexp_replace(\n",
    "                                                    regexp_replace(\n",
    "                                                        df['Last Payment Date'], '-Jan-', '-01-'\n",
    "                                                    ), '-Feb-', '-02-'\n",
    "                                                ), '-Mar-', '-03-'\n",
    "                                            ), '-Apr-', '-04-'\n",
    "                                        ), '-May-', '-05-'\n",
    "                                    ), '-Jun-', '-06-'\n",
    "                                ), '-Jul-', '-07-'\n",
    "                            ), '-Aug-', '-08-'\n",
    "                        ), '-Sep-', '-09-'\n",
    "                    ), '-Oct-', '-10-'\n",
    "                ), '-Nov-', '-11-'\n",
    "            ), '-Dec-', '-12-'\n",
    "        ), 'dd-MM-yy'  # Assuming this is the desired format for TO_TIMESTAMP\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2ef1e-9f05-4bff-95a7-2014919bcafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE NAMESPACE nessie.ageing_active__\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df8988-17ef-4e1e-ac5d-f47bc73a7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "df_updated.writeTo(\"nessie.ageing_active__.ageing_active\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ed352-0861-45f1-9f38-12f4828a348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_view.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b179a-371a-4e65-a052-ab769fa39fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition into more than one partition (e.g., 100 partitions)\n",
    "recovery_view.repartition(100).write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"recovery.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b156b8-3616-490a-b619-56a93cc43934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of coalesce(1), try coalescing to a larger number of partitions (e.g., 10 or 20)\n",
    "df_spark.coalesce(20).write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"recovrytry.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39e431-f65f-4c4e-a725-9107f22d676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a CSV file\n",
    "recovery_view.write.option(\"header\", \"true\").csv(\"recovery_view.csv\")\n",
    " cbv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b4266-ec52-4206-99b8-211d4d02a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a single CSV file\n",
    "recovery_view.coalesce(1).write.csv(\"recovery_view1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e982f05-a066-4fe0-a9a1-200af54d7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a single CSV file\n",
    "df.coalesce(1).write.option(\"header\", \"true\").csv(\"weekly_ageing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1cd5af-abc3-4efc-9495-f4354a2edc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"recovery_view2.csv/part-00000-fcb2ca61-ccac-4761-bac3-2da2e51a2074-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b01cba-19c2-464b-9440-42776416ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls \"recovery_view2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53251d-6f7c-4fac-927c-c6faa351a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae65ab2-3dbe-400d-bee9-89a24437c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333047f3-0e17-4b69-9604-28531dbba278",
   "metadata": {},
   "source": [
    "#### Deleting old tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ac9850bd-4063-4eb1-a214-47abc5061d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+\n",
      "|           namespace|           tableName|isTemporary|\n",
      "+--------------------+--------------------+-----------+\n",
      "|EnergyConsumption...|EnergyConsumption...|      false|\n",
      "|              ageing|     ageing_data_raw|      false|\n",
      "|        ageing_check|ageing_check_data...|      false|\n",
      "|      ageing_cleaned|ageing_cleaned_da...|      false|\n",
      "|             billing|    billing_data_raw|      false|\n",
      "|                 crm|        crm_data_raw|      false|\n",
      "|                crm_|       crm__data_raw|      false|\n",
      "|           crmschema|            crm_fact|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_billing_and_...|fact_billing_and_...|      false|\n",
      "|fact_network_and_...|fact_network_and_...|      false|\n",
      "|fact_network_and_...|fact_network_and_...|      false|\n",
      "|       fault_tickets|fault_tickets_dat...|      false|\n",
      "|        feedermaster|feedermaster_data...|      false|\n",
      "|       feedervoltage|feedervoltage_dat...|      false|\n",
      "|                 oms|        oms_data_raw|      false|\n",
      "+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemas = spark.sql(\"SHOW TABLES IN nessie\").limit(50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "958cf557-bff9-4f84-ad92-3d75fd6756e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'PURGE': extra input 'PURGE'.(line 1, pos 42)\n\n== SQL ==\nDROP schema if exists nessie.ageing_check PURGE\n------------------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[257], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDROP schema if exists nessie.ageing_check PURGE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'PURGE': extra input 'PURGE'.(line 1, pos 42)\n\n== SQL ==\nDROP schema if exists nessie.ageing_check PURGE\n------------------------------------------^^^\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP schema if exists nessie.ageing_check PURGE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c0964ab6-714c-456f-8f84-e6e9413f2a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping table: nessie.ageing_check.ageing_check_data_raw\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3595.sql.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3://warehouse/ageing_check/ageing_check_data_raw_4e9c5b20-7110-4955-a337-92fea4de4988/metadata/00000-dafa3847-38a0-4168-ad85-7f162829e834.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:194)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:280)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:273)\n\tat org.apache.iceberg.nessie.NessieTableOperations.lambda$doRefresh$1(NessieTableOperations.java:105)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doRefresh(NessieTableOperations.java:99)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:843)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:170)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:164)\n\tat org.apache.spark.sql.execution.datasources.v2.DropTableExec.run(DropTableExec.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat jdk.internal.reflect.GeneratedMethodAccessor393.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 183C818FDFF16110, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:125)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:82)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:60)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)\n\t... 74 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m         fq_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnessie.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropping table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfq_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m         \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDROP TABLE IF EXISTS \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfq_table\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Drop the empty schemas\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m schema \u001b[38;5;129;01min\u001b[39;00m schemas_to_drop:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3595.sql.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3://warehouse/ageing_check/ageing_check_data_raw_4e9c5b20-7110-4955-a337-92fea4de4988/metadata/00000-dafa3847-38a0-4168-ad85-7f162829e834.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:194)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:280)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:273)\n\tat org.apache.iceberg.nessie.NessieTableOperations.lambda$doRefresh$1(NessieTableOperations.java:105)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doRefresh(NessieTableOperations.java:99)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:843)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:170)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:164)\n\tat org.apache.spark.sql.execution.datasources.v2.DropTableExec.run(DropTableExec.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat jdk.internal.reflect.GeneratedMethodAccessor393.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 183C818FDFF16110, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:125)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:82)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:60)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)\n\t... 74 more\n"
     ]
    }
   ],
   "source": [
    "# Get all tables in all namespaces (schemas)\n",
    "df = spark.sql(\"SHOW TABLES IN nessie\")\n",
    "df = df.filter(\"isTemporary = false\")\n",
    "\n",
    "# Get unique namespaces except 'starschema'\n",
    "namespaces = df.select(\"namespace\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "schemas_to_drop = [ns for ns in namespaces if ns != \"starschema\"]\n",
    "\n",
    "# Drop all tables from schemas other than 'starschema'\n",
    "for schema in schemas_to_drop:\n",
    "    tables = df.filter(f\"namespace = '{schema}'\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "    for table in tables:\n",
    "        fq_table = f\"nessie.{schema}.{table}\"\n",
    "        print(f\"Dropping table: {fq_table}\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {fq_table}\")\n",
    "\n",
    "# Drop the empty schemas\n",
    "for schema in schemas_to_drop:\n",
    "    tables = df.filter(f\"namespace = '{schema}'\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "    for table in tables:\n",
    "        fq_table = f\"nessie.{schema}.{table}\"\n",
    "        print(f\"Attempting to drop: {fq_table}\")\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {fq_table}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to drop {fq_table}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f33e9b1a-274f-4ee6-8781-51946ea983c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP SCHEMA IF EXISTS ageing CASCADE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "53b0b082-ea8e-4b49-a30b-f548dc1d4258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP table nessie.ageing_cleaned.ageing_active\").show()\n",
    "# spark.sql(\"DROP schema nessie.ageing\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "50ee813a-40cb-4cb8-9fb5-991d695effb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a5c86a7-fcb4-44eb-8346-c40a7043cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped schema: default\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get all schemas (databases)\n",
    "all_schemas = [db.name for db in spark.catalog.listDatabases()]\n",
    "\n",
    "# Step 2: Filter out the one you want to keep\n",
    "schemas_to_drop = [db for db in all_schemas if db.lower() != \"starschema\"]\n",
    "\n",
    "# Step 3: Drop each schema\n",
    "for schema in all_schemas:\n",
    "    # spark.sql(f\"DROP DATABASE IF EXISTS {schema} CASCADE\")\n",
    "    print(f\"Dropped schema: {schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "37642c5a-a9e0-4b95-a2fd-bd80eace13de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default']\n"
     ]
    }
   ],
   "source": [
    "print(all_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "6ac49f58-77de-4686-9db9-cfd27b2941c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6197b-e9fe-43ae-9109-797132104e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
