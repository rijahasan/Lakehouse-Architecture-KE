{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9147bea-bcb9-4244-97f9-514e63185e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#172.18.0.4\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06a0ee2-b840-4052-a045-903190dcc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URI = \"http://nessie:19120/api/v1\"  # Nessie Server URI\n",
    "WAREHOUSE = \"s3://warehouse/\"               # Minio Address to Write to\n",
    "STORAGE_URI = \"http://172.18.0.5:9000\"     # Minio IP address from docker inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc715ed-c2b6-4ba2-acd8-a25d2442e221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eaf47da-74a7-4f76-a4c4-3c99dd6acc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('billing')\n",
    "    \n",
    "    # .set(\"spark.executor.instances\", \"3\")  # 3 Executors\n",
    "    # .set(\"spark.executor.cores\", \"3\")  # 3 Cores per executor\n",
    "    # .set(\"spark.executor.memory\", \"3g\")  # 3GB RAM per executor\n",
    "    # .set(\"spark.driver.memory\", \"4g\")  # Ensure driver has enough memory\n",
    "    # .set(\"spark.sql.shuffle.partitions\", \"200\")  # Tune for large joins\n",
    "    # .set(\"spark.default.parallelism\", \"9\")  # Optimize parallel processing\n",
    "    # .set(\"spark.executor.instances\", \"3\")  # 3 Executors\n",
    "    # .set(\"spark.executor.cores\", \"2\")  # 3 Cores per executor\n",
    "    # .set(\"spark.executor.memory\", \"6g\")  # 3GB RAM per executor\n",
    "    # .set(\"spark.driver.memory\", \"6g\")  # Ensure driver has enough memory\n",
    "    # .set(\"spark.sql.shuffle.partitions\", \"200\")  # Tune for large joins\n",
    "    # .set(\"spark.default.parallelism\", \"200\")  # Optimize parallel processing\n",
    "    # .set(\"spark.memory.fraction\", \"0.7\")  #\n",
    "    # .set(\"spark.executor.memoryOverhead\", \"1024\") \n",
    "#     .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "# .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "        .set(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "        .set('spark.jars', '/opt/spark/workjars/iceberg-spark-runtime-3.5_2.12-1.5.0.jar,/opt/spark/workjars/nessie-spark-extensions-3.5_2.12-0.77.1.jar,/opt/spark/workjars/bundle-2.24.8.jar,/opt/spark/workjars/url-connection-client-2.24.8.jar')\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', CATALOG_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', STORAGE_URI)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "\n",
    "            .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.driver.memory\", \"2g\")\n",
    "        .set(\"spark.executor.memoryOverhead\", \"512m\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"64\")\n",
    "        .set(\"spark.shuffle.spill\", \"true\")\n",
    "        .set(\"spark.shuffle.memoryFraction\", \"0.4\")\n",
    "#         .set(\"spark.executor.instances\", \"2\")  # Use 2 executors\n",
    "#         .set(\"spark.executor.cores\", \"2\")  # 2 cores per executor\n",
    "#         .set(\"spark.executor.memory\", \"5g\")  # 5GB per executor (10GB total)\n",
    "#         .set(\"spark.driver.memory\", \"2g\")  # 2GB for the driver\n",
    "#         .set(\"spark.memory.fraction\", \"0.6\")  # Lower memory fraction to leave room for overhead\n",
    "#         .set(\"spark.executor.memoryOverhead\", \"1g\")  # 1GB overhead per executor\n",
    "#         .set(\"spark.sql.shuffle.partitions\", \"100\")  # Adjust based on data size\n",
    "#         .set(\"spark.default.parallelism\", \"100\")  # Adjust parallelism\n",
    "#         .set(\"spark.dynamicAllocation.enabled\", \"true\")  # Enable dynamic allocation\n",
    "#         .set(\"spark.shuffle.service.enabled\", \"true\")  # Required for dynamic allocation\n",
    "#         .set(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "#         .set('spark.jars', '/opt/spark/workjars/iceberg-spark-runtime-3.5_2.12-1.5.0.jar,/opt/spark/workjars/nessie-spark-extensions-3.5_2.12-0.77.1.jar,/opt/spark/workjars/bundle-2.24.8.jar,/opt/spark/workjars/url-connection-client-2.24.8.jar')\n",
    "#         .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "#         .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "#         .set('spark.sql.catalog.nessie.uri', CATALOG_URI)\n",
    "#         .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "#         .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "#         .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "#         .set('spark.sql.catalog.nessie.s3.endpoint', STORAGE_URI)\n",
    "#         .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "#         .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "#     .set(\"spark.shuffle.service.enabled\", \"true\")  # Enable external shuffle service\n",
    "# .set(\"spark.shuffle.service.port\", \"7337\")  # Default shuffle service port\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1da920-e676-4759-b5f3-05e633e89f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/27 17:16:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").config(conf=conf).getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a06d68-b528-405e-8060-aaf5a12113f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Iceberg packages found\n",
      "org.apache.iceberg.spark.SparkCatalog\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.jars.packages\", \"No Iceberg packages found\"))\n",
    "print(spark.conf.get(\"spark.sql.catalog.nessie\", \"No Nessie catalog found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53f3ccc-d246-4aac-b2c5-85ae5fa22df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a947119-4c34-4fd3-b7fc-ebc246206305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/22 12:58:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.master(\"spark://spark:7077\").getOrCreate()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestExecutors\") \\\n",
    "    .master(\"spark://spark:7077\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d71a1521-a923-414d-a8e9-ab2d927c0d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Info:\n",
      "  - Executors: 1\n",
      "  - Cores per executor: 12\n",
      "  - Executor Memory: Not Set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_executors = spark._jsc.sc().getExecutorMemoryStatus().size() - 1  # Exclude driver\n",
    "\n",
    "# Get number of cores per executor\n",
    "num_cores = spark.sparkContext.defaultParallelism\n",
    "\n",
    "# Get memory per executor\n",
    "executor_memory = spark.conf.get(\"spark.executor.memory\", \"Not Set\")\n",
    "\n",
    "print(f\"Cluster Info:\")\n",
    "print(f\"  - Executors: {num_executors}\")\n",
    "print(f\"  - Cores per executor: {num_cores}\")\n",
    "print(f\"  - Executor Memory: {executor_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c70209ad-d3bb-4e8a-bf36-639c813ca81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1000).rdd.getNumPartitions()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5019e826-df73-4450-83c9-6a3399fb6512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(034b7b88bc5d:37659 -> (455501414,455501414))\n"
     ]
    }
   ],
   "source": [
    "print(spark._jsc.sc().getExecutorMemoryStatus())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478688e-efc7-40fc-b0c3-206203ce9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,software.amazon.awssdk:bundle:2.24.8,software.amazon.awssdk:url-connection-client:2.24.8\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cee0990-2792-4509-b889-7427376c61d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/mnt/HabibData/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls \"/mnt/HabibData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b527e85-3bc9-4cd6-b88a-cb31d9638c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = \"/mnt/HabibData/Ageing_202301.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bba51ad-dd14-4ca4-baef-89b5d63c9c01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/mnt/HabibData/Ageing_202301.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelimiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# df.show()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# df.printSchema()\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/mnt/HabibData/Ageing_202301.csv."
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv)\n",
    "\n",
    "# df.show()\n",
    "# df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9fd6f63-ea58-41d1-9970-552f2822af2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Createdon,punchCloseAt,OutageType,saifi,saidi,consumer_count_fdr,outageSubType,initialoffreason,htclosingreason,fdr_Id,fdr_name,grid_name,Relay,isemergency,israintripping,TAT,duration,dts_Id,consumer_count_fdr,datetime,tempmax,tempmin,temp,dew,humidity,precip,precipprob,precipcover,preciptype,snow,snowdepth,windgust,windspeed,winddir,sealevelpressure,cloudcover,visibility,solarradiation,solarenergy,uvindex,severerisk|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2023-01-01,2023-01-01 01:45:52,DTS,,,,DTS Off,Voltage Fluctuation,,3146,GALLANT SUMMIT,,,,NO,96,01:36:06,529533,,2023-01-01,28.0,11.0,19.0,8.6,57.3,0.0,0,0.0,,0,0,25.9,20.5,238.4,1019.0,0.0,4.2,185.0,16.1,7,10                                                                                                                                                                                                                   |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.select('Time').limit(1).show()\n",
    "df.limit(1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c655072-84dd-40d9-b687-b6321da5eb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE nessie.test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b792616d-40c3-43b7-92ee-e3a650812595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"DROP NAMESPACE nessie.test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d118026-0ee4-4970-8a14-918749802bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# print(\"Spark Session Started\")\n",
    "df.writeTo(\"nessie.test.test_data_raw\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "564ad0e7-4cba-4eee-9918-bef63c4ef881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Createdon,punchCloseAt,OutageType,saifi,saidi,consumer_count_fdr,outageSubType,initialoffreason,htclosingreason,fdr_Id,fdr_name,grid_name,Relay,isemergency,israintripping,TAT,duration,dts_Id,consumer_count_fdr,datetime,tempmax,tempmin,temp,dew,humidity,precip,precipprob,precipcover,preciptype,snow,snowdepth,windgust,windspeed,winddir,sealevelpressure,cloudcover,visibility,solarradiation,solarenergy,uvindex,severerisk|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                2023-01-01,2023-0...|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"nessie.test.test_data_raw\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa87fdb8-9770-45dc-b3d4-7b12425c0705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+------------+----------+---------------+------------------+--------------------+------------+--------------------+-----------------+--------------+----------------+\n",
      "|Fact_ID|Billing_Document_ID|Contract_Account_ID|  Due_Date|Total_Amount|Tax_Amount|Recovery_Amount|Outstanding_Amount|Average_Payment_Time|Units_Billed|Benefits_Adjustments|Import_Tax_Amount|Energy_Charges|Surcharge_Amount|\n",
      "+-------+-------------------+-------------------+----------+------------+----------+---------------+------------------+--------------------+------------+--------------------+-----------------+--------------+----------------+\n",
      "|      0|       100056932870|       400000000009|22.01.2024|   163068.55|      NULL|   2.72257278E8|   -2.7209420945E8|                NULL|      2538.0|             4697.81|              0.0|      91680.96|          2538.0|\n",
      "|      1|       100056953384|       400000000009|23.01.2024|   161306.28|      NULL|   2.72257278E8|   -2.7209597172E8|                NULL|      2506.0|             4846.12|              0.0|      90698.05|          2506.0|\n",
      "|      2|       100056934751|       400000000009|22.01.2024|   171915.17|      NULL|   2.72257278E8|   -2.7208536283E8|                NULL|      2624.0|             5839.47|              0.0|      94813.19|          2624.0|\n",
      "|      3|       100056959892|       400000000009|23.01.2024|   150890.86|      NULL|   2.72257278E8|   -2.7210638714E8|                NULL|      2332.0|             4632.08|              0.0|      85347.88|          2332.0|\n",
      "|      4|       100056935754|       400000000009|22.01.2024|   144523.23|      NULL|   2.72257278E8|   -2.7211275477E8|                NULL|      2250.0|             4149.45|              0.0|      82118.37|          2250.0|\n",
      "|      5|       100056986982|       400000000009|23.01.2024|   182576.34|      NULL|   2.72257278E8|   -2.7207470166E8|                NULL|      2778.0|              6183.6|              0.0|     101449.74|          2778.0|\n",
      "|      6|       100056935786|       400000000009|22.01.2024|   271835.11|      NULL|   2.72257278E8|   -2.7198544289E8|                NULL|      4161.0|             8100.92|              0.0|     150648.93|          4161.0|\n",
      "|      7|       100057103115|       400000000009|23.01.2024|    280078.9|      NULL|   2.72257278E8|    -2.719771991E8|                NULL|      4325.0|   7852.249999999999|              0.0|     157352.72|          4325.0|\n",
      "|      8|       100056937458|       400000000009|22.01.2024|   154683.79|      NULL|   2.72257278E8|   -2.7210259421E8|                NULL|      2397.0|             4551.93|              0.0|      87456.54|          2397.0|\n",
      "|      9|       100057112164|       400000000009|23.01.2024|   147032.72|      NULL|   2.72257278E8|   -2.7211024528E8|                NULL|      2206.0|             4223.54|              0.0|      80239.27|          2206.0|\n",
      "+-------+-------------------+-------------------+----------+------------+----------+---------------+------------------+--------------------+------------+--------------------+-----------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from nessie.fact_billing_and_recovery_final1.fact_billing_and_recovery_data_raw LIMIT 10;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28fa859c-b0b9-47c1-9bdc-85838ce2dd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "# Load data into DataFrames\n",
    "billing_data = spark.read.table(\"nessie.billing.billing_data_raw\")\n",
    "fault_tickets = spark.read.table(\"nessie.fault_tickets.fault_tickets_data_raw\")\n",
    "energy_consumption = spark.read.table(\"nessie.EnergyConsumptionFeederwise.EnergyConsumptionFeederwise_data_raw\")\n",
    "feeder_master = spark.read.table(\"nessie.feedermaster.feedermaster_data_raw\")\n",
    "feeder_voltage = spark.read.table(\"nessie.feedervoltage.feedervoltage_data_raw\")\n",
    "pmt_master = spark.read.table(\"nessie.pmtmaster.pmtmaster_data_raw\")\n",
    "power_report = spark.read.table(\"nessie.power_report.power_report_data_raw\")\n",
    "recovery_data = spark.read.table(\"nessie.recovery.recovery_data_raw\")\n",
    "\n",
    "# Register temporary views\n",
    "energy_consumption.createOrReplaceTempView(\"energy_consumption\")\n",
    "billing_data.createOrReplaceTempView(\"billing_data\")\n",
    "fault_tickets.createOrReplaceTempView(\"fault_tickets\")\n",
    "feeder_master.createOrReplaceTempView(\"feeder_master\")\n",
    "feeder_voltage.createOrReplaceTempView(\"feeder_voltage\")\n",
    "pmt_master.createOrReplaceTempView(\"pmt_master\")\n",
    "power_report.createOrReplaceTempView(\"power_report\")\n",
    "recovery_data.createOrReplaceTempView(\"recovery_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d9003-60ab-4527-a8bd-fb8b1f534e73",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26df817c-931c-4d8f-84cb-ad0e8d1d0515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------------------+\n",
      "|Overall Total|LPS Billed|Recovery_Percentage|\n",
      "+-------------+----------+-------------------+\n",
      "|     200000.0|    788.46|        2.0078846E7|\n",
      "|     200000.0|    443.67|        2.0044367E7|\n",
      "|     200000.0|   -260.87|        1.9973913E7|\n",
      "|     200000.0|   -426.19|        1.9957381E7|\n",
      "|     200000.0|   -124.58|        1.9987542E7|\n",
      "|     200000.0|       0.0|              2.0E7|\n",
      "|     200000.0|       0.0|              2.0E7|\n",
      "|     200000.0|    763.56|        2.0076356E7|\n",
      "|     200000.0|     259.2|         2.002592E7|\n",
      "|     200000.0|     260.0|           2.0026E7|\n",
      "+-------------+----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_heavy_join = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    r.`Overall Total`,\n",
    "    b.`LPS Billed`, \n",
    "    (r.`Overall Total` + b.`LPS Billed`) * 100 AS Recovery_Percentage\n",
    "FROM recovery_data r\n",
    "INNER JOIN billing_data b \n",
    "ON r.`Posting date in the document` = b.`Posting date in the document`;\n",
    "\n",
    "\"\"\")\n",
    "df_heavy_join.show(10)  # Action: Triggers execution\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# numPartitions = 500\n",
    "# recovery_data = recovery_data.repartition(numPartitions)\n",
    "# billing_data = billing_data.repartition(numPartitions)\n",
    "\n",
    "\n",
    "# # Repartition both tables on \"IBC\" for better join performance\n",
    "# recovery_data = recovery_data.repartition(\"IBC\")\n",
    "# billing_data = billing_data.repartition(\"IBC\")\n",
    "\n",
    "# # Select only required columns to reduce memory usage\n",
    "# recovery_data = recovery_data.select(\"IBC\", \"Posting date in the document\", \"Overall Total\")\n",
    "# billing_data = billing_data.select(\"IBC\", \"Posting date in the document\", \"LPS Billed\")\n",
    "\n",
    "# # Perform the FULL OUTER JOIN\n",
    "# result = recovery_data.alias(\"r\").join(\n",
    "#     billing_data.alias(\"b\"),\n",
    "#     on=[\"IBC\", \"Posting date in the document\"],\n",
    "#     how=\"full_outer\"\n",
    "# ).select(\n",
    "#     col(\"Overall Total\"),\n",
    "#     col(\"LPS Billed\"),\n",
    "#     (col(\"Overall Total\") + col(\"LPS Billed\") * 100).alias(\"Recovery_Percentage\")\n",
    "# )\n",
    "\n",
    "# # Cache the result for repeated use\n",
    "# result.cache()\n",
    "\n",
    "# # Show the first 10 rows\n",
    "# result.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8b44f-8229-4de0-9074-5101550442f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES IN nessie\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae631aa-5ad8-4eae-8b54-6ff1a9a0d5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------+\n",
      "|col_name             |data_type|comment|\n",
      "+---------------------+---------+-------+\n",
      "|Time                 |timestamp|NULL   |\n",
      "|Feeder               |int      |NULL   |\n",
      "|Device ID            |string   |NULL   |\n",
      "|DTS ID               |string   |NULL   |\n",
      "|Active Power (kW)    |string   |NULL   |\n",
      "|Reactive Power (kVAR)|string   |NULL   |\n",
      "|Apparent Power (kVA) |string   |NULL   |\n",
      "+---------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE power_report\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b75ff40-c132-49f1-83a4-e1509b523863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|col_name         |data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|Ticket No        |bigint   |NULL   |\n",
      "|Ticket Created at|string   |NULL   |\n",
      "|TicketDTSID      |string   |NULL   |\n",
      "|Feeder ID        |string   |NULL   |\n",
      "|Noti. No         |string   |NULL   |\n",
      "|UserStatus       |string   |NULL   |\n",
      "|SystemStatus     |string   |NULL   |\n",
      "|Notification Time|string   |NULL   |\n",
      "|Emergency Type   |string   |NULL   |\n",
      "|CompletedAt      |string   |NULL   |\n",
      "|Subject          |string   |NULL   |\n",
      "|AreaSecuredTime  |string   |NULL   |\n",
      "|TrueCallerTime   |string   |NULL   |\n",
      "|Item Description |string   |NULL   |\n",
      "|IsAreaSecured    |string   |NULL   |\n",
      "|AreaSecureTAT    |string   |NULL   |\n",
      "|IsTrueCaller     |string   |NULL   |\n",
      "|TrueCallerTAT    |string   |NULL   |\n",
      "|CreatedBy        |string   |NULL   |\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE fault_tickets\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a493f22a-e6d4-4fd7-97a9-434fa340a17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW fact_network_and_losses AS\n",
    "SELECT\n",
    "    -- Keys\n",
    "    fm.Feeder AS Feeder_ID,  -- Using `Feeder` from feeder_master\n",
    "    ec.`Device ID` AS Device_ID,  -- Corrected to use `Device ID` with backticks\n",
    "    pm.`DTS ID` AS Grid_Station_ID,  -- Corrected to use `DTS ID` with backticks\n",
    "    ft.`Ticket No` AS Ticket_No,\n",
    "    \n",
    "    -- Metrics\n",
    "    COUNT(ft.Subject) AS Fault_Count,\n",
    "    SUM(CASE WHEN ft.SystemStatus = 'UnPlanned' THEN 1 ELSE 0 END) +\n",
    "    SUM(ppr.`Active Power (kW)`) AS Energy_Created,  -- Changed to `ppr` as column exists there\n",
    "    SUM(ppr.`Reactive Power (kVAR)`) AS Energy_Transmitted,  -- Changed to `ppr` as column exists there\n",
    "    COUNT(ft.`Emergency Type`) AS Emergency_Count,\n",
    "    COUNT(ft.`Ticket No`) AS Tickets_Processed,\n",
    "    AVG(`ft`.`CompletedAt` - ft.`Notification Time`) AS Fault_Resolution_Time,\n",
    "    MAX(ft.Subject) AS Most_Common_Fault\n",
    "FROM\n",
    "    fault_tickets ft\n",
    "    INNER JOIN feeder_master fm ON ft.`Feeder ID` = fm.Feeder\n",
    "    INNER JOIN energy_consumption ec ON fm.Feeder = ec.FeederID\n",
    "    INNER JOIN pmt_master pm ON pm.`DTS ID` = ft.`TicketDTSID`\n",
    "    INNER JOIN power_report ppr ON ppr.`Device ID` = ec.`Device ID`\n",
    "GROUP BY\n",
    "    fm.Feeder, ec.`Device ID`, pm.`DTS ID`, ft.`Ticket No`;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "348bc469-db5d-4075-9c09-4acf14ced16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+------------+----------------------+------------------+--------------+------------+--------------+--------------+-------------------------------------------+----------------------------+------------------+--------------------------------------+----------------+------------+-------------------+-----------------------------------+---------------+---------------------------------------+----------+-------------+-----------------------+----------------+------+---------+----------------------------+-----------------+------+-------------------+---------+--------------------+---------------+-------------------------------------+-------------------+--------------+------------------+---------------+----------------+-------------------+---------------------+--------------------+------------+--------------------+--------------------+----------------+--------------+-------------------+-------------------+-------------+----------------+-----------------+------------+----------+--------------------+----------------------+------------------------+---------------+--------------+--------------+--------------------+----------------+--------------+-------------+--------------------+-------------------+-----------------+--------+-------+---------+----------+--------------------+----------+--------------------+---------+--------------------+---------+--------------------+-------+----------+--------------+-------------+---------------+--------------------------+-------------------------+--------------------+--------------------+--------------------+---------+-------------------+--------------+--------------------+--------------------+----------+-------------------+---------------------------+-----------------------+---------------------------+-----------------------+-------------------------------+---------------------------+------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------------+--------------+-----------+-------------+\n",
      "|Business Partner|Industry Code|   PSC's IBC|Strategic/NonStrategic|PSC Classification|PSC Department|PSC Ministry|PSC Sub - Dept|PSC - Sub Area|Contract Account (Partner-Independent Data)|Posting date in the document|Reversal Indicator|Account Determination ID for Contracts|Billing Document|Meter Reader|Meter Reader PR NO.|Contract Number: Utilities Industry|Consumer Status|Installation Number: Utilities Industry|       IBC|Billing Class|Supply Guarantee Reason|   Rate Category|Region|Cycle Day|Industry Type Classification|Ord-Ind-PSC (OIP)|Tariff|BCM/Reversal Reason|Bill Type|Due Date (Print Doc)|Own Consumption|Issue Date (Print Doc's Posting Date)|Legacy Consumer No.|Print Document|Revenue Adjustment|    Unit Ranges|Unit Ranges Ind.|Calendar Year/Month|Calendar year/quarter|Periodic Quarterly A|GST_RETAILER|Import-OffPeak Units|Import-On Peak Units|GST (Further 1%)|GST (Extra 5%)|GST (Steel Melters)|Import Amt NetMtrng|RET_EMP_BENEF|No. Billing Docs|No. of Print Docs|Units Billed|Net Amount|Award Winner Benefit|GST on Retailers (5 %)|GST on Retailers (7.5 %)|Sanctioned Load|Connected Load|Maximum Demand|Additional Surcharge|Electricity Duty|Energy Charges|Fixed Charges|Emp Benefit -Officer|Free Benefit Amount|Fuel Adj. Charges|ISP Adj.|    GST|ID Meters|Income Tax|Income Tax Surcharge|Meter Rent|Misc. Energy Charges|PM Relief|Power Factor Penalty|Retro FSA|Sales Tax Adjustment|TVL Fee|LPS Billed|Off Peak Units|On Peak Units|Seasonal Impact|Uniform Qtr.Adj. on Import|Retro FCA Chrgs on Import|Deferred FCA Adj. Im|Deferred FCA Adjustm|Power holding Ltd. S|Surcharge|ISPA Adj. Ind. Peak|Quarterly Adj.|PM Relief Res. Comm.|Uniform Quatrly Adj.|MUCT (KMC)|Kissan Package 2022|New Qtr Adj Import Off Peak|New Qtr Adj Import Peak|New Qtr Adj Export Off Peak|New Qtr Adj Export Peak|Import Units Off Peak  Current|Import Units Peak  Current|Import Units Non-TOU  Current|Uniform Quarterly Ad|Quarterly Adj. Curre|Consp-Non IND-Import|Consp-Non TOD-Import|Normal GST on Import|WithHolding GST -IMP|Withholding ITax-IMP|No. of Print Docs/Bill Docs|Zero Rate ISPA|Surcharge_1|GST on Marble|\n",
      "+----------------+-------------+------------+----------------------+------------------+--------------+------------+--------------+--------------+-------------------------------------------+----------------------------+------------------+--------------------------------------+----------------+------------+-------------------+-----------------------------------+---------------+---------------------------------------+----------+-------------+-----------------------+----------------+------+---------+----------------------------+-----------------+------+-------------------+---------+--------------------+---------------+-------------------------------------+-------------------+--------------+------------------+---------------+----------------+-------------------+---------------------+--------------------+------------+--------------------+--------------------+----------------+--------------+-------------------+-------------------+-------------+----------------+-----------------+------------+----------+--------------------+----------------------+------------------------+---------------+--------------+--------------+--------------------+----------------+--------------+-------------+--------------------+-------------------+-----------------+--------+-------+---------+----------+--------------------+----------+--------------------+---------+--------------------+---------+--------------------+-------+----------+--------------+-------------+---------------+--------------------------+-------------------------+--------------------+--------------------+--------------------+---------+-------------------+--------------+--------------------+--------------------+----------+-------------------+---------------------------+-----------------------+---------------------------+-----------------------+-------------------------------+---------------------------+------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------------+--------------+-----------+-------------+\n",
      "|     SHAHZAD ALI| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038636810|                  08.01.2024|              NULL|                           Residential|    110064529724|        NULL|                  0|                           33652916|         ACTIVE|                             7003696797|   DEFENCE|     Domestic|           Not assigned|A1-R Residential|    C1|        2|                Not assigned|              ORD|   A1-|                  N|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           LC443343|  718014384241|              NULL|101 - 200 Units|1 - 10,000 Units|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|             0.0|           0.0|                0.0|                0.0|          0.0|               1|              1.0|       173.0|   7080.21|                 0.0|                   0.0|                     0.0|            4.0|           4.0|           0.0|                 0.0|           88.23|       3970.35|          0.0|                 0.0|                0.0|              0.0|     0.0|1074.69|      0.0|       0.0|                 0.0|       0.0|                 0.0|      0.0|                 0.0|      0.0|                 0.0|   35.0|    588.23|         173.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|              558.79|   262.96|                0.0|           0.0|                 0.0|              399.97|       0.0|                0.0|                        0.0|                    0.0|                      109.7|                    0.0|                            0.0|                        0.0|                           0.0|              567.68|               12.84|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "|            Raja| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038636829|                  08.01.2024|              NULL|                           Residential|    260028818682|        NULL|                  0|                           33672100|         ACTIVE|                             7003708319|  LAYARI-I|     Domestic|           Not assigned|A1-R Residential|    C7|        2|                Not assigned|              ORD|   A1-|                  N|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           LC444913|  721014317532|              NULL|101 - 200 Units|1 - 10,000 Units|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|             0.0|           0.0|                0.0|                0.0|          0.0|               1|              1.0|       117.0|   4905.73|                 0.0|                   0.0|                     0.0|            2.0|           2.0|           0.0|                 0.0|            61.0|       2685.15|          0.0|                 0.0|                0.0|              0.0|     0.0| 742.99|      0.0|       0.0|                 0.0|       0.0|                 0.0|      0.0|                 0.0|      0.0|                 0.0|   35.0|    406.67|         117.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|              377.91|   177.84|                0.0|           0.0|                 0.0|              336.96|       0.0|                0.0|                        0.0|                    0.0|                      96.28|                    0.0|                            0.0|                        0.0|                           0.0|              383.92|                8.68|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "|       SAMIULLAH| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038636845|                  08.01.2024|              NULL|                            Commercial|    663000759170|        NULL|                  0|                           33655547|         ACTIVE|                             7003702512|TIPUSULTAN|   Commercial|           Not assigned| A2-C Commercial|    C2|        2|                Not assigned|              ORD|   A2-|                  N|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           AM263283|  724014068685|              NULL|301 - 700 Units|1 - 10,000 Units|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|          979.03|       2937.09|                0.0|                0.0|          0.0|               1|              1.0|       485.0|  36343.17|                 0.0|                   0.0|                     0.0|            4.0|           4.0|           0.0|                 0.0|          479.92|      18308.75|          0.0|                 0.0|                0.0|              0.0|     0.0|4405.63|      0.0|    3485.7|                 0.0|       0.0|                 0.0|      0.0|                 0.0|      0.0|                 0.0|   60.0|   2399.58|         485.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|             1566.55|    737.2|                0.0|           0.0|                 0.0|             1358.88|       0.0|                0.0|                        0.0|                    0.0|                     396.95|                    0.0|                            0.0|                        0.0|                           0.0|             1591.48|               35.99|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "|      NOOR ISLAM| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038636896|                  08.01.2024|              NULL|                           Residential|    260028818654|        NULL|                  0|                           33683506|         ACTIVE|                             7003708068|  LAYARI-I|     Domestic|           Not assigned|A1-R Residential|    C7|        2|                Not assigned|              ORD|   A1-|                  A|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           LC444855|  721014317534|              NULL| 51 - 100 Units|1 - 10,000 Units|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|             0.0|           0.0|                0.0|                0.0|          0.0|               1|              1.0|        66.0|   1312.17|                 0.0|                   0.0|                     0.0|            2.0|           2.0|           0.0|                 0.0|            16.0|        510.84|          0.0|                 0.0|                0.0|              0.0|     0.0| 194.82|      0.0|       0.0|                 0.0|       0.0|                 0.0|      0.0|                 0.0|      0.0|                 0.0|   35.0|    106.64|          66.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|               28.38|   100.32|                0.0|           0.0|                 0.0|               131.9|       0.0|                0.0|                        0.0|                    0.0|                      73.44|                    0.0|                            0.0|                        0.0|                           0.0|              216.57|                 4.9|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "| Tareeqa Khatoon| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038636907|                  08.01.2024|              NULL|                           Residential|    260028818662|        NULL|                  0|                           33682835|         ACTIVE|                             7003708078|  LAYARI-I|     Domestic|           Not assigned|A1-R Residential|    C7|        2|                Not assigned|              ORD|   A1-|                  N|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           LC444873|  721014317535|              NULL|  21 - 50 Units|1 - 10,000 Units|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|             0.0|           0.0|                0.0|                0.0|          0.0|               1|              1.0|        24.0|    787.99|                 0.0|                   0.0|                     0.0|            2.0|           2.0|           0.0|                 0.0|            9.43|        185.76|          0.0|                 0.0|                0.0|              0.0|     0.0| 114.86|      0.0|       0.0|                 0.0|       0.0|                 0.0|      0.0|                 0.0|      0.0|                 0.0|   35.0|     62.87|          24.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|               10.32|    36.48|                0.0|           0.0|                 0.0|              224.64|       0.0|                0.0|                        0.0|                    0.0|                      90.97|                    0.0|                            0.0|                        0.0|                           0.0|               78.75|                1.78|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "| MUHAMMAD ISMAIL| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038639070|                  08.01.2024|              NULL|                           Residential|    260028818669|        NULL|                  0|                           33688513|         ACTIVE|                             7003708086|  LAYARI-I|     Domestic|           Not assigned|A1-R Residential|    C7|        2|                Not assigned|              ORD|   A1-|                  N|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           LC444880|  721014317537|              NULL| 51 - 100 Units|1 - 10,000 Units|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|             0.0|           0.0|                0.0|                0.0|          0.0|               1|              1.0|        79.0|   1452.41|                 0.0|                   0.0|                     0.0|            2.0|           2.0|           0.0|                 0.0|           17.75|        611.46|          0.0|                 0.0|                0.0|              0.0|     0.0| 216.22|      0.0|       0.0|                 0.0|       0.0|                 0.0|      0.0|                 0.0|      0.0|                 0.0|   35.0|    118.35|          79.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|               33.97|   120.08|                0.0|           0.0|                 0.0|               90.52|       0.0|                0.0|                        0.0|                    0.0|                      62.32|                    0.0|                            0.0|                        0.0|                           0.0|              259.23|                5.86|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "|        MUHAMMAD| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038639208|                  08.01.2024|              NULL|                            Commercial|    260028818685|        NULL|                  0|                           33689967|         ACTIVE|                             7003708332|  LAYARI-I|   Commercial|           Not assigned| A2-C Commercial|    C7|        2|                Not assigned|              ORD|   A2-|                  N|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           LC444916|  721014317538|              NULL|        Minimum|         Minimum|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|            7.14|          8.93|                0.0|                0.0|          0.0|               1|              1.0|         0.0|    286.69|                 0.0|                   0.0|                     0.0|            2.0|           2.0|           0.0|                 0.0|             3.5|           0.0|          0.0|                 0.0|                0.0|              0.0|     0.0|  32.13|      0.0|       0.0|                 0.0|       0.0|               144.5|      0.0|                 0.0|      0.0|                 0.0|   60.0|      17.5|           0.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|                 0.0|      0.0|                0.0|           0.0|                 0.0|               19.94|       0.0|                0.0|                        0.0|                    0.0|                      10.55|                    0.0|                            0.0|                        0.0|                           0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "|       ALAM KHAN| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038639267|                  08.01.2024|              NULL|                           Residential|    110064529741|        NULL|                  0|                           33673224|         ACTIVE|                             7003716806|   DEFENCE|     Domestic|           Not assigned|A1-R Residential|    C1|        2|                Not assigned|              ORD|   A1-|                  N|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           LC445738|  718014384242|              NULL|        Minimum|         Minimum|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|             0.0|           0.0|                0.0|                0.0|          0.0|               1|              1.0|         0.0|     123.5|                 0.0|                   0.0|                     0.0|            2.0|           2.0|           0.0|                 0.0|             0.0|           0.0|          0.0|                 0.0|                0.0|              0.0|     0.0|   13.5|      0.0|       0.0|                 0.0|       0.0|                75.0|      0.0|                 0.0|      0.0|                 0.0|   35.0|       7.5|           0.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|                 0.0|      0.0|                0.0|           0.0|                 0.0|                 0.0|       0.0|                0.0|                        0.0|                    0.0|                        0.0|                    0.0|                            0.0|                        0.0|                           0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "|       ALAM KHAN| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038639275|                  08.01.2024|              NULL|                           Residential|    110064529742|        NULL|                  0|                           33673225|         ACTIVE|                             7003716807|   DEFENCE|     Domestic|           Not assigned|A1-R Residential|    C1|        2|                Not assigned|              ORD|   A1-|                  N|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           LC445739|  718014384243|              NULL|  21 - 50 Units|1 - 10,000 Units|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|             0.0|           0.0|                0.0|                0.0|          0.0|               1|              1.0|        48.0|    961.21|                 0.0|                   0.0|                     0.0|            2.0|           2.0|           0.0|                 0.0|            11.6|        371.52|          0.0|                 0.0|                0.0|              0.0|     0.0| 141.29|      0.0|       0.0|                 0.0|       0.0|                 0.0|      0.0|                 0.0|      0.0|                 0.0|   35.0|     77.33|          48.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|               20.64|    72.96|                0.0|           0.0|                 0.0|              143.99|       0.0|                0.0|                        0.0|                    0.0|                       3.14|                    0.0|                            0.0|                        0.0|                           0.0|              157.51|                3.56|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "|     ISRAR AHMED| Not assigned|Not assigned|                  NULL|      Not assigned|  Not assigned|Not assigned|  Not assigned|  Not assigned|                               400038613047|                  08.01.2024|              NULL|                           Residential|    642001343948|        NULL|                  0|                           33664008|         ACTIVE|                             7003701603|    GARDEN|     Domestic|           Not assigned|A1-R Residential|    C2|        2|                Not assigned|              ORD|   A1-|                  N|  Regular|          22.01.2024|   Not assigned|                           08.01.2024|           LC443755|  721014313880|              NULL|101 - 200 Units|1 - 10,000 Units|              12024|                20241|                 0.0|         0.0|                 0.0|                 0.0|             0.0|           0.0|                0.0|                0.0|          0.0|               1|              1.0|       161.0|    6749.3|                 0.0|                   0.0|                     0.0|            4.0|           4.0|           0.0|                 0.0|           84.09|       3694.95|          0.0|                 0.0|                0.0|              0.0|     0.0|1024.21|      0.0|       0.0|                 0.0|       0.0|                 0.0|      0.0|                 0.0|      0.0|                 0.0|   35.0|     560.6|         161.0|          0.0|            0.0|                       0.0|                      0.0|                 0.0|                 0.0|              520.03|   244.72|                0.0|           0.0|                 0.0|              423.22|       0.0|                0.0|                        0.0|                    0.0|                     182.82|                    0.0|                            0.0|                        0.0|                           0.0|              528.31|               11.95|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                          1|           0.0|        0.0|          0.0|\n",
      "+----------------+-------------+------------+----------------------+------------------+--------------+------------+--------------+--------------+-------------------------------------------+----------------------------+------------------+--------------------------------------+----------------+------------+-------------------+-----------------------------------+---------------+---------------------------------------+----------+-------------+-----------------------+----------------+------+---------+----------------------------+-----------------+------+-------------------+---------+--------------------+---------------+-------------------------------------+-------------------+--------------+------------------+---------------+----------------+-------------------+---------------------+--------------------+------------+--------------------+--------------------+----------------+--------------+-------------------+-------------------+-------------+----------------+-----------------+------------+----------+--------------------+----------------------+------------------------+---------------+--------------+--------------+--------------------+----------------+--------------+-------------+--------------------+-------------------+-----------------+--------+-------+---------+----------+--------------------+----------+--------------------+---------+--------------------+---------+--------------------+-------+----------+--------------+-------------+---------------+--------------------------+-------------------------+--------------------+--------------------+--------------------+---------+-------------------+--------------+--------------------+--------------------+----------+-------------------+---------------------------+-----------------------+---------------------------+-----------------------+-------------------------------+---------------------------+------------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------------+--------------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM billing_data\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f2f62df5-e659-443e-888b-9710c9f55f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hakuna matata, table banata\n"
     ]
    }
   ],
   "source": [
    "query2 = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW fact_billing_and_recovery AS\n",
    "SELECT\n",
    "    -- Primary Key\n",
    "    MONOTONICALLY_INCREASING_ID() AS Fact_ID, -- Unique identifier for each record in the fact table\n",
    "    \n",
    "    -- Foreign Keys\n",
    "    b.`Billing Document` AS Billing_Document_ID, -- Foreign key to billing_data\n",
    "    r.`Contract Account (Partner-Independent Data)` AS Contract_Account_ID, -- Foreign key to recovery_data\n",
    "    \n",
    "    -- Additional Date Fields for Analysis\n",
    "    b.`Due Date (Print Doc)` AS Due_Date, -- Correct column with date conversion\n",
    "    \n",
    "    -- Metrics\n",
    "    SUM(b.`Net Amount`) AS Total_Amount, -- Summation of Net Amount for Total_Amount\n",
    "    SUM(\n",
    "        b.Tariff +\n",
    "        b.GST +\n",
    "        b.`GST (Further 1%)` +\n",
    "        b.`GST (Extra 5%)` +\n",
    "        b.`GST_Retailer` +\n",
    "        b.`GST on Retailers (5 %)` +\n",
    "        b.`GST on Retailers (7.5 %)` +\n",
    "        b.`GST` +\n",
    "        b.`Income Tax` +\n",
    "        b.`Income Tax Surcharge` +\n",
    "        b.`Normal GST on import` +\n",
    "        b.`WithHolding GST -IMP` +\n",
    "        b.`WithHolding ITax-IMP` +\n",
    "        b.Surcharge +\n",
    "        b.`Surcharge_1` +\n",
    "        b.`GST on Marble` +\n",
    "        b.`Electricity Duty` +\n",
    "        b.`Additional Surcharge` +\n",
    "        b.`MUCT (KMC)`\n",
    "    ) AS Tax_Amount, -- Summation of all specified tax components\n",
    "    SUM(r.`Total Amount`) AS Recovery_Amount, -- Total recovery amount from recovery\n",
    "    (SUM(b.`Net Amount`) - SUM(r.`Total Amount`)) AS Outstanding_Amount, -- Difference between Total Amount and Recovery Amount\n",
    "    AVG(DATEDIFF(r.`Document Date`, b.`Issue Date (Print Doc's Posting Date)`)) AS Average_Payment_Time, -- Document Date - Issue Date for payment time\n",
    "    SUM(b.`Units Billed`) AS Units_Billed, -- Total units billed\n",
    "    SUM(\n",
    "        b.`Emp Benefit -Officer` +\n",
    "        b.`Free Benefit Amount` +\n",
    "        b.`PM Relief` +\n",
    "        b.`PM Relief Res. Comm.` +\n",
    "        b.`Award Winner Benefit` +\n",
    "        b.`ISP Adj.` +\n",
    "        b.`Fuel Adj. Charges` +\n",
    "        b.`Sales Tax Adjustment` +\n",
    "        b.`Uniform Qtr.Adj. on Import` +\n",
    "        b.`Deferred FCA Adj. Im` +\n",
    "        b.`Deferred FCA Adjustm` +\n",
    "        b.`ISPA Adj. Ind. Peak` +\n",
    "        b.`Quarterly Adj.` +\n",
    "        b.`Uniform Quatrly Adj.` +\n",
    "        b.`New Qtr Adj Import Off Peak` +\n",
    "        b.`New Qtr Adj Import Peak` +\n",
    "        b.`New Qtr Adj Export Off Peak` +\n",
    "        b.`New Qtr Adj Export Peak` +\n",
    "        b.`Quarterly Adj. Curre`\n",
    "    ) AS Benefits_Adjustments, -- Summation of benefits and adjustments\n",
    "    SUM(\n",
    "        b.`Import-OffPeak Units` +\n",
    "        b.`Import-On Peak Units` +\n",
    "        b.`Import Units Off Peak  Current` +\n",
    "        b.`Import Units Non-TOU  Current` +\n",
    "        b.`Consp-Non TOD-Import`\n",
    "    ) AS Import_Tax_Amount, -- Summation of import-related taxes\n",
    "    SUM(b.`Energy Charges`) AS Energy_Charges, -- Energy charges from billing\n",
    "    SUM(b.`Units Billed`) AS Surcharge_Amount -- Replacing with summation of units billed\n",
    "FROM\n",
    "    billing_data b\n",
    "    INNER JOIN recovery_data r \n",
    "    ON b.`Contract Account (Partner-Independent Data)` = r.`Contract Account (Partner-Independent Data)`\n",
    "WHERE\n",
    "    b.`Contract Account (Partner-Independent Data)` IS NOT NULL -- Exclude rows with NULL Contract Account\n",
    "GROUP BY\n",
    "    b.`Billing Document`,\n",
    "    r.`Contract Account (Partner-Independent Data)`,\n",
    "    b.`Due Date (Print Doc)`;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query2)\n",
    "print(\"hakuna matata, table banata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "72144508-c87f-4a91-88fa-a7be037ee119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+------------+----------+---------------+------------------+--------------------+------------+--------------------+-----------------+--------------+----------------+\n",
      "|Fact_ID|Billing_Document_ID|Contract_Account_ID|  Due_Date|Total_Amount|Tax_Amount|Recovery_Amount|Outstanding_Amount|Average_Payment_Time|Units_Billed|Benefits_Adjustments|Import_Tax_Amount|Energy_Charges|Surcharge_Amount|\n",
      "+-------+-------------------+-------------------+----------+------------+----------+---------------+------------------+--------------------+------------+--------------------+-----------------+--------------+----------------+\n",
      "|      0|       100056932870|       400000000009|22.01.2024|   163068.55|      NULL|   2.72257278E8|   -2.7209420945E8|                NULL|      2538.0|             4697.81|              0.0|      91680.96|          2538.0|\n",
      "|      1|       100056934751|       400000000009|22.01.2024|   171915.17|      NULL|   2.72257278E8|   -2.7208536283E8|                NULL|      2624.0|             5839.47|              0.0|      94813.19|          2624.0|\n",
      "|      2|       100056935754|       400000000009|22.01.2024|   144523.23|      NULL|   2.72257278E8|   -2.7211275477E8|                NULL|      2250.0|             4149.45|              0.0|      82118.37|          2250.0|\n",
      "|      3|       100056935786|       400000000009|22.01.2024|   271835.11|      NULL|   2.72257278E8|   -2.7198544289E8|                NULL|      4161.0|             8100.92|              0.0|     150648.93|          4161.0|\n",
      "|      4|       100056937458|       400000000009|22.01.2024|   154683.79|      NULL|   2.72257278E8|   -2.7210259421E8|                NULL|      2397.0|             4551.93|              0.0|      87456.54|          2397.0|\n",
      "|      5|       100056944421|       400000000009|22.01.2024|   234953.92|      NULL|   2.72257278E8|   -2.7202232408E8|                NULL|      3652.0|   8792.380000000001|              0.0|      133279.3|          3652.0|\n",
      "|      6|       100056948040|       400000000009|22.01.2024|   186821.35|      NULL|   2.72257278E8|   -2.7207045665E8|                NULL|      2908.0|              5985.6|              0.0|     102885.04|          2908.0|\n",
      "|      7|       100056958040|       400000000009|22.01.2024|   137518.05|      NULL|   2.72257278E8|   -2.7211975995E8|                NULL|      2072.0|  3526.8299999999995|              0.0|      76375.94|          2072.0|\n",
      "|      8|       100056964324|       400000000009|22.01.2024|   218781.55|      NULL|   2.72257278E8|   -2.7203849645E8|                NULL|      3482.0|             6688.02|              0.0|     123193.16|          3482.0|\n",
      "|      9|       110064545915|       400000000009|22.01.2024|   160502.08|      NULL|   2.72257278E8|   -2.7209677592E8|                NULL|      2432.0|             4456.12|              0.0|       90056.0|          2432.0|\n",
      "+-------+-------------------+-------------------+----------+------------+----------+---------------+------------------+--------------------+------------+--------------------+-----------------+--------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM fact_billing_and_recovery\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18586b00-5e59-4927-aa4a-abaab55a2156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"CREATE NAMESPACE nessie.fact_network_and_loss;\").show()\n",
    "spark.sql(\"CREATE NAMESPACE nessie.fact_billing_and_recovery_final1;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe701fc0-4890-4cc0-b791-9d6479603f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE nessie.fact_network_and_losses;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e43fca-f895-4fc5-8a40-20edb4cc44cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/13 13:44:21 ERROR Executor: Exception in task 2.0 in stage 15.0 (TID 17)4]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$Lambda$4884/0x0000000841af3840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4443/0x000000084188b440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$4367/0x0000000841825440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "24/12/13 13:44:21 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 15.0 (TID 17),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$Lambda$4884/0x0000000841af3840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4443/0x000000084188b440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$4367/0x0000000841825440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "24/12/13 13:44:21 WARN TaskSetManager: Lost task 2.0 in stage 15.0 (TID 17) (25d3229c2ec2 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$Lambda$4884/0x0000000841af3840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4443/0x000000084188b440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$4367/0x0000000841825440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\n",
      "24/12/13 13:44:21 ERROR TaskSetManager: Task 2 in stage 15.0 failed 1 times; aborting job\n",
      "24/12/13 13:44:21 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:122)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.processCurrentSortedGroup(SortBasedAggregationIterator.scala:118)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:149)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:30)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.processCurrentSortedGroup(SortBasedAggregationIterator.scala:120)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:149)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:30)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/12/13 13:44:21 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 16, attempt 0, stage 15.0)\n",
      "24/12/13 13:44:21 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:122)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.processCurrentSortedGroup(SortBasedAggregationIterator.scala:118)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:149)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:30)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.processCurrentSortedGroup(SortBasedAggregationIterator.scala:120)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:149)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:30)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/12/13 13:44:21 ERROR DataWritingSparkTask: Aborting commit for partition 3 (task 18, attempt 0, stage 15.0)\n",
      "24/12/13 13:44:21 ERROR AppendDataExec: Data source write support IcebergBatchWrite(table=fact_network_and_losses.fact_network_and_loss_data_raw, format=PARQUET) is aborting.\n",
      "24/12/13 13:44:21 ERROR AppendDataExec: Data source write support IcebergBatchWrite(table=fact_network_and_losses.fact_network_and_loss_data_raw, format=PARQUET) failed to abort.\n",
      "24/12/13 13:44:21 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Writing job failed.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobFailedError(QueryExecutionErrors.scala:903)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:336)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:577)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:573)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:567)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:216)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:196)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:134)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 15.0 failed 1 times, most recent failure: Lost task 2.0 in stage 15.0 (TID 17) (25d3229c2ec2 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$Lambda$4884/0x0000000841af3840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4443/0x000000084188b440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$4367/0x0000000841825440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n",
      "\t... 74 more\n",
      "\tSuppressed: java.lang.IllegalStateException: Shutdown in progress\n",
      "\t\tat java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)\n",
      "\t\tat java.base/java.lang.Runtime.addShutdownHook(Runtime.java:216)\n",
      "\t\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addShutdownHook(MoreExecutors.java:289)\n",
      "\t\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addDelayedShutdownHook(MoreExecutors.java:266)\n",
      "\t\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:241)\n",
      "\t\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:246)\n",
      "\t\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors.getExitingExecutorService(MoreExecutors.java:129)\n",
      "\t\tat org.apache.iceberg.util.ThreadPools.newWorkerPool(ThreadPools.java:89)\n",
      "\t\tat org.apache.iceberg.aws.s3.S3FileIO.executorService(S3FileIO.java:339)\n",
      "\t\tat org.apache.iceberg.aws.s3.S3FileIO.deleteFiles(S3FileIO.java:213)\n",
      "\t\tat org.apache.iceberg.spark.source.SparkCleanupUtil.bulkDelete(SparkCleanupUtil.java:99)\n",
      "\t\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deletePaths(SparkCleanupUtil.java:91)\n",
      "\t\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deleteFiles(SparkCleanupUtil.java:85)\n",
      "\t\tat org.apache.iceberg.spark.source.SparkWrite.abort(SparkWrite.java:244)\n",
      "\t\tat org.apache.iceberg.spark.source.SparkWrite.access$800(SparkWrite.java:84)\n",
      "\t\tat org.apache.iceberg.spark.source.SparkWrite$BaseBatchWrite.abort(SparkWrite.java:281)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:406)\n",
      "\t\t... 74 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:77)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$Lambda$4884/0x0000000841af3840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4443/0x000000084188b440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$4367/0x0000000841825440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "24/12/13 13:44:21 WARN Utils: Suppressing exception in catch: Shutdown in progress\n",
      "java.lang.IllegalStateException: Shutdown in progress\n",
      "\tat java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)\n",
      "\tat java.base/java.lang.Runtime.addShutdownHook(Runtime.java:216)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addShutdownHook(MoreExecutors.java:289)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addDelayedShutdownHook(MoreExecutors.java:266)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:241)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:246)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors.getExitingExecutorService(MoreExecutors.java:129)\n",
      "\tat org.apache.iceberg.util.ThreadPools.newWorkerPool(ThreadPools.java:89)\n",
      "\tat org.apache.iceberg.aws.s3.S3FileIO.executorService(S3FileIO.java:339)\n",
      "\tat org.apache.iceberg.aws.s3.S3FileIO.deleteFiles(S3FileIO.java:213)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.bulkDelete(SparkCleanupUtil.java:99)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deletePaths(SparkCleanupUtil.java:91)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deleteFiles(SparkCleanupUtil.java:85)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deleteTaskFiles(SparkCleanupUtil.java:57)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.abort(SparkWrite.java:742)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/12/13 13:44:21 WARN Utils: Suppressing exception in catch: Shutdown in progress\n",
      "java.lang.IllegalStateException: Shutdown in progress\n",
      "\tat java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)\n",
      "\tat java.base/java.lang.Runtime.addShutdownHook(Runtime.java:216)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addShutdownHook(MoreExecutors.java:289)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addDelayedShutdownHook(MoreExecutors.java:266)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:241)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:246)\n",
      "\tat org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors.getExitingExecutorService(MoreExecutors.java:129)\n",
      "\tat org.apache.iceberg.util.ThreadPools.newWorkerPool(ThreadPools.java:89)\n",
      "\tat org.apache.iceberg.aws.s3.S3FileIO.executorService(S3FileIO.java:339)\n",
      "\tat org.apache.iceberg.aws.s3.S3FileIO.deleteFiles(S3FileIO.java:213)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.bulkDelete(SparkCleanupUtil.java:99)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deletePaths(SparkCleanupUtil.java:91)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deleteFiles(SparkCleanupUtil.java:85)\n",
      "\tat org.apache.iceberg.spark.source.SparkCleanupUtil.deleteTaskFiles(SparkCleanupUtil.java:57)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.abort(SparkWrite.java:742)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.spark.util.Utils.exceptionString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fact_network_and_loss_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM fact_network_and_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mfact_network_and_loss_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnessie.fact_network_and_losses.fact_network_and_loss_data_raw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminio shinio here we go!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:2100\u001b[0m, in \u001b[0;36mDataFrameWriterV2.createOrReplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateOrReplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;124;03m    Create a new table or replace an existing table with the contents of the data frame.\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;124;03m    If the table exists, its configuration and data will be replaced.\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m     stackTrace\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m convert_exception(origin\u001b[38;5;241m.\u001b[39mgetCause())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m     stackTrace\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m convert_exception(origin\u001b[38;5;241m.\u001b[39mgetCause())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:157\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkUpgradeException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    156\u001b[0m c: Py4JJavaError \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mgetCause()\n\u001b[0;32m--> 157\u001b[0m stacktrace: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexceptionString\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    159\u001b[0m     is_instance_of(gw, c, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.api.python.PythonException\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.util.Utils.exceptionString"
     ]
    }
   ],
   "source": [
    "fact_network_and_loss_df = spark.sql(\"SELECT * FROM fact_network_and_losses\")\n",
    "\n",
    "fact_network_and_loss_df.writeTo(\"nessie.fact_network_and_losses.fact_network_and_loss_data_raw\").createOrReplace()\n",
    "\n",
    "print (\"minio shinio here we go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bfba9326-33ec-4d24-8451-30eaec167d50",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `fact_billing_and_recovery` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [fact_billing_and_recovery], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have already created a DataFrame `fact_billing_and_recovery`\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fact_billing_and_recovery_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM fact_billing_and_recovery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Writing the DataFrame into Dremio\u001b[39;00m\n\u001b[1;32m      5\u001b[0m fact_billing_and_recovery_df\u001b[38;5;241m.\u001b[39mwriteTo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnessie.fact_billing_and_recovery_final1.fact_billing_and_recovery_data_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateOrReplace()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `fact_billing_and_recovery` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [fact_billing_and_recovery], [], false\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already created a DataFrame `fact_billing_and_recovery`\n",
    "fact_billing_and_recovery_df = spark.sql(\"SELECT * FROM fact_billing_and_recovery\")\n",
    "\n",
    "# Writing the DataFrame into Dremio\n",
    "fact_billing_and_recovery_df.writeTo(\"nessie.fact_billing_and_recovery_final1.fact_billing_and_recovery_data_raw\").createOrReplace()\n",
    "\n",
    "print(\"dou piyasi in dremio lesso (fact_table for billing shilling)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81b270ec-92a6-4a16-aff4-a6b0ccff5595",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `nessie`.`fact_network_and_loss2`.`fact_network_data_raw` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [nessie, fact_network_and_loss2, fact_network_data_raw], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnessie.fact_network_and_loss2.fact_network_data_raw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:484\u001b[0m, in \u001b[0;36mDataFrameReader.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.4.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tblA\")\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `nessie`.`fact_network_and_loss2`.`fact_network_data_raw` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [nessie, fact_network_and_loss2, fact_network_data_raw], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"nessie.fact_network_and_loss2.fact_network_data_raw\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0930e-b382-4f8f-bcb0-6fcc72ff16ed",
   "metadata": {},
   "source": [
    "### Appending values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "66871a4e-8d37-4b3e-8d8a-32dfd1344be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e01bc2-64d0-4833-87c9-6f63b9ca1cbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `nessie`.`fact_network_and_loss11`.`fact_network_and_loss_data_raw`.`snapshots` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 3 pos 9;\n'Project [*]\n+- 'UnresolvedRelation [nessie, fact_network_and_loss11, fact_network_and_loss_data_raw, snapshots], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43m    SELECT *\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m    FROM nessie.fact_network_and_loss11.fact_network_and_loss_data_raw.snapshots\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `nessie`.`fact_network_and_loss11`.`fact_network_and_loss_data_raw`.`snapshots` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 3 pos 9;\n'Project [*]\n+- 'UnresolvedRelation [nessie, fact_network_and_loss11, fact_network_and_loss_data_raw, snapshots], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM nessie.fact_network_and_loss11.fact_network_and_loss_data_raw.snapshots\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed950274-4901-4bfd-b6b6-958e16ec37d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
